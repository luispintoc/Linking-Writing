{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.fft import fft\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to run locally\n",
    "run_local = True\n",
    "\n",
    "if run_local:\n",
    "    df = pd.read_csv('../data/train_logs.csv')\n",
    "    scores = pd.read_csv('../data/train_scores.csv')\n",
    "    df_test = pd.read_csv('../data/test_logs.csv')\n",
    "else:\n",
    "    df = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n",
    "    scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n",
    "    df_test = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')\n",
    "\n",
    "df = df.merge(scores, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for pandas apply that check number of large text changes\n",
    "def count_large_text_changes(text_changes):\n",
    "    return len([tc for tc in text_changes if len(tc) > 20])\n",
    "\n",
    "def count_extremely_large_text_changes(text_changes):\n",
    "    return len([tc for tc in text_changes if len(tc) > 100])\n",
    "\n",
    "# For a given type of activity, count the number of times it occurs\n",
    "def count_nonproduction(action_list):\n",
    "    action_type = 'Nonproduction'\n",
    "    return len([action for action in action_list if action == action_type])\n",
    "\n",
    "# count input\n",
    "def count_input(action_list):\n",
    "    action_type = 'Input'\n",
    "    return len([action for action in action_list if action == action_type])\n",
    "\n",
    "# count remove/cut\n",
    "def count_remove(action_list):\n",
    "    action_type = 'Remove/Cut'\n",
    "    return len([action for action in action_list if action == action_type])\n",
    "\n",
    "# Count Replace\n",
    "def count_replace(action_list):\n",
    "    action_type = 'Replace'\n",
    "    return len([action for action in action_list if action == action_type])\n",
    "\n",
    "# Count Paste\n",
    "def count_paste(action_list):\n",
    "    action_type = 'Paste'\n",
    "    return len([action for action in action_list if action == action_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_aggregation_functions(df):\n",
    "\n",
    "    # Feature engineering for typing behavior features\n",
    "    typing_features = df.groupby('id').agg({\n",
    "        'activity': 'count',                # Total number of activities\n",
    "        'action_time': ['sum', 'mean'],     # Total and average action time\n",
    "        'word_count': 'max',                # Maximum word count\n",
    "        'text_change': 'nunique',           # Number of unique text changes\n",
    "        'cursor_position': 'mean',           # Average cursor position\n",
    "        'text_change' : count_large_text_changes,\n",
    "        'text_change' : count_extremely_large_text_changes,\n",
    "        'activity': count_nonproduction,\n",
    "        'activity': count_input,\n",
    "        'activity': count_remove,\n",
    "        'activity': count_replace,\n",
    "        'activity': count_paste,\n",
    "    })\n",
    "\n",
    "    # Flatten the multi-level column index\n",
    "    typing_features.columns = ['_'.join(col).strip() for col in typing_features.columns.values]\n",
    "\n",
    "    return typing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the function to calculate top N frequencies and their magnitudes for each 'id' using groupby and apply\n",
    "def calculate_fft_features(group):\n",
    "\n",
    "    group['pos'] = group['cursor_position']%30\n",
    "    group['line'] = (group['cursor_position']/30).astype(int)\n",
    "\n",
    "    # Perform Fourier Transform on 'pos'\n",
    "    fft_values = fft(group['pos'])[1:]\n",
    "    \n",
    "    # Generate frequencies corresponding to the Fourier Transform values\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), 1)[1:]\n",
    "    \n",
    "    # Take absolute value to get magnitude\n",
    "    fft_magnitude = np.abs(fft_values)\n",
    "    \n",
    "    # Identify indices where the frequencies are positive\n",
    "    positive_indices = np.where(frequencies > 0)[0]\n",
    "    \n",
    "    # Filter out only positive frequencies and skip the zero frequency\n",
    "    frequencies = frequencies[positive_indices]\n",
    "    magnitudes = fft_magnitude[positive_indices]\n",
    "    \n",
    "    # Frequency Domain Features\n",
    "    peak_freq = frequencies[np.argmax(magnitudes)]\n",
    "    if np.sum(magnitudes) == 0:\n",
    "        mean_freq = 0  # or some other appropriate default value\n",
    "    else:\n",
    "        mean_freq = np.average(frequencies, weights=magnitudes)\n",
    "\n",
    "    median_freq = frequencies[len(magnitudes) // 2]\n",
    "    bandwidth = np.ptp(frequencies)\n",
    "    freq_skewness = scipy.stats.skew(magnitudes)\n",
    "    freq_kurtosis = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    # Other Features\n",
    "    total_energy = np.sum(magnitudes ** 2)\n",
    "    \n",
    "    # Spectral Entropy\n",
    "    psd_norm = np.abs(magnitudes) / np.sum(np.abs(magnitudes))\n",
    "    spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + np.finfo(float).eps))\n",
    "    \n",
    "    # Spectral Flatness\n",
    "    spectral_flatness = np.exp(np.mean(np.log(magnitudes + np.finfo(float).eps))) / np.mean(magnitudes)\n",
    "    \n",
    "    # Spectral Roll-off\n",
    "    spectral_sum = np.cumsum(magnitudes)\n",
    "    spectral_rolloff = frequencies[np.searchsorted(spectral_sum, 0.85 * spectral_sum[-1])]\n",
    "    \n",
    "    # Statistical Features\n",
    "    mean_amplitude = np.mean(magnitudes)\n",
    "    std_amplitude = np.std(magnitudes)\n",
    "    skew_amplitude = scipy.stats.skew(magnitudes)\n",
    "    kurtosis_amplitude = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    features = {\n",
    "        \"Peak Frequency\": peak_freq,\n",
    "        \"Mean Frequency\": mean_freq,\n",
    "        \"Median Frequency\": median_freq,\n",
    "        \"Bandwidth\": bandwidth,\n",
    "        \"Frequency Skewness\": freq_skewness,\n",
    "        \"Frequency Kurtosis\": freq_kurtosis,\n",
    "        \"Total Energy\": total_energy,\n",
    "        \"Spectral Entropy\": spectral_entropy,\n",
    "        \"Spectral Flatness\": spectral_flatness,\n",
    "        \"Spectral Roll-off\": spectral_rolloff,\n",
    "        \"Mean Amplitude\": mean_amplitude,\n",
    "        \"Std Amplitude\": std_amplitude,\n",
    "        \"Skew Amplitude\": skew_amplitude,\n",
    "        \"Kurtosis Amplitude\": kurtosis_amplitude\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "def apply_fft_feats(df):\n",
    "    return df.groupby('id').apply(calculate_fft_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, df_test, scale_needed=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the input dataframes for training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = pd.DataFrame({'id': df['id'].unique()})\n",
    "    y_train = df.groupby('id')['score'].first().reset_index(drop=True)\n",
    "    X_test = pd.DataFrame({'id': df_test['id'].unique()})\n",
    "\n",
    "    # Feature engineering\n",
    "    agg_train_list = []\n",
    "    agg_test_list = []\n",
    "\n",
    "    # Aggregate and collect features without merging\n",
    "    for func in [raw_aggregation_functions, apply_fft_feats]:\n",
    "        agg_train = func(df)\n",
    "        agg_test = func(df_test)\n",
    "\n",
    "        # Check if the index name is 'id'\n",
    "        if agg_train.index.name != 'id' or agg_test.index.name != 'id':\n",
    "            raise ValueError(\"The index must be 'id' for aggregation functions.\")\n",
    "        \n",
    "        agg_train_list.append(agg_train)\n",
    "        agg_test_list.append(agg_test)\n",
    "\n",
    "    # Concatenate all aggregated features horizontally, aligned by index\n",
    "    agg_train = pd.concat(agg_train_list, axis=1)\n",
    "    agg_test = pd.concat(agg_test_list, axis=1)\n",
    "\n",
    "    # Reset index before merge\n",
    "    agg_train.reset_index(inplace=True)\n",
    "    agg_test.reset_index(inplace=True)\n",
    "\n",
    "    # Perform a single merge operation\n",
    "    X_train = X_train.merge(agg_train, on='id', how='left')\n",
    "    X_test = X_test.merge(agg_test, on='id', how='left')\n",
    "\n",
    "    # Remove 'id' column before scaling\n",
    "    X_train_ids = X_train['id']\n",
    "    X_train = X_train.drop(columns=['id'])\n",
    "    X_test_ids = X_test['id']\n",
    "    X_test = X_test.drop(columns=['id'])\n",
    "\n",
    "    # Standardize features\n",
    "    if scale_needed:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_train_ids, y_train, X_test, X_test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains a regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(model, X_test):\n",
    "    \"\"\"\n",
    "    Makes predictions using the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(X_test_ids, preds):\n",
    "    try:\n",
    "        # Check if X_test_ids and preds have the same length\n",
    "        if len(X_test_ids) != len(preds):\n",
    "            raise ValueError(\"The lengths of X_test_ids and preds must match.\")\n",
    "        \n",
    "        # Create a submission file\n",
    "        submission = pd.DataFrame({'id': X_test_ids, 'score': preds})\n",
    "        submission.to_csv('./submission.csv', index=False)\n",
    "        print('Submitted')\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_ids, y_train, X_test, X_test_ids = preprocessing(df, df_test, scale_needed=True)\n",
    "model = train(X_train, y_train)\n",
    "preds = predict(model, X_test)\n",
    "# submit(X_test_ids, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
