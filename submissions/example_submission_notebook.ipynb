{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c739d4b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-15T21:58:42.903556Z",
     "iopub.status.busy": "2023-10-15T21:58:42.903205Z",
     "iopub.status.idle": "2023-10-15T21:58:44.810249Z",
     "shell.execute_reply": "2023-10-15T21:58:44.808872Z"
    },
    "papermill": {
     "duration": 1.914842,
     "end_time": "2023-10-15T21:58:44.813339",
     "exception": false,
     "start_time": "2023-10-15T21:58:42.898497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.fft import fft\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2941e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "SEED = 1234\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa02da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:58:44.822948Z",
     "iopub.status.busy": "2023-10-15T21:58:44.822430Z",
     "iopub.status.idle": "2023-10-15T21:59:01.134253Z",
     "shell.execute_reply": "2023-10-15T21:59:01.132916Z"
    },
    "papermill": {
     "duration": 16.318596,
     "end_time": "2023-10-15T21:59:01.137083",
     "exception": false,
     "start_time": "2023-10-15T21:58:44.818487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flag to run locally\n",
    "run_local = True\n",
    "\n",
    "if run_local:\n",
    "    df = pd.read_csv('../data/train_logs.csv')\n",
    "    scores = pd.read_csv('../data/train_scores.csv')\n",
    "    df_test = df\n",
    "else:\n",
    "    df = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\n",
    "    scores = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')\n",
    "    df_test = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')\n",
    "\n",
    "df = df.merge(scores, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7266ca81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:59:01.178378Z",
     "iopub.status.busy": "2023-10-15T21:59:01.177958Z",
     "iopub.status.idle": "2023-10-15T21:59:01.270495Z",
     "shell.execute_reply": "2023-10-15T21:59:01.269525Z"
    },
    "papermill": {
     "duration": 0.098794,
     "end_time": "2023-10-15T21:59:01.272520",
     "exception": false,
     "start_time": "2023-10-15T21:59:01.173726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------- Check the size of a text change, \n",
    "# Note: Should be applied to column \"text_change\"\n",
    "\n",
    "# Function for pandas apply that check number of large text changes\n",
    "def count_large_text_changes(text_changes, size=20):\n",
    "    return len([tc for tc in text_changes if len(tc) > size])\n",
    "\n",
    "def count_extremely_large_text_changes(text_changes, size=100):\n",
    "    return len([tc for tc in text_changes if len(tc) > size])\n",
    "\n",
    "def count_tiny_text_changes(text_changes, size=5):\n",
    "    return len([tc for tc in text_changes if len(tc) < size])\n",
    "\n",
    "# ------------- For a given type of activity, count the number of times it occurs ------------ #\n",
    "# Note: Should be applied to column \"activity\"\n",
    "\n",
    "# Count nonproduction\n",
    "def count_nonproduction(action_list):\n",
    "    return len([action for action in action_list if action == 'Nonproduction'])\n",
    "\n",
    "# count input\n",
    "def count_input(action_list):\n",
    "    return len([action for action in action_list if action == 'Input'])\n",
    "\n",
    "# count remove/cut\n",
    "def count_remove(action_list):\n",
    "    return len([action for action in action_list if action == 'Remove/Cut'])\n",
    "\n",
    "# Count Replace\n",
    "def count_replace(action_list):\n",
    "    return len([action for action in action_list if action == 'Replace'])\n",
    "\n",
    "# Count Paste\n",
    "def count_paste(action_list):\n",
    "    return len([action for action in action_list if action == 'Paste'])\n",
    "\n",
    "# ------------- For a given chunk of text that was moved, determine the size and the distance moved ------------ #\n",
    "# Create move vectors features uses the four helpers below\n",
    "def create_move_vectors_features(df): \n",
    "    \n",
    "    # Create selection_vectors (the position of the selection before and after the move)\n",
    "    df['selection_vectors'] = df['activity'].map(get_move_from_vectors)\n",
    "\n",
    "    # Create functions from this\n",
    "    df['distance_of_moved_selection'] = df['selection_vectors'].map(distance_of_move_of_selection)\n",
    "    df['size_of_moved_selection'] = df['selection_vectors'].map(size_of_moved_selection)\n",
    "    df.drop(['selection_vectors'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to extract the distance vectors from the activity column\n",
    "def split_activity(activity):\n",
    "    return [np.array(a[1:-1].split(', '), dtype=int) for a in re.findall(r'\\[[0-9]*, [0-9]*\\]', activity)]\n",
    "\n",
    "# Extract the vectors from the activity column when Move From is in the activity\n",
    "def get_move_from_vectors(activity):\n",
    "    if 'Move From' in activity:\n",
    "        return split_activity(activity)\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def distance_of_move_of_selection(selection_vectors):\n",
    "    if len(selection_vectors) > 0:\n",
    "        return selection_vectors[1][0] - selection_vectors[0][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# How large was the selection\n",
    "def size_of_moved_selection(selection_vectors):\n",
    "    if len(selection_vectors) > 0:\n",
    "        return selection_vectors[0][1] - selection_vectors[0][0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "## Time features\n",
    "### The total amount of time spent on the essay (as a fraction of total time allowed)\n",
    "# The total amount of time the person spent writing the essay as a fraction of the total time\n",
    "def fraction_of_time_spent_writing(writing_times):\n",
    "    total_time = 1800000 # Half an hour in milliseconds\n",
    "    max_time = max(writing_times)\n",
    "    return max_time / total_time\n",
    "\n",
    "# This function simply normalizes the text of Move From to be uniform, \n",
    "# should be used once Move From features have already been created\n",
    "def normalize_move_from(activity):\n",
    "    if 'Move From' in activity:\n",
    "        return 'Move From'\n",
    "    else:\n",
    "        return activity\n",
    "    \n",
    "def create_action_time_features(df):\n",
    "    # Normalize move from column\n",
    "    df['activity'] = df['activity'].map(normalize_move_from)\n",
    "\n",
    "    # Calculate average time, max time and total time of different actions\n",
    "    action_time_features = df.groupby(['id', 'activity']).agg(\n",
    "        {'action_time': ['mean', 'max', 'sum', 'count']}\n",
    "    )\n",
    "\n",
    "    # Flatten multi index columns\n",
    "    action_time_features.columns = ['_'.join(col).strip() for col in action_time_features.columns.values]\n",
    "\n",
    "    # Unstack multi index rows\n",
    "    action_time_features = action_time_features.unstack('activity')\n",
    "\n",
    "    # Re-flatten multi index columns\n",
    "    action_time_features.columns = ['_'.join(col).strip() for col in action_time_features.columns.values]\n",
    "    action_time_features.fillna(0, inplace=True) # Fill na with 0s \n",
    "    return action_time_features\n",
    "\n",
    "def raw_aggregation_functions(df):\n",
    "\n",
    "    # Create features related to individual action time\n",
    "    action_time_features = create_action_time_features(df)\n",
    "\n",
    "    # Create features related to moved selections of text\n",
    "    df = create_move_vectors_features(df)\n",
    "\n",
    "    # Feature engineering for typing behavior features\n",
    "    typing_features = df.groupby('id').agg({\n",
    "        'activity': 'count',                # Total number of activities\n",
    "        'action_time': ['sum', 'mean'],     # Total and average action time\n",
    "        'word_count': 'max',                # Maximum word count\n",
    "        'text_change': 'nunique',           # Number of unique text changes\n",
    "        'cursor_position': 'mean',           # Average cursor position\n",
    "        'text_change' : count_large_text_changes,\n",
    "        'text_change' : count_extremely_large_text_changes,\n",
    "        'text_change' : count_tiny_text_changes,\n",
    "        'activity': count_nonproduction,\n",
    "        'activity': count_input,\n",
    "        'activity': count_remove,\n",
    "        'activity': count_replace,\n",
    "        'activity': count_paste,\n",
    "        'distance_of_moved_selection': ['mean', 'max'],\n",
    "        'size_of_moved_selection': ['mean', 'max'],\n",
    "        'up_time': fraction_of_time_spent_writing, # Amount of time spent on the essay,\n",
    "    })\n",
    "\n",
    "    # Flatten the multi-level column index\n",
    "    typing_features.columns = ['_'.join(col).strip() for col in typing_features.columns.values]\n",
    "\n",
    "    # Merge action time features with typing features\n",
    "    features = pd.merge(typing_features, action_time_features, on='id')\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Optimize the function to calculate top N frequencies and their magnitudes for each 'id' using groupby and apply\n",
    "def calculate_fft_features(group):\n",
    "\n",
    "    group['pos'] = group['cursor_position']%30\n",
    "    group['line'] = (group['cursor_position']/30).astype(int)\n",
    "\n",
    "    # Perform Fourier Transform on 'pos'\n",
    "    fft_values = fft(group['pos'])[1:]\n",
    "    \n",
    "    # Generate frequencies corresponding to the Fourier Transform values\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), 1)[1:]\n",
    "    \n",
    "    # Take absolute value to get magnitude\n",
    "    fft_magnitude = np.abs(fft_values)\n",
    "    \n",
    "    # Identify indices where the frequencies are positive\n",
    "    positive_indices = np.where(frequencies > 0)[0]\n",
    "    \n",
    "    # Filter out only positive frequencies and skip the zero frequency\n",
    "    frequencies = frequencies[positive_indices]\n",
    "    magnitudes = fft_magnitude[positive_indices]\n",
    "    \n",
    "    # Frequency Domain Features\n",
    "    peak_freq = frequencies[np.argmax(magnitudes)]\n",
    "    if np.sum(magnitudes) == 0:\n",
    "        mean_freq = 0  # or some other appropriate default value\n",
    "    else:\n",
    "        mean_freq = np.average(frequencies, weights=magnitudes)\n",
    "\n",
    "    median_freq = frequencies[len(magnitudes) // 2]\n",
    "    bandwidth = np.ptp(frequencies)\n",
    "    freq_skewness = scipy.stats.skew(magnitudes)\n",
    "    freq_kurtosis = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    # Other Features\n",
    "    total_energy = np.sum(magnitudes ** 2)\n",
    "    \n",
    "    # Spectral Entropy\n",
    "    psd_norm = np.abs(magnitudes) / np.sum(np.abs(magnitudes))\n",
    "    spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + np.finfo(float).eps))\n",
    "    \n",
    "    # Spectral Flatness\n",
    "    spectral_flatness = np.exp(np.mean(np.log(magnitudes + np.finfo(float).eps))) / np.mean(magnitudes)\n",
    "    \n",
    "    # Spectral Roll-off\n",
    "    spectral_sum = np.cumsum(magnitudes)\n",
    "    spectral_rolloff = frequencies[np.searchsorted(spectral_sum, 0.85 * spectral_sum[-1])]\n",
    "    \n",
    "    # Statistical Features\n",
    "    mean_amplitude = np.mean(magnitudes)\n",
    "    std_amplitude = np.std(magnitudes)\n",
    "    skew_amplitude = scipy.stats.skew(magnitudes)\n",
    "    kurtosis_amplitude = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    features = {\n",
    "        \"Peak Frequency\": peak_freq,\n",
    "        \"Mean Frequency\": mean_freq,\n",
    "        \"Median Frequency\": median_freq,\n",
    "        \"Bandwidth\": bandwidth,\n",
    "        \"Frequency Skewness\": freq_skewness,\n",
    "        \"Frequency Kurtosis\": freq_kurtosis,\n",
    "        \"Total Energy\": total_energy,\n",
    "        \"Spectral Entropy\": spectral_entropy,\n",
    "        \"Spectral Flatness\": spectral_flatness,\n",
    "        \"Spectral Roll-off\": spectral_rolloff,\n",
    "        \"Mean Amplitude\": mean_amplitude,\n",
    "        \"Std Amplitude\": std_amplitude,\n",
    "        \"Skew Amplitude\": skew_amplitude,\n",
    "        \"Kurtosis Amplitude\": kurtosis_amplitude\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)\n",
    "\n",
    "def apply_fft_feats(df):\n",
    "    return df.groupby('id').apply(calculate_fft_features)\n",
    "\n",
    "##### MAKE SURE FUNC LIST IS UPDATED BEFORE RUNNING/PASTING TO NOTEBOOK #####\n",
    "func_list = [raw_aggregation_functions, apply_fft_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3f5cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:59:01.280083Z",
     "iopub.status.busy": "2023-10-15T21:59:01.279610Z",
     "iopub.status.idle": "2023-10-15T21:59:01.288982Z",
     "shell.execute_reply": "2023-10-15T21:59:01.288017Z"
    },
    "papermill": {
     "duration": 0.01547,
     "end_time": "2023-10-15T21:59:01.290939",
     "exception": false,
     "start_time": "2023-10-15T21:59:01.275469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(df, df_test, func_list, scale_needed=True, run_local=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the input dataframes for training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = pd.DataFrame({'id': df['id'].unique()})\n",
    "    y_train = df.groupby('id')['score'].first().reset_index(drop=True)\n",
    "    X_test = pd.DataFrame({'id': df_test['id'].unique()})\n",
    "\n",
    "    # Feature engineering\n",
    "    agg_train_list = []\n",
    "    agg_test_list = []\n",
    "\n",
    "    # Aggregate and collect features without merging\n",
    "    for func in func_list:\n",
    "        agg_train = func(df)\n",
    "        agg_test = func(df_test)\n",
    "\n",
    "        # Check if the index name is 'id'\n",
    "        if agg_train.index.name != 'id' or agg_test.index.name != 'id':\n",
    "            raise ValueError(\"The index must be 'id' for aggregation functions.\")\n",
    "        \n",
    "        agg_train_list.append(agg_train)\n",
    "        agg_test_list.append(agg_test)\n",
    "\n",
    "    # Concatenate all aggregated features horizontally, aligned by index\n",
    "    agg_train = pd.concat(agg_train_list, axis=1)\n",
    "    agg_test = pd.concat(agg_test_list, axis=1)\n",
    "\n",
    "    # Reset index before merge\n",
    "    agg_train.reset_index(inplace=True)\n",
    "    agg_test.reset_index(inplace=True)\n",
    "\n",
    "    # Perform a single merge operation\n",
    "    X_train = X_train.merge(agg_train, on='id', how='left')\n",
    "    X_test = X_test.merge(agg_test, on='id', how='left')\n",
    "\n",
    "    # Remove 'id' column before scaling\n",
    "    X_train_ids = X_train['id']\n",
    "    X_train = X_train.drop(columns=['id'])\n",
    "    X_test_ids = X_test['id']\n",
    "    X_test = X_test.drop(columns=['id'])\n",
    "\n",
    "    # Save X_train before scaling if run_local=True\n",
    "    if run_local:\n",
    "        X_train.to_csv('X_train.csv', index=False)\n",
    "        y_train.to_csv('y_train.csv', index=False)\n",
    "\n",
    "    # Standardize features\n",
    "    if scale_needed:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_train_ids, y_train, X_test, X_test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c50318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:59:01.298398Z",
     "iopub.status.busy": "2023-10-15T21:59:01.298031Z",
     "iopub.status.idle": "2023-10-15T21:59:01.303733Z",
     "shell.execute_reply": "2023-10-15T21:59:01.302598Z"
    },
    "papermill": {
     "duration": 0.01179,
     "end_time": "2023-10-15T21:59:01.305902",
     "exception": false,
     "start_time": "2023-10-15T21:59:01.294112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains a regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Lasso(alpha=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7861ba09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:59:01.313167Z",
     "iopub.status.busy": "2023-10-15T21:59:01.312766Z",
     "iopub.status.idle": "2023-10-15T21:59:01.317785Z",
     "shell.execute_reply": "2023-10-15T21:59:01.316873Z"
    },
    "papermill": {
     "duration": 0.010965,
     "end_time": "2023-10-15T21:59:01.319717",
     "exception": false,
     "start_time": "2023-10-15T21:59:01.308752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(model, X_test):\n",
    "    \"\"\"\n",
    "    Makes predictions using the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950b1d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:59:01.327547Z",
     "iopub.status.busy": "2023-10-15T21:59:01.327208Z",
     "iopub.status.idle": "2023-10-15T21:59:01.332501Z",
     "shell.execute_reply": "2023-10-15T21:59:01.331626Z"
    },
    "papermill": {
     "duration": 0.011608,
     "end_time": "2023-10-15T21:59:01.334283",
     "exception": false,
     "start_time": "2023-10-15T21:59:01.322675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(X_test_ids, preds):\n",
    "    try:\n",
    "        # Check if X_test_ids and preds have the same length\n",
    "        if len(X_test_ids) != len(preds):\n",
    "            raise ValueError(\"The lengths of X_test_ids and preds must match.\")\n",
    "        \n",
    "        # Create a submission file\n",
    "        submission = pd.DataFrame({'id': X_test_ids, 'score': preds})\n",
    "        submission.to_csv('./submission.csv', index=False)\n",
    "        print('Submitted')\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f3a37e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T21:59:01.341802Z",
     "iopub.status.busy": "2023-10-15T21:59:01.341401Z",
     "iopub.status.idle": "2023-10-15T21:59:21.376692Z",
     "shell.execute_reply": "2023-10-15T21:59:21.375028Z"
    },
    "papermill": {
     "duration": 20.041395,
     "end_time": "2023-10-15T21:59:21.378669",
     "exception": true,
     "start_time": "2023-10-15T21:59:01.337274",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted\n"
     ]
    }
   ],
   "source": [
    "X_train, X_train_ids, y_train, X_test, X_test_ids = preprocessing(df, df_test, func_list, scale_needed=True, run_local=run_local)\n",
    "model = train(X_train, y_train)\n",
    "preds = predict(model, X_test)\n",
    "submit(X_test_ids, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.59591,
   "end_time": "2023-10-15T21:59:22.203766",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-15T21:58:39.607856",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
