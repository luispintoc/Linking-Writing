{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/code/awqatak/silver-bullet-single-model-165-features#Polars-FE-&-Helper-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kaggle = False\n",
    "\n",
    "if is_kaggle:\n",
    "    base_dir = '/kaggle/input'\n",
    "    data_dir = f'{base_dir}/linking-writing-processes-to-writing-quality'\n",
    "    output_dir = '/kaggle/working'\n",
    "else:\n",
    "    base_dir = '../'\n",
    "    data_dir = f'{base_dir}/data'\n",
    "    models_dir = f'{base_dir}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars FE & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter((pl.col('time_diff') > 3) & (pl.col('time_diff') < 5)).count(),\n",
    "                                   pauses_5_sec = pl.col('time_diff').filter(pl.col('time_diff') > 5).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "\n",
    "    print(\"< Cursor moving animation >\")\n",
    "\n",
    "    # Adding 'pos' and 'line' columns\n",
    "    df = df.with_columns([\n",
    "        (pl.col('cursor_position') % 30).alias('pos'),\n",
    "        (pl.col('cursor_position') / 30).cast(pl.Int32).alias('line')\n",
    "    ])\n",
    "\n",
    "    # Adding 'dist_moved' column\n",
    "    df = df.with_columns(\n",
    "        [pl.col('cursor_position').diff().over('id').fill_null(0).abs().alias('dist_moved')]\n",
    "    )\n",
    "\n",
    "    # Calculating average distance moved per event\n",
    "    avg_dist_per_event = df.groupby('id').agg(\n",
    "        pl.mean('dist_moved').alias('avg_dist_per_event')\n",
    "    )\n",
    "\n",
    "    # Joining with the feature set\n",
    "    feats = feats.join(avg_dist_per_event, on='id', how='left')\n",
    "\n",
    "    # Adding 'line_change' column\n",
    "    df = df.with_columns(\n",
    "        [pl.col('line').diff().over('id').fill_null(0).ne(0).alias('line_change')]\n",
    "    )\n",
    "\n",
    "    # Calculating revisits per line and first line revisits\n",
    "    revisits_per_line = df.filter(pl.col('line_change') == False).group_by(['id', 'line']).agg(\n",
    "        pl.count().alias('revisit_count')\n",
    "    )\n",
    "    first_line_revisits = revisits_per_line.filter(pl.col('line') == 0).group_by('id').agg(\n",
    "        pl.sum('revisit_count').alias('first_line_revisits')\n",
    "    )\n",
    "\n",
    "    # Joining with the feature set\n",
    "    feats = feats.join(first_line_revisits, on='id', how='left')\n",
    "\n",
    "    # Calculating line changes\n",
    "    line_changes = df.groupby('id').agg(\n",
    "        pl.sum('line_change').alias('line_changes')\n",
    "    )\n",
    "\n",
    "    # Joining with the feature set\n",
    "    feats = feats.join(line_changes, on='id', how='left')\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas FE & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum'] #, 'std', 'var', 'sem']\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "\n",
    "def calculate_relative_paragraph_sizes(input_df):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    df['total_paragraphs'] = df['paragraph'].apply(len)\n",
    "\n",
    "    df['relative_intro_size'] = 1 / df['total_paragraphs']  # First paragraph is the introduction\n",
    "    df['relative_body_size'] = (df['total_paragraphs'] - 2) / df['total_paragraphs']  # Middle paragraphs are the body\n",
    "    df['relative_conclusion_size'] = 1 / df['total_paragraphs']  # Last paragraph is the conclusion\n",
    "\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "\n",
    "    df['word_count_intro'] = df['paragraph_word_count'].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    df['word_count_body'] = df['paragraph_word_count'].apply(lambda x: sum(x[1:-1]) if len(x) > 2 else 0)\n",
    "    df['word_count_conclusion'] = df['paragraph_word_count'].apply(lambda x: x[-1] if len(x) > 1 else 0)\n",
    "\n",
    "    df['total_word_count'] = df['paragraph_word_count'].apply(sum)\n",
    "\n",
    "    df['intro_ratio'] = df['word_count_intro'] / df['total_word_count']\n",
    "    df['body_ratio'] = df['word_count_body'] / df['total_word_count']\n",
    "    df['conclusion_ratio'] = df['word_count_conclusion'] / df['total_word_count']\n",
    "\n",
    "    df['intro_body_ratio'] = df['word_count_intro'] / df['word_count_body']\n",
    "    df['intro_conclusion_ratio'] = df['word_count_intro'] / df['word_count_conclusion']\n",
    "    df['body_conclusion_ratio'] = df['word_count_body'] / df['word_count_conclusion']\n",
    "\n",
    "    df.drop(columns=['paragraph', 'total_paragraphs', 'essay', 'paragraph_word_count', 'word', 'sent'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]\n",
    "\n",
    "def count_pauses_2s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 2000).sum()\n",
    "\n",
    "def count_pauses_5s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 5000).sum()\n",
    "\n",
    "def pause_proportion_2s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def pause_proportion_5s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def mean_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.mean() / 1000 if not pauses.empty else 0\n",
    "\n",
    "def median_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.median() / 1000 if not pauses.empty else 0\n",
    "\n",
    "# Method to aggregate all pause-related features\n",
    "def aggregate_pause_features(df):\n",
    "    \"\"\"Aggregates all pause-related features for each essay.\"\"\"\n",
    "    grouped = df.groupby('id')\n",
    "    pause_features = pd.DataFrame()\n",
    "    pause_features['n_pauses_2s'] = grouped.apply(count_pauses_2s)\n",
    "    pause_features['n_pauses_5s'] = grouped.apply(count_pauses_5s)\n",
    "    pause_features['pause_proportion_2s'] = grouped.apply(pause_proportion_2s)\n",
    "    pause_features['pause_proportion_5s'] = grouped.apply(pause_proportion_5s)\n",
    "    pause_features['mean_pause_length'] = grouped.apply(mean_pause_length)\n",
    "    pause_features['median_pause_length'] = grouped.apply(median_pause_length)\n",
    "    return pause_features\n",
    "\n",
    "def process_variance(group):\n",
    "    \"\"\"Calculates the variance in the writing process over time for each essay.\"\"\"\n",
    "    if len(group) < 2:  # Handling for groups with a single row\n",
    "        return 0\n",
    "\n",
    "    bins = np.linspace(group['down_time'].min(), group['up_time'].max(), 11)\n",
    "    divisions = pd.cut(group['down_time'], bins=bins, include_lowest=True, labels=range(1, 11))\n",
    "    production_deciles = group.groupby(divisions).agg(n_events=('event_id', 'count'))\n",
    "    return np.std(production_deciles['n_events'], ddof=1)\n",
    "\n",
    "def aggregate_process_variance(df):\n",
    "    \"\"\"Aggregates the process variance feature for each essay.\"\"\"\n",
    "    return df.groupby('id').apply(process_variance).rename('process_variance').to_frame()\n",
    "\n",
    "def calculate_segment_visits(train_logs, train_essays):\n",
    "    # Calculate the end position of the intro and body for each essay\n",
    "    train_essays['intro_end'] = train_essays['essay'].apply(lambda x: len(x.split('\\n')[0]))\n",
    "    train_essays['body_end'] = train_essays['essay'].apply(lambda x: len('\\n'.join(x.split('\\n')[:-1])))\n",
    "\n",
    "    # Create a dictionary for quick lookup\n",
    "    ends_dict = train_essays.set_index('id')[['intro_end', 'body_end']].to_dict('index')\n",
    "\n",
    "    # Function to categorize position\n",
    "    def categorize_position(row):\n",
    "        intro_end = ends_dict[row['id']]['intro_end']\n",
    "        body_end = ends_dict[row['id']]['body_end']\n",
    "        if row['cursor_position'] <= intro_end:\n",
    "            return 'intro'\n",
    "        elif row['cursor_position'] <= body_end:\n",
    "            return 'body'\n",
    "        else:\n",
    "            return 'conclusion'\n",
    "\n",
    "    # Categorize cursor positions\n",
    "    train_logs['segment'] = train_logs.apply(categorize_position, axis=1)\n",
    "\n",
    "    # Count visits to each segment\n",
    "    segment_visits = train_logs.groupby(['id', 'segment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    segment_visits.columns = [f'{col}_visits' for col in segment_visits.columns]\n",
    "\n",
    "    return segment_visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Cursor moving animation >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 192\n"
     ]
    }
   ],
   "source": [
    "# train_logs    = pl.scan_csv(data_dir + '/train_logs.csv')\n",
    "train_logs    = pl.scan_csv('./train_logs_corrected.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(calculate_relative_paragraph_sizes(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(aggregate_pause_features(train_logs).reset_index(), on='id', how='left')\n",
    "train_feats            = train_feats.merge(aggregate_process_variance(train_logs).reset_index(), on='id', how='left')\n",
    "train_feats            = train_feats.merge(calculate_segment_visits(train_logs, train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv(data_dir + '/train_scores.csv')\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaNs: []\n",
      "Columns with all infinite values: []\n",
      "Columns with more than 90% of the same value: ['cursor_position_min']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def filter_columns(df):\n",
    "    # Initialize lists to store column names for each condition\n",
    "    all_nan_columns = []\n",
    "    all_inf_columns = []\n",
    "    single_value_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for col in df.columns:\n",
    "        # Check if the column contains numeric data\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check for all NaN values\n",
    "            if df[col].isna().all():\n",
    "                all_nan_columns.append(col)\n",
    "            else:\n",
    "                # Check for all infinite values (positive or negative infinity)\n",
    "                if np.isinf(df[col]).all():\n",
    "                    all_inf_columns.append(col)\n",
    "                else:\n",
    "                    # Check for more than 90% of columns with only one unique value\n",
    "                    unique_value_count = df[col].nunique()\n",
    "                    total_count = df.shape[0]\n",
    "                    if unique_value_count == 1 and (total_count - df[col].isna().sum()) / total_count > 0.8:\n",
    "                        single_value_columns.append(col)\n",
    "\n",
    "    return {\n",
    "        \"AllNaNColumns\": all_nan_columns,\n",
    "        \"AllInfColumns\": all_inf_columns,\n",
    "        \"SingleValueColumns\": single_value_columns\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame\n",
    "filtered_columns = filter_columns(data)\n",
    "print(\"Columns with all NaNs:\", filtered_columns[\"AllNaNColumns\"])\n",
    "print(\"Columns with all infinite values:\", filtered_columns[\"AllInfColumns\"])\n",
    "print(\"Columns with more than 90% of the same value:\", filtered_columns[\"SingleValueColumns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('silver_bullet_192feat_corrected.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
