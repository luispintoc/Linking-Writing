{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kaggle = False\n",
    "\n",
    "if is_kaggle:\n",
    "    base_dir = '/kaggle/input'\n",
    "    data_dir = f'{base_dir}/linking-writing-processes-to-writing-quality'\n",
    "    output_dir = '/kaggle/working'\n",
    "else:\n",
    "    base_dir = '../'\n",
    "    data_dir = f'{base_dir}/data'\n",
    "    models_dir = f'{base_dir}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = pl.scan_csv('../2_clean_data/train_logs_double_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'train_double_corr_786feats.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_logs_df = pd.read_csv(f'../kaggle feat_eng/train_logs_corrected.csv')\n",
    "# train_scores_df = pd.read_csv(f'{data_dir}/train_scores.csv')\n",
    "\n",
    "# test_logs_df = pd.read_csv(f'../kaggle feat_eng/test_logs_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_features(train_feats, temp_feats, suffix):\n",
    "    for col in temp_feats.columns:\n",
    "        if col != 'id':\n",
    "            temp_feats = temp_feats.rename(columns={col: col + suffix})\n",
    "    train_feats = train_feats.merge(temp_feats, on='id', how='left')\n",
    "    return train_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features related to counts and bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete'] #, 'Unidentified', 'Clear', 'PageUp', 'PageDown']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left')\n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.len(),\n",
    "                             input_word_length_mean = pl.col('text_change').map_elements(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').map_elements(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').map_elements(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').map_elements(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').map_elements(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').name.suffix('_sum'), pl.mean(num_cols).name.suffix('_mean'), pl.std(num_cols).name.suffix('_std'),\n",
    "                                 pl.median(num_cols).name.suffix('_median'), pl.min(num_cols).name.suffix('_min'), pl.max(num_cols).name.suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).name.suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "\n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last_distinct()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').name.suffix('_mean'), pl.std('P-bursts').name.suffix('_std'), pl.count('P-bursts').name.suffix('_count'),\n",
    "                                   pl.median('P-bursts').name.suffix('_median'), pl.max('P-bursts').name.suffix('_max'),\n",
    "                                   pl.first('P-bursts').name.suffix('_first'), pl.last('P-bursts').name.suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last_distinct()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').name.suffix('_mean'), pl.std('R-bursts').name.suffix('_std'), \n",
    "                                   pl.median('R-bursts').name.suffix('_median'), pl.max('R-bursts').name.suffix('_max'),\n",
    "                                   pl.first('R-bursts').name.suffix('_first'), pl.last('R-bursts').name.suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "for col in train_feats.columns:\n",
    "    if col != 'id':\n",
    "        train_feats = train_feats.rename(columns={col: col + '-count_bursts'})\n",
    "len(train_feats.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Essay Reconstruction >\n"
     ]
    }
   ],
   "source": [
    "train_logs = train_logs.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_essays = get_essay_df(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum', 'std'] #, q0, q4, 'sem', 'std', 'var', 'skew', pd.DataFrame.kurt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              essay\n",
       "0  001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...\n",
       "1  0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...\n",
       "2  0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...\n",
       "3  0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...\n",
       "4  0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idle time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feats = train_logs.groupby('id')['idle_time'].apply(lambda x: x.unique()[0]).reset_index()\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-idle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bursts v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_bursts(df):\n",
    "    df = df.sort_values(['id', 'event_id'])\n",
    "    df['next_down_time'] = df.groupby('id')['down_time'].shift(-1)\n",
    "    df['pause'] = df['next_down_time'] - df['up_time']\n",
    "    df['is_revision'] = ~df['activity'].isin(['Input', 'Nonproduction'])\n",
    "\n",
    "    bursts = []\n",
    "    for _, group in df.groupby('id'):\n",
    "        p_burst_1 = p_burst_2 = r_burst = 0\n",
    "        p_bursts_1 = []\n",
    "        p_bursts_2 = []\n",
    "        r_bursts = []\n",
    "        for _, row in group.iterrows():\n",
    "            if row['pause'] > 2000:  # End of a P-burst\n",
    "                p_bursts_1.append(p_burst_1)\n",
    "                p_burst_1 = 0\n",
    "            if row['pause'] > 5000:  # End of a P-burst\n",
    "                p_bursts_2.append(p_burst_2)\n",
    "                p_burst_2 = 0\n",
    "            if row['is_revision']:  # End of an R-burst\n",
    "                r_bursts.append(r_burst)\n",
    "                r_burst = 0\n",
    "            p_burst_1 += 1\n",
    "            p_burst_2 += 1\n",
    "            r_burst += 1\n",
    "        p_bursts_1.append(p_burst_1)  # Last P-burst\n",
    "        p_bursts_2.append(p_burst_2)  # Last P-burst\n",
    "        r_bursts.append(r_burst)  # Last R-burst\n",
    "        bursts.append((group['id'].iloc[0], p_bursts_1, p_bursts_2, r_bursts))\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    bursts_df = pd.DataFrame(bursts, columns=['id', 'p_bursts_2sec', 'p_bursts_5sec', 'r_bursts'])\n",
    "\n",
    "    return bursts_df\n",
    "\n",
    "bursts_df = calculate_bursts(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantile functions\n",
    "q1 = lambda x: np.quantile(x, 0.25) if x else np.nan\n",
    "q3 = lambda x: np.quantile(x, 0.75) if x else np.nan\n",
    "\n",
    "# Define the first and last functions\n",
    "first = lambda x: x[0] if x else np.nan\n",
    "last = lambda x: x[-1] if x else np.nan\n",
    "\n",
    "# Define the aggregations\n",
    "AGGS = [len, np.mean, min, max, first, last, q1, np.median, q3, sum, np.std]\n",
    "\n",
    "# Apply the function to each cell in the 'p_bursts_2sec', 'p_bursts_5sec', and 'r_bursts' columns\n",
    "for col in ['p_bursts_2sec', 'p_bursts_5sec', 'r_bursts']:\n",
    "    for i, agg in enumerate(AGGS):\n",
    "        bursts_df[f'{col}_agg{i}'] = bursts_df[col].apply(lambda x: agg(x) if x else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts_df = bursts_df.drop(['p_bursts_2sec','p_bursts_5sec','r_bursts'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157\n"
     ]
    }
   ],
   "source": [
    "train_feats = append_features(train_feats, bursts_df, suffix='-bursts_v2')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bursts v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_bursts(df):\n",
    "#     df = df.sort_values(['id', 'event_id'])\n",
    "#     df['next_down_time'] = df.groupby('id')['down_time'].shift(-1)\n",
    "#     df['pause'] = df['next_down_time'] - df['up_time']\n",
    "\n",
    "#     df['is_input'] = df['activity'].isin(['Input', 'Nonproduction'])\n",
    "#     df['is_input_shifted'] = df['is_input'].shift(-1) # Shift in the right direction\n",
    "\n",
    "#     p_bursts_exclu = []\n",
    "#     r_bursts_exclu = []\n",
    "#     p_bursts_not_exclu = []\n",
    "#     r_bursts_not_exclu = []\n",
    "#     for _, group in df.groupby('id'):\n",
    "#         r_burst = p_burst = burst_size = 0\n",
    "#         for _, row in group.iterrows():\n",
    "#             if row['pause'] > 2000:  # End of a P-burst\n",
    "#                 p_bursts_exclu.append(burst_size)\n",
    "#                 p_bursts_not_exclu.append(p_burst)\n",
    "#                 burst_size  = 0\n",
    "#                 p_burst = 0\n",
    "#             if not row['is_input'] and row['is_input_shifted']:\n",
    "#                 r_bursts_exclu.append(burst_size)\n",
    "#                 r_bursts_not_exclu.append(r_burst)\n",
    "#                 burst_size = 0\n",
    "#                 r_burst = 0\n",
    "\n",
    "#             burst_size += 1\n",
    "#             r_burst += 1\n",
    "#             p_burst += 1\n",
    "           \n",
    "#         p_bursts_not_exclu.append(p_burst)  # Last P-burst\n",
    "#         r_bursts_not_exclu.append(r_burst)  # Last R-burst\n",
    "\n",
    "#     # Create a DataFrame from the lists\n",
    "#     bursts_df = pd.DataFrame({\n",
    "#         'id': df['id'].unique(),\n",
    "#         'p_bursts_exclu': p_bursts_exclu,\n",
    "#         'r_bursts_exclu': r_bursts_exclu,\n",
    "#         'p_bursts_not_exclu': p_bursts_not_exclu,\n",
    "#         'r_bursts_not_exclu': r_bursts_not_exclu\n",
    "#     })\n",
    "\n",
    "#     # df_agg = df[['id','p_bursts_exclu', 'r_bursts_exclu', 'p_bursts_not_exclu', 'r_bursts_not_exclu']].groupby(['id']).agg(AGGREGATIONS)\n",
    "#     # df_agg.columns = ['_'.join(x) for x in df_agg.columns]\n",
    "#     # df_agg['id'] = df_agg.index\n",
    "#     # df_agg = df_agg.reset_index(drop=True)\n",
    "#     # df_agg.drop(columns=['p_bursts_exclu', 'r_bursts_exclu', 'p_bursts_not_exclu', 'r_bursts_not_exclu'], inplace=True)\n",
    "\n",
    "\n",
    "#     return bursts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = calculate_bursts(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = pd.read_csv('../0_data/train_scores.csv')\n",
    "# x = x.merge(scores, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_feats = pd.DataFrame()\n",
    "# temp_feats['id'] = train_essays['id'].unique()\n",
    "# temp_feats = temp_feats.merge(calculate_bursts(train_logs), on='id', how='left')\n",
    "\n",
    "# train_feats = append_features(train_feats, temp_feats, suffix='-bursts_v3')\n",
    "# print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations at word/sentence/paragraph/essay level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q0(x):\n",
    "    return x.quantile(0.10)\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def q4(x):\n",
    "    return x.quantile(0.90)\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def essay_feats(df):\n",
    "    # Number of characters in paragraphs\n",
    "    df['essay_len'] = df['essay'].apply(lambda x: len(x))\n",
    "    # Number of words in paragraphs\n",
    "    df['essay_word_count'] = df['essay'].apply(lambda x: len(x.split(' ')))\n",
    "    return df[['id','essay_len','essay_word_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(essay_feats(train_essays), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-word_sent_parag_agg')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pressed keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-pressed_keys')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pauses_2s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 2000).sum()\n",
    "\n",
    "def count_pauses_5s(group):\n",
    "    \"\"\"Counts pauses longer than 5000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 5000).sum()\n",
    "\n",
    "def pause_proportion_2s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def pause_proportion_5s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 5000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def mean_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.mean() / 1000 if not pauses.empty else 0\n",
    "\n",
    "def median_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.median() / 1000 if not pauses.empty else 0\n",
    "\n",
    "# Method to aggregate all pause-related features\n",
    "def aggregate_pause_features(df):\n",
    "    \"\"\"Aggregates all pause-related features for each essay.\"\"\"\n",
    "    grouped = df.groupby('id')\n",
    "    pause_features = pd.DataFrame()\n",
    "    pause_features['n_pauses_2s'] = grouped.apply(count_pauses_2s)\n",
    "    pause_features['n_pauses_5s'] = grouped.apply(count_pauses_5s)\n",
    "    pause_features['pause_proportion_2s'] = grouped.apply(pause_proportion_2s)\n",
    "    pause_features['pause_proportion_5s'] = grouped.apply(pause_proportion_5s)\n",
    "    pause_features['mean_pause_length'] = grouped.apply(mean_pause_length)\n",
    "    pause_features['median_pause_length'] = grouped.apply(median_pause_length)\n",
    "    return pause_features\n",
    "\n",
    "def process_variance(group):\n",
    "    \"\"\"Calculates the variance in the writing process over time for each essay.\"\"\"\n",
    "    if len(group) < 2:  # Handling for groups with a single row\n",
    "        return 0\n",
    "\n",
    "    bins = np.linspace(group['down_time'].min(), group['up_time'].max(), 11)\n",
    "    divisions = pd.cut(group['down_time'], bins=bins, include_lowest=True, labels=range(1, 11))\n",
    "    production_deciles = group.groupby(divisions).agg(n_events=('event_id', 'count'))\n",
    "    return np.std(production_deciles['n_events'], ddof=1)\n",
    "\n",
    "def aggregate_process_variance(df):\n",
    "    \"\"\"Aggregates the process variance feature for each essay.\"\"\"\n",
    "    return df.groupby('id').apply(process_variance).rename('process_variance').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(aggregate_pause_features(train_logs).reset_index(), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(aggregate_process_variance(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-paussed_features')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cursor visits to different segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cursor_visits(df, aggregations=['count', 'sum', 'skew']):\n",
    "\n",
    "    feats = df[['id']].copy()\n",
    "    \n",
    "    # Adding 'pos' and 'line' columns\n",
    "    df['pos'] = df['cursor_position'] % 30\n",
    "    df['line'] = (df['cursor_position'] / 30).astype(int)\n",
    "\n",
    "    # Adding 'dist_moved' column\n",
    "    df['dist_moved'] = df.groupby('id')['cursor_position'].diff().fillna(0).abs()\n",
    "\n",
    "    # Calculating average distance moved per event\n",
    "    for agg in aggregations:\n",
    "        avg_dist_per_event = df.groupby('id')['dist_moved'].agg(agg).reset_index().rename(columns={'dist_moved': f'{agg}_dist_per_event'})\n",
    "        feats = feats.merge(avg_dist_per_event, on='id', how='left')\n",
    "\n",
    "    # Adding 'line_change' column\n",
    "    df['line_change'] = df.groupby('id')['line'].diff().fillna(0).ne(0)\n",
    "\n",
    "    # # Calculating revisits per line and first line revisits\n",
    "    # for agg in aggregations:\n",
    "    #     revisits_per_line = df[df['line_change'] == False].groupby(['id', 'line']).size().reset_index(name='revisit_count')\n",
    "    #     first_line_revisits = revisits_per_line[revisits_per_line['line'] == 0].groupby('id')['revisit_count'].agg(agg).reset_index().rename(columns={'revisit_count': f'{agg}_first_line_revisits'})\n",
    "    #     feats = feats.merge(first_line_revisits, on='id', how='left')\n",
    "\n",
    "    # Calculating line changes\n",
    "    line_changes = df.groupby('id')['line_change'].agg('count').reset_index().rename(columns={'line_change': 'count_line_changes'})\n",
    "    feats = feats.merge(line_changes, on='id', how='left')\n",
    "\n",
    "    total_lines = df.groupby('id')['line'].max().reset_index().rename(columns={'line': 'total_lines'})\n",
    "    df = df.merge(total_lines, on='id', how='left')\n",
    "    df['relative_line'] = df['line'] / df['total_lines']\n",
    "\n",
    "    # Create a new column 'relative_line_bucket' which categorizes 'relative_line' into 5 buckets\n",
    "    for agg in aggregations:\n",
    "        for bins in [3, 5, 10]:\n",
    "            df['relative_line_bucket'] = pd.cut(df['relative_line'], bins=bins, labels=False)\n",
    "\n",
    "            # Calculating agg time spent per line bucket\n",
    "            avg_time_per_line = df.groupby(['id', 'relative_line_bucket'])['action_time'].agg(agg).reset_index(name=f'{agg}_time_per_line_bucket')\n",
    "            avg_time_per_line_pivot = avg_time_per_line.pivot(index='id', columns='relative_line_bucket', values=f'{agg}_time_per_line_bucket').reset_index()\n",
    "            avg_time_per_line_pivot.columns = ['id'] + [f'{agg}_time_per_line_bucket_{i}/{[bins]}' for i in range(bins)]\n",
    "            feats = feats.merge(avg_time_per_line_pivot, on='id', how='left')\n",
    "\n",
    "\n",
    "    return feats.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(cursor_visits(train_logs), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-cursor_visits')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segments visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_segment_visits(train_logs, train_essays):\n",
    "    # Calculate the end position of the intro and body for each essay\n",
    "    train_essays['intro_end'] = train_essays['essay'].apply(lambda x: len(x.split('\\n')[0]))\n",
    "    train_essays['body_end'] = train_essays['essay'].apply(lambda x: len('\\n'.join(x.split('\\n')[:-1])))\n",
    "\n",
    "    # Create a dictionary for quick lookup\n",
    "    ends_dict = train_essays.set_index('id')[['intro_end', 'body_end']].to_dict('index')\n",
    "\n",
    "    # Function to categorize position\n",
    "    def categorize_position(row):\n",
    "        intro_end = ends_dict[row['id']]['intro_end']\n",
    "        body_end = ends_dict[row['id']]['body_end']\n",
    "        if row['cursor_position'] <= intro_end:\n",
    "            return 'intro'\n",
    "        elif row['cursor_position'] <= body_end:\n",
    "            return 'body'\n",
    "        else:\n",
    "            return 'conclusion'\n",
    "\n",
    "    # Categorize cursor positions\n",
    "    train_logs['segment'] = train_logs.apply(categorize_position, axis=1)\n",
    "\n",
    "    # Count visits to each segment\n",
    "    segment_visits = train_logs.groupby(['id', 'segment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    segment_visits.columns = [f'{col}_visits' for col in segment_visits.columns]\n",
    "\n",
    "    return segment_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(calculate_segment_visits(train_logs, train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-segments_visit')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative size of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_paragraph_sizes(input_df, essay_column):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    # Calculate the total number of paragraphs\n",
    "    df['total_paragraphs'] = df['paragraphs'].apply(len)\n",
    "\n",
    "    # Calculate the relative sizes\n",
    "    df['relative_body_size'] = df['total_paragraphs'].apply(lambda x: (x-2)/x if x >= 3 else 0)  # Middle paragraphs are the body\n",
    "\n",
    "    # Calculate the word count for each paragraph\n",
    "    df['paragraph_word_count'] = df['paragraphs'].apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "\n",
    "    # Separate paragraphs into intro, body, and conclusion\n",
    "    df['word_count_intro'] = df['paragraph_word_count'].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    df['word_count_body'] = df['paragraph_word_count'].apply(lambda x: sum(x[1:-1]) if len(x) > 2 else 0)\n",
    "    df['word_count_conclusion'] = df['paragraph_word_count'].apply(lambda x: x[-1] if len(x) > 1 else 0)\n",
    "\n",
    "    # Calculate total word count for each essay\n",
    "    df['total_word_count'] = df['paragraph_word_count'].apply(sum)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    df['intro_ratio'] = df['word_count_intro'] / df['total_word_count']\n",
    "    df['body_ratio'] = df['word_count_body'] / df['total_word_count']\n",
    "    df['conclusion_ratio'] = df['word_count_conclusion'] / df['total_word_count']\n",
    "    \n",
    "    df['intro_body_ratio'] = df['word_count_intro'] / df['word_count_body']\n",
    "    df['intro_conclusion_ratio'] = df['word_count_intro'] / df['word_count_conclusion']\n",
    "    df['body_conclusion_ratio'] = df['word_count_body'] / df['word_count_conclusion']\n",
    "\n",
    "    # Drop intermediate columns if needed\n",
    "    df.drop(columns=['word', 'sent', 'paragraph', 'paragraphs', essay_column, 'paragraph_word_count'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(calculate_relative_paragraph_sizes(train_essays, 'essay').reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-paragraph_ratios')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intro_feats(input_df, essay_column):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                    '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '[', ']']\n",
    "    \n",
    "    def count_punctuations(text, punctuations):\n",
    "        return {p: text.count(p) for p in punctuations}\n",
    "\n",
    "    punctuations_df = df['paragraphs'].apply(lambda x: count_punctuations(x[0], punctuations) if x else {p: 0 for p in punctuations})\n",
    "    punctuations_df = pd.DataFrame(punctuations_df.tolist())\n",
    "\n",
    "    df = pd.concat([df[['id']], punctuations_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(intro_feats(train_essays, 'essay').reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-intro_punctuation')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intro_aggs(input_df):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df['essay'].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "    df['intro'] = df['paragraphs'].apply(lambda x: x[0] if x else '')\n",
    "\n",
    "    def word_feats(df):\n",
    "        df['word'] = df['intro'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "        df = df.explode('word')\n",
    "        df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "        df = df[df['word_len'] != 0]\n",
    "\n",
    "        word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "        word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "        word_agg_df['id'] = word_agg_df.index\n",
    "        word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "        return word_agg_df\n",
    "\n",
    "    def sent_feats(df):\n",
    "        df['sent'] = df['intro'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        df = df.explode('sent')\n",
    "        df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "        # Number of characters in sentences\n",
    "        df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "        # Number of words in sentences\n",
    "        df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "        sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index\n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "        sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "        return sent_agg_df\n",
    "    \n",
    "    temp_1 = word_feats(df)\n",
    "    temp_2 = sent_feats(df)\n",
    "\n",
    "    return temp_1.merge(temp_2, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(intro_aggs(train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-intro_aggs')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion_feats(input_df, essay_column):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                    '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '[', ']']\n",
    "    \n",
    "    def count_punctuations(text, punctuations):\n",
    "        return {p: text.count(p) for p in punctuations}\n",
    "\n",
    "    punctuations_df = df['paragraphs'].apply(lambda x: count_punctuations(x[-1], punctuations) if x else {p: 0 for p in punctuations})\n",
    "    punctuations_df = pd.DataFrame(punctuations_df.tolist())\n",
    "\n",
    "    df = pd.concat([df[['id']], punctuations_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(intro_feats(train_essays, 'essay').reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-conclusion_punctuation')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion_aggs(input_df):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df['essay'].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "    df['intro'] = df['paragraphs'].apply(lambda x: x[-1] if x else '')\n",
    "\n",
    "    def word_feats(df):\n",
    "        df['word'] = df['intro'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "        df = df.explode('word')\n",
    "        df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "        df = df[df['word_len'] != 0]\n",
    "\n",
    "        word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "        word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "        word_agg_df['id'] = word_agg_df.index\n",
    "        word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "        return word_agg_df\n",
    "\n",
    "    def sent_feats(df):\n",
    "        df['sent'] = df['intro'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        df = df.explode('sent')\n",
    "        df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "        # Number of characters in sentences\n",
    "        df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "        # Number of words in sentences\n",
    "        df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "        sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index\n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "        sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "        return sent_agg_df\n",
    "    \n",
    "    temp_1 = word_feats(df)\n",
    "    temp_2 = sent_feats(df)\n",
    "\n",
    "    return temp_1.merge(temp_2, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(conclusion_aggs(train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-conclusion_aggs')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_feats(input_df, essay_column):\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                    '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '[', ']']\n",
    "\n",
    "    def count_punctuations(paragraphs, punctuations):\n",
    "        counts = {p: 0 for p in punctuations}\n",
    "        for paragraph in paragraphs:\n",
    "            for p in punctuations:\n",
    "                counts[p] += paragraph.count(p)\n",
    "        return counts\n",
    "\n",
    "    punctuations_df = df['paragraphs'].apply(lambda x: count_punctuations(x[1:-1], punctuations) if len(x) >= 3 else {p: 0 for p in punctuations})\n",
    "    punctuations_df = pd.DataFrame(punctuations_df.tolist())\n",
    "\n",
    "    df = pd.concat([df[['id']], punctuations_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(body_feats(train_essays, 'essay').reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-body_punctuation')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_aggs(input_df):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df['essay'].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "    df['body'] = df['paragraphs'].apply(lambda x: x[1:-1] if len(x) >= 3 else [])\n",
    "\n",
    "    def word_feats(df):\n",
    "        df['word'] = df['body'].apply(lambda paragraphs: [word for paragraph in paragraphs for word in re.split(' |\\\\n|\\\\.|\\\\?|\\\\!', paragraph)] if paragraphs else [])\n",
    "        df = df.explode('word')\n",
    "        df['word_len'] = df['word'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "        df = df[df['word_len'] != 0]\n",
    "\n",
    "        word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "        word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "        word_agg_df['id'] = word_agg_df.index\n",
    "        word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "        return word_agg_df\n",
    "\n",
    "    def sent_feats(df):\n",
    "        df['sent'] = df['body'].apply(lambda paragraphs: [sent for paragraph in paragraphs for sent in re.split('\\\\.|\\\\?|\\\\!', paragraph)] if paragraphs else [])\n",
    "        df = df.explode('sent')\n",
    "        df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip() if pd.notna(x) else '')\n",
    "        # Number of characters in sentences\n",
    "        df['sent_len'] = df['sent'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "        # Number of words in sentences\n",
    "        df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "        sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index\n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "        sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "        return sent_agg_df\n",
    "    \n",
    "    temp_1 = word_feats(df)\n",
    "    temp_2 = sent_feats(df)\n",
    "\n",
    "    return temp_1.merge(temp_2, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(body_aggs(train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-body_aggs')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Customer tokenizer\n",
    "# def custom_tokenizer(text):\n",
    "#     words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "#     filtered_words = [word for word in words if word.count('q') < 11] # Do not consider words with more than 10 q's\n",
    "#     return filtered_words\n",
    "\n",
    "# tfidf_vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,2))\n",
    "\n",
    "# def make_text_features(self, df, column='text_change', fit_transform=True):\n",
    "#     \"\"\"Extracts text features using CountVectorizer and TfidfVectorizer, along with custom features.\"\"\"\n",
    "#     # Filter and concatenate text changes\n",
    "#     filtered_df = df[(~df[column].str.contains('=>')) & (df[column] != 'NoChange')]\n",
    "#     concatenated_texts = filtered_df.groupby('id')[column].apply(' '.join).reset_index()\n",
    "\n",
    "#     # Apply CountVectorizer and TfidfVectorizer\n",
    "#     if fit_transform:\n",
    "#         bow_features = self.count_vect.fit_transform(concatenated_texts[column])\n",
    "#         tfidf_features = self.tfidf_vect.fit_transform(concatenated_texts[column])\n",
    "#     else:\n",
    "#         bow_features = self.count_vect.transform(concatenated_texts[column])\n",
    "#         tfidf_features = self.tfidf_vect.transform(concatenated_texts[column])\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     bow_df = pd.DataFrame(bow_features.toarray(), columns=[f'bow_{name}' for name in self.count_vect.get_feature_names_out()])\n",
    "#     tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{name}' for name in self.tfidf_vect.get_feature_names_out()])\n",
    "\n",
    "#     # Custom Feature: Length of each essay\n",
    "#     custom_features_df = pd.DataFrame({'custom_length': concatenated_texts[column].apply(len)})\n",
    "\n",
    "#     # Merge all features\n",
    "#     merged_features = pd.concat([concatenated_texts[['id']], bow_df, tfidf_df, custom_features_df], axis=1)\n",
    "#     return merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gap list for time gap features\n",
    "gap_list = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "def compute_time_gaps(df, gap_list):\n",
    "    \"\"\"Computes time gaps between events for a list of specified gaps.\"\"\"\n",
    "    for gap in gap_list:\n",
    "        df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "        df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "\n",
    "    time_gap_cols = [f'action_time_gap{gap}' for gap in gap_list]\n",
    "    return df[['id'] + time_gap_cols].groupby('id').agg(['mean', 'std', 'min', 'max', 'median', 'sum', 'skew'])\n",
    "\n",
    "\n",
    "def compute_cursor_position_change_features(df, gap_list, aggregations):\n",
    "    \"\"\"Computes cursor position change features for specified gaps per 'id'.\"\"\"\n",
    "    result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for gap in gap_list:\n",
    "        col_shift = f'cursor_position_shift{gap}'\n",
    "        df[col_shift] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "        df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[col_shift]\n",
    "        df[f'cursor_position_abs_change{gap}'] = abs(df[f'cursor_position_change{gap}'])\n",
    "        # Aggregate the results per 'id'\n",
    "        id_features = df.groupby('id').agg({\n",
    "            f'cursor_position_change{gap}': aggregations,\n",
    "            f'cursor_position_abs_change{gap}': aggregations\n",
    "        }).reset_index()\n",
    "        result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "    result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_word_count_change_features(df, gap_list, aggregations):\n",
    "    \"\"\"Computes word count change features for specified gaps per 'id'.\"\"\"\n",
    "    result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for gap in gap_list:\n",
    "        col_shift = f'word_count_shift{gap}'\n",
    "        df[col_shift] = df.groupby('id')['word_count'].shift(gap)\n",
    "        df[f'word_count_change{gap}'] = df['word_count'] - df[col_shift]\n",
    "        df[f'word_count_abs_change{gap}'] = abs(df[f'word_count_change{gap}'])\n",
    "        # Aggregate the results per 'id'\n",
    "        id_features = df.groupby('id').agg({\n",
    "            f'word_count_change{gap}': aggregations,  # You can choose different aggregation functions as needed\n",
    "            f'word_count_abs_change{gap}': aggregations\n",
    "        }).reset_index()\n",
    "        result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "    result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "# Compute time gap features\n",
    "time_gap_features = compute_time_gaps(train_logs, gap_list)\n",
    "time_gap_features.columns = ['{}_{}'.format(col1, col2) for col1, col2 in time_gap_features.columns]\n",
    "time_gap_features = time_gap_features.reset_index()\n",
    "\n",
    "train_feats = append_features(train_feats, time_gap_features, suffix='-time_gaps')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "\n",
    "temp = compute_cursor_position_change_features(train_logs, gap_list, AGGREGATIONS)\n",
    "temp.columns = ['_'.join(col).strip() for col in temp.columns.values]\n",
    "temp_feats = temp_feats.merge(temp, left_on='id', right_on='id_', how='left').drop('id_', axis=1)\n",
    "\n",
    "temp = compute_word_count_change_features(train_logs, gap_list, AGGREGATIONS)\n",
    "temp.columns = ['_'.join(col).strip() for col in temp.columns.values]\n",
    "temp_feats = temp_feats.merge(temp, left_on='id', right_on='id_', how='left').drop('id_', axis=1)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-cursor_word_changes')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_punctuations(df):\n",
    "    \"\"\"Counts the number of punctuation marks used in each essay.\"\"\"\n",
    "    punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                    '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '[', ']']\n",
    "\n",
    "    # Filter the DataFrame to include only rows with punctuation events\n",
    "    punctuation_df = df[df['down_event'].isin(punctuations)]\n",
    "    punctuation_df = df[~df['down_event'].isin(['\"','#','$','%',')','*',';','<','?','@','^'])] # Not useful characters\n",
    "\n",
    "    # Group by 'id' and 'down_event' and count the occurrences of each punctuation\n",
    "    punctuation_counts = punctuation_df.groupby(['id', 'down_event'])['down_event'].count().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate the total punctuation count for each 'id'\n",
    "    total_punctuation_counts = punctuation_counts.sum(axis=1)\n",
    "\n",
    "    # Add the total count as a new column\n",
    "    punctuation_counts['Total'] = total_punctuation_counts\n",
    "\n",
    "    return punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(match_punctuations(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-punctuation')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the function to calculate top N frequencies and their magnitudes for each 'id' using groupby and apply\n",
    "def calculate_fft_features(group):\n",
    "\n",
    "    from scipy.fft import fft\n",
    "    import scipy\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy.signal import savgol_filter\n",
    "\n",
    "    group['pos'] = group['cursor_position']%30\n",
    "    group['line'] = (group['cursor_position']/30).astype(int)\n",
    "\n",
    "    # Perform Fourier Transform on 'pos'\n",
    "    fft_values = fft(group['pos'])[1:]\n",
    "    \n",
    "    # Generate frequencies corresponding to the Fourier Transform values\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), 1)[1:]\n",
    "    \n",
    "    # Take absolute value to get magnitude\n",
    "    fft_magnitude = np.abs(fft_values)\n",
    "    \n",
    "    # Identify indices where the frequencies are positive\n",
    "    positive_indices = np.where(frequencies > 0)[0]\n",
    "    \n",
    "    # Filter out only positive frequencies and skip the zero frequency\n",
    "    frequencies = frequencies[positive_indices]\n",
    "    magnitudes = fft_magnitude[positive_indices]\n",
    "    \n",
    "    # Frequency Domain Features\n",
    "    peak_freq = frequencies[np.argmax(magnitudes)]\n",
    "    mean_freq = np.average(frequencies, weights=magnitudes)\n",
    "    median_freq = frequencies[len(magnitudes) // 2]\n",
    "    bandwidth = np.ptp(frequencies)\n",
    "    freq_skewness = scipy.stats.skew(magnitudes)\n",
    "    freq_kurtosis = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    # Other Features\n",
    "    total_energy = np.sum(magnitudes ** 2)\n",
    "    \n",
    "    # Spectral Entropy\n",
    "    psd_norm = np.abs(magnitudes) / np.sum(np.abs(magnitudes))\n",
    "    spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + np.finfo(float).eps))\n",
    "    \n",
    "    # Spectral Flatness\n",
    "    spectral_flatness = np.exp(np.mean(np.log(magnitudes + np.finfo(float).eps))) / np.mean(magnitudes)\n",
    "    \n",
    "    # Spectral Roll-off\n",
    "    spectral_sum = np.cumsum(magnitudes)\n",
    "    spectral_rolloff = frequencies[np.searchsorted(spectral_sum, 0.85 * spectral_sum[-1])]\n",
    "    \n",
    "    # Statistical Features\n",
    "    mean_amplitude = np.mean(magnitudes)\n",
    "    std_amplitude = np.std(magnitudes)\n",
    "    skew_amplitude = scipy.stats.skew(magnitudes)\n",
    "    kurtosis_amplitude = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    # def exponential_decay(t, A, B, C):\n",
    "    #     return A * np.exp(-B * t) + C\n",
    "\n",
    "    # # Fit curve to FFT\n",
    "    # try:\n",
    "    #     popt, pcov = curve_fit(exponential_decay, frequencies, savgol_filter(magnitudes, 30, 3))\n",
    "    #     A, B, C = popt\n",
    "    # except:\n",
    "    #     A, B, C = 0, 0, 0\n",
    "\n",
    "    features = {\n",
    "        # \"Fit Amplitude\": A,\n",
    "        # \"Fit Decay\": B,\n",
    "        # \"Fit Offset\": C,\n",
    "        # \"Magnitudes\": magnitudes,\n",
    "        # \"Frequencies\": frequencies,\n",
    "        \"Peak Frequency\": peak_freq,\n",
    "        \"Mean Frequency\": mean_freq,\n",
    "        \"Median Frequency\": median_freq,\n",
    "        \"Bandwidth\": bandwidth,\n",
    "        \"Frequency Skewness\": freq_skewness,\n",
    "        \"Frequency Kurtosis\": freq_kurtosis,\n",
    "        \"Total Energy\": total_energy,\n",
    "        \"Spectral Entropy\": spectral_entropy,\n",
    "        \"Spectral Flatness\": spectral_flatness,\n",
    "        \"Spectral Roll-off\": spectral_rolloff,\n",
    "        \"Mean Amplitude\": mean_amplitude,\n",
    "        \"Std Amplitude\": std_amplitude,\n",
    "        \"Skew Amplitude\": skew_amplitude,\n",
    "        \"Kurtosis Amplitude\": kurtosis_amplitude\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978\n"
     ]
    }
   ],
   "source": [
    "temp_feats = train_logs.groupby('id').apply(calculate_fft_features).reset_index()\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-fft')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard / mouse feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyboard_mouse_feats(train_logs_df):\n",
    "\n",
    "    # Creates two shift variables which lag the original variable by 1 and 2 periods respectively.\n",
    "    event_df = train_logs_df[['id', 'event_id', 'down_event']].copy(deep=True)\n",
    "\n",
    "    event_df['down_event_shift_1'] = event_df['down_event'].shift(periods=1)\n",
    "\n",
    "    event_df = event_df[['id', 'event_id', 'down_event_shift_1', 'down_event']]\n",
    "\n",
    "    ctrl_x_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'x')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "\n",
    "    # Creating a DataFrame that contains all counts at an id level.\n",
    "    kb_shortcut_df = pd.DataFrame(event_df['id'].unique(), columns=['id'])\n",
    "    kb_shortcut_df['ctrl_x_cnt'] = ctrl_x_df['count']\n",
    "\n",
    "    # Calculating the proportion of mouse click events\n",
    "    mouse_event_df = pd.DataFrame(train_logs_df['id'].unique(), columns=['id'])\n",
    "    \n",
    "    mouse_event_df['mouse_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['down_event'].apply(lambda x: (x.isin(['Leftclick', 'Rightclick', 'Middleclick', 'Unknownclick']).sum())).reset_index()['down_event']\n",
    "\n",
    "    mouse_event_df['all_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['event_id'].max().reset_index()['event_id']\n",
    "\n",
    "    mouse_event_df['mouse_event_perc'] = (mouse_event_df['mouse_event_cnt']/mouse_event_df['all_event_cnt'])*100.0\n",
    "    \n",
    "    return kb_shortcut_df.merge(mouse_event_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_keyboard_mouse_feats(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-key_mouse')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"Generates aggregated time-related features for each essay ID.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Time-based calculations\n",
    "    df['action_time_sec'] = (df['up_time'] - df['down_time']) / 1000.0\n",
    "    df['time_since_last_event'] = df.groupby('id')['down_time'].diff() / 1000.0\n",
    "    df['cumulative_action_time'] = df.groupby('id')['action_time_sec'].cumsum()\n",
    "\n",
    "    # Prepare aggregation dictionary\n",
    "    aggregations = {\n",
    "        'action_time_sec': ['mean', 'sum', 'max', 'std'],\n",
    "        'time_since_last_event': ['mean', 'max', 'std'],\n",
    "        'cumulative_action_time': ['max']\n",
    "    }\n",
    "\n",
    "    # Add rolling window features to aggregations\n",
    "    for window in [5, 10, 15, 20, 30, 50]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).mean())\n",
    "        df[f'rolling_std_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).std())\n",
    "        df[f'rolling_ema_{window}'] = df.groupby('id', group_keys=False)['action_time_sec'].apply(lambda x: x.ewm(span=window, adjust=False).mean())\n",
    "        \n",
    "        aggregations[f'rolling_mean_{window}'] = ['mean', 'median', 'std']\n",
    "        aggregations[f'rolling_ema_{window}'] = ['mean', 'median', 'std']\n",
    "        aggregations[f'rolling_std_{window}'] = ['mean', 'median', 'std']\n",
    "\n",
    "    # Aggregating features for each ID\n",
    "    aggregated_features = df.groupby('id').agg(aggregations)\n",
    "    aggregated_features.columns = ['_'.join(col) for col in aggregated_features.columns]\n",
    "    return aggregated_features\n",
    "\n",
    "def create_additional_time_features(df):\n",
    "    \"\"\"Generates additional aggregated time features for each essay ID.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['time_diff'] = abs(df.groupby('id')['down_time'].diff() - df['up_time'].shift(1)) / 1000\n",
    "    df['time_diff'] = df['time_diff'].fillna(0)  # Handling the first row for each ID\n",
    "\n",
    "    # Prepare aggregation dictionary\n",
    "    aggregates = {'time_diff': ['max', 'median']}\n",
    "\n",
    "    # Adding boolean counts for pauses\n",
    "    for pause in [0.5, 1, 1.5, 2, 3, 5, 10, 20]:\n",
    "        df[f'pauses_{pause}_sec'] = df['time_diff'].apply(lambda x: x > pause)\n",
    "        aggregates[f'pauses_{pause}_sec'] = ['sum', 'count']\n",
    "\n",
    "    # Aggregating features for each ID\n",
    "    additional_features = df.groupby('id').agg(aggregates)\n",
    "    additional_features.columns = ['_'.join(col) for col in additional_features.columns]\n",
    "    return additional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(create_time_features(train_logs).reset_index(), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(create_additional_time_features(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-time_feat')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average down event per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_down_event_per_minute(df, events):\n",
    "\n",
    "    # Convert max down_time from milliseconds to minutes for each id\n",
    "    max_down_time = df.groupby('id')['down_time'].max() / (1000 * 60)\n",
    "\n",
    "    # Count number of each unique down_event for each id\n",
    "    event_count = df[df['down_event'].isin(events)].groupby(['id', 'down_event']).size().reset_index(name='counts')\n",
    "\n",
    "    # Calculate average number of each unique down_event per minute\n",
    "    event_count['avg_event_per_minute'] = event_count['counts'] / event_count['id'].map(max_down_time)\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    pivot_df = event_count.pivot(index='id', columns='down_event', values='avg_event_per_minute')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_avg_down_event_per_minute(*(train_logs, events)).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-avg_event_per_minute')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average deletion per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deletions_per_min(df):\n",
    "\n",
    "    def calc(df, event_name):\n",
    "        # Convert down_time from milliseconds to seconds\n",
    "        df['down_time_sec'] = df['down_time'] / 1000\n",
    "        # Convert down_time_sec to datetime\n",
    "        df['down_time_sec'] = pd.to_datetime(df['down_time_sec'], unit='s')\n",
    "        # Set down_time_sec as index\n",
    "        df.set_index('down_time_sec', inplace=True)\n",
    "        # Calculate the length of deletions\n",
    "        df['deletion_length'] = df['text_change'].str.len()\n",
    "        # Resample to 1 minute intervals and sum deletion length\n",
    "        df_sum = df.groupby('id').resample('1T')['deletion_length'].sum()\n",
    "        # Calculate the average of the sums for each ID\n",
    "        df_avg = df_sum.groupby('id').mean()\n",
    "        # Reset index\n",
    "        df_avg = df_avg.reset_index()\n",
    "        # Rename the column\n",
    "        df_avg.rename(columns={'deletion_length': f'{event_name}'}, inplace=True)\n",
    "\n",
    "        return df_avg\n",
    "\n",
    "    # Filter the DataFrame for 'Delete' down_event and 'q' text_change\n",
    "    df_delete = df[(df['down_event']=='Delete')&(df['text_change']=='q')]\n",
    "    \n",
    "    # Repeat the same steps for 'Backspace'\n",
    "    df_backspace = df[(df['down_event']=='Backspace')&(df['text_change']=='q')]\n",
    "\n",
    "    temp1 = calc(df_delete, 'Delete')\n",
    "    temp2 = calc(df_backspace, 'Backspace')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_avg = temp1.merge(temp2, on='id', how='outer')\n",
    "    df_avg = df_avg.fillna(0)\n",
    "\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\1561204781.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = df['down_time'] / 1000\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\1561204781.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = pd.to_datetime(df['down_time_sec'], unit='s')\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\1561204781.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deletion_length'] = df['text_change'].str.len()\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\1561204781.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = df['down_time'] / 1000\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\1561204781.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = pd.to_datetime(df['down_time_sec'], unit='s')\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\1561204781.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deletion_length'] = df['text_change'].str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calculate_deletions_per_min(train_logs)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-avg_char_deletion_per_minute')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average insertion per min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_insertions_per_min(df):\n",
    "\n",
    "    df_q = df[(df['down_event']=='q')&(df['text_change']=='q')]\n",
    "\n",
    "    # Convert down_time from milliseconds to seconds\n",
    "    df_q['down_time_sec'] = df_q['down_time'] / 1000\n",
    "    # Convert down_time_sec to datetime\n",
    "    df_q['down_time_sec'] = pd.to_datetime(df_q['down_time_sec'], unit='s')\n",
    "    # Set down_time_sec as index\n",
    "    df_q.set_index('down_time_sec', inplace=True)\n",
    "    # Calculate the length of deletions\n",
    "    df_q['insert_q'] = df_q['text_change'].str.len()\n",
    "    # Resample to 1 minute intervals and sum deletion length\n",
    "    df_sum = df_q.groupby('id').resample('1T')['insert_q'].sum()\n",
    "    # Calculate the average of the sums for each ID\n",
    "    df_avg = df_sum.groupby('id').mean()\n",
    "    # Reset index}\n",
    "    df_avg = df_avg.reset_index().fillna(0)\n",
    "\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\2281980173.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q['down_time_sec'] = df_q['down_time'] / 1000\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\2281980173.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q['down_time_sec'] = pd.to_datetime(df_q['down_time_sec'], unit='s')\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\2281980173.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q['insert_q'] = df_q['text_change'].str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calculate_insertions_per_min(train_logs)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-avg_char_insert_per_minute')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total number of words (no deletions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_words(currTextInput):\n",
    "#     word_count = 0\n",
    "#     for i in range(len(currTextInput.values)):\n",
    "#         Input = currTextInput.values[i]\n",
    "#         if Input[0] == 'Replace':\n",
    "#             replaceTxt = Input[2].split(' => ')\n",
    "#             word_count += len(replaceTxt[1].split())\n",
    "#         elif Input[0] == 'Paste':\n",
    "#             word_count += len(Input[2].split())\n",
    "#         elif Input[0] == 'Remove/Cut':\n",
    "#             if i < len(currTextInput.values) - 1 and currTextInput.values[i+1][0] != 'Paste':\n",
    "#                 continue\n",
    "#             word_count -= len(Input[2].split())\n",
    "#         elif \"M\" not in Input[0]:\n",
    "#             word_count += len(Input[2].split())\n",
    "#     return word_count\n",
    "\n",
    "# def get_word_count_df(df):\n",
    "#     df = df[df.activity != 'Nonproduction']\n",
    "#     temp = df.groupby('id').apply(lambda x: count_words(x[['activity', 'cursor_position', 'text_change']]))\n",
    "#     word_count_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "#     word_count_df = word_count_df.merge(temp.rename('word_count'), on='id')\n",
    "#     return word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('< Word Count Calculation >')\n",
    "# temp_feats = get_word_count_df(train_logs)\n",
    "\n",
    "# train_feats = append_features(train_feats, temp_feats, suffix='-count_without_del')\n",
    "# print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-keystroke intervals (IKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IKI(df, down_event, aggregations):\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_periods = df[df['down_event'] == down_event]\n",
    "\n",
    "    # Calculate the interkeystroke intervals (IKIs) for each ID\n",
    "    df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n",
    "    df_periods = df_periods[~df_periods[\"down_time_diff\"].isnull()]\n",
    "\n",
    "    # Calculate the IKI statistics for each ID\n",
    "    df_periods = df_periods.groupby('id')['down_time_diff'].agg(aggregations).reset_index()\n",
    "\n",
    "    return df_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actual_IKI(df_periods, aggregations):\n",
    "\n",
    "    # Calculate the interkeystroke intervals (IKIs) for each ID\n",
    "    df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n",
    "    df_periods = df_periods[~df_periods[\"down_time_diff\"].isnull()]\n",
    "\n",
    "    # Calculate the IKI statistics for each ID\n",
    "    df_periods = df_periods.groupby('id')['down_time_diff'].agg(aggregations).reset_index()\n",
    "\n",
    "    return df_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_IKI(df, aggregations):\n",
    "\n",
    "    # Filter the DataFrame for when the down event is any sentence terminating punctuation\n",
    "    df_periods = df[df['down_event'].isin(['.', '?', '!'])]\n",
    "\n",
    "    # Calculate the interkeystroke intervals (IKIs) for each ID\n",
    "    df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n",
    "    df_periods = df_periods[~df_periods[\"down_time_diff\"].isnull()]\n",
    "\n",
    "    # Calculate the IKI statistics for each ID\n",
    "    df_periods = df_periods.groupby('id')['down_time_diff'].agg(aggregations).reset_index()\n",
    "\n",
    "    return df_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\369627138.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\2189237047.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_19188\\369627138.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calculate_IKI(train_logs, 'Space', AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_word')\n",
    "print(len(train_feats.columns))\n",
    "\n",
    "temp_feats = calculate_sentence_IKI(train_logs, AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_sentence')\n",
    "print(len(train_feats.columns))\n",
    "\n",
    "temp_feats = calculate_IKI(train_logs, 'Enter', AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_paragraph')\n",
    "\n",
    "temp_feats = calculate_actual_IKI(train_logs, AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_actual')\n",
    "\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_latencies(logs):\n",
    "    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "    group = logs.groupby('id')['time_diff']\n",
    "    largest_lantency = group.max()\n",
    "    smallest_lantency = group.min()\n",
    "    median_lantency = group.median()\n",
    "    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'id': logs['id'].unique(),\n",
    "        'largest_lantency': largest_lantency,\n",
    "        'smallest_lantency': smallest_lantency,\n",
    "        'median_lantency': median_lantency,\n",
    "        'initial_pause': initial_pause,\n",
    "        'pauses_half_sec': pauses_half_sec,\n",
    "        'pauses_1_sec': pauses_1_sec,\n",
    "        'pauses_1_half_sec': pauses_1_half_sec,\n",
    "        'pauses_2_sec': pauses_2_sec,\n",
    "        'pauses_3_sec': pauses_3_sec,\n",
    "    }).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calc_latencies(train_logs)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-latencies')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IWD(group):\n",
    "\n",
    "    group['word_group'] = (group['word_count'].diff() > 0).cumsum()\n",
    "\n",
    "    result = group.groupby(['word_count', 'word_group']).agg({\n",
    "        'down_time': 'min',\n",
    "        'up_time': 'max',\n",
    "        'action_time': 'sum'\n",
    "    })\n",
    "\n",
    "    result['duration'] = result['up_time'] - result['down_time']\n",
    "    result['intraword pause'] = result['duration'] - result['action_time']\n",
    "    result['intraword pause'] = result['intraword pause'].apply(lambda x: x if x > 0 else 0)\n",
    "    result['IWD'] = result['intraword pause'] / result['duration']\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_iwd_aggregations(temp_df, aggregations):\n",
    "\n",
    "    temp_df = temp_df[['id', 'duration', 'intraword pause', 'IWD']]\n",
    "    temp_df = temp_df.groupby('id').agg(aggregations)\n",
    "    temp_df.columns = ['_'.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167\n"
     ]
    }
   ],
   "source": [
    "temp_logs = train_logs[(train_logs['activity']=='Input')&(train_logs['text_change']=='q')]\n",
    "temp_feats = temp_logs.groupby('id').apply(calculate_IWD).reset_index()\n",
    "temp_feats = get_iwd_aggregations(temp_feats, aggregations=AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IWD')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaNs: []\n",
      "Columns with all infinite values: []\n",
      "Columns with more than 80% of the same value: ['down_time_min-count_bursts', '<-intro_punctuation', '@-intro_punctuation', '^-intro_punctuation', '*-intro_punctuation', '_-intro_punctuation', '+-intro_punctuation', '[-intro_punctuation', ']-intro_punctuation', '<-conclusion_punctuation', '@-conclusion_punctuation', '^-conclusion_punctuation', '*-conclusion_punctuation', '_-conclusion_punctuation', '+-conclusion_punctuation', '[-conclusion_punctuation', ']-conclusion_punctuation', '<-body_punctuation', '>-body_punctuation', '@-body_punctuation', '^-body_punctuation', '_-body_punctuation', 'cursor_position_change1_q3-cursor_word_changes', 'cursor_position_abs_change2_min-cursor_word_changes', 'word_count_change1_q1-cursor_word_changes', 'word_count_change1_median-cursor_word_changes', 'word_count_change1_q3-cursor_word_changes', 'word_count_abs_change1_min-cursor_word_changes', 'word_count_abs_change1_q1-cursor_word_changes', 'word_count_abs_change1_median-cursor_word_changes', 'word_count_abs_change1_q3-cursor_word_changes', 'word_count_change2_q1-cursor_word_changes', 'word_count_change2_median-cursor_word_changes', 'word_count_abs_change2_min-cursor_word_changes', 'word_count_abs_change2_q1-cursor_word_changes', 'word_count_abs_change2_median-cursor_word_changes', 'word_count_change3_q1-cursor_word_changes', 'word_count_abs_change3_min-cursor_word_changes', 'word_count_abs_change3_q1-cursor_word_changes', 'word_count_abs_change5_min-cursor_word_changes', 'word_count_abs_change10_min-cursor_word_changes', 'smallest_lantency-latencies', 'initial_pause-latencies']\n"
     ]
    }
   ],
   "source": [
    "def filter_columns(df):\n",
    "    # Initialize lists to store column names for each condition\n",
    "    all_nan_columns = []\n",
    "    all_inf_columns = []\n",
    "    single_value_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for col in df.columns:\n",
    "        # Check if the column contains numeric data\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check for all NaN values\n",
    "            if df[col].isna().all():\n",
    "                all_nan_columns.append(col)\n",
    "            else:\n",
    "                # Check for all infinite values (positive or negative infinity)\n",
    "                if np.isinf(df[col]).all():\n",
    "                    all_inf_columns.append(col)\n",
    "                else:\n",
    "                    # Check for more than 90% of columns with only one unique value\n",
    "                    unique_value_count = df[col].nunique()\n",
    "                    total_count = df.shape[0]\n",
    "                    if unique_value_count == 1 and (total_count - df[col].isna().sum()) / total_count > 0.8:\n",
    "                        single_value_columns.append(col)\n",
    "\n",
    "    return {\n",
    "        \"AllNaNColumns\": all_nan_columns,\n",
    "        \"AllInfColumns\": all_inf_columns,\n",
    "        \"SingleValueColumns\": single_value_columns\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame\n",
    "filtered_columns = filter_columns(train_feats)\n",
    "print(\"Columns with all NaNs:\", filtered_columns[\"AllNaNColumns\"])\n",
    "print(\"Columns with all infinite values:\", filtered_columns[\"AllInfColumns\"])\n",
    "print(\"Columns with more than 80% of the same value:\", filtered_columns[\"SingleValueColumns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = train_feats.drop(columns=filtered_columns[\"AllNaNColumns\"] + filtered_columns[\"AllInfColumns\"] + filtered_columns[\"SingleValueColumns\"])\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats.to_csv('output/train_feats_double_corr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
