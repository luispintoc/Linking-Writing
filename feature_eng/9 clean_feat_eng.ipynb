{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kaggle = False\n",
    "\n",
    "if is_kaggle:\n",
    "    base_dir = '/kaggle/input'\n",
    "    data_dir = f'{base_dir}/linking-writing-processes-to-writing-quality'\n",
    "    output_dir = '/kaggle/working'\n",
    "else:\n",
    "    base_dir = '../'\n",
    "    data_dir = f'{base_dir}/data'\n",
    "    models_dir = f'{base_dir}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_logs_df = pd.read_csv(f'../kaggle feat_eng/train_logs_corrected.csv')\n",
    "# train_scores_df = pd.read_csv(f'{data_dir}/train_scores.csv')\n",
    "\n",
    "# test_logs_df = pd.read_csv(f'../kaggle feat_eng/test_logs_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = pl.scan_csv('../kaggle feat_eng/train_logs_corrected.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features related to counts and bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.len(),\n",
    "                             input_word_length_mean = pl.col('text_change').map_elements(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').map_elements(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').map_elements(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').map_elements(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').map_elements(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').name.suffix('_sum'), pl.mean(num_cols).name.suffix('_mean'), pl.std(num_cols).name.suffix('_std'),\n",
    "                                 pl.median(num_cols).name.suffix('_median'), pl.min(num_cols).name.suffix('_min'), pl.max(num_cols).name.suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).name.suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last_distinct()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').name.suffix('_mean'), pl.std('P-bursts').name.suffix('_std'), pl.count('P-bursts').name.suffix('_count'),\n",
    "                                   pl.median('P-bursts').name.suffix('_median'), pl.max('P-bursts').name.suffix('_max'),\n",
    "                                   pl.first('P-bursts').name.suffix('_first'), pl.last('P-bursts').name.suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last_distinct()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').name.suffix('_mean'), pl.std('R-bursts').name.suffix('_std'), \n",
    "                                   pl.median('R-bursts').name.suffix('_median'), pl.max('R-bursts').name.suffix('_max'),\n",
    "                                   pl.first('R-bursts').name.suffix('_first'), pl.last('R-bursts').name.suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "for col in train_feats.columns:\n",
    "    if col != 'id':\n",
    "        train_feats = train_feats.rename(columns={col: col + '-count_bursts'})\n",
    "len(train_feats.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Essay Reconstruction >\n"
     ]
    }
   ],
   "source": [
    "train_logs = train_logs.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_essays = get_essay_df(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              essay\n",
       "0  001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...\n",
       "1  0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...\n",
       "2  0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...\n",
       "3  0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...\n",
       "4  0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations at word/sentence/paragraph/essay level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q0(x):\n",
    "    return x.quantile(0.10)\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def q4(x):\n",
    "    return x.quantile(0.90)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum', q0, q4, 'sem', 'std', 'var', 'skew', pd.DataFrame.kurt]\n",
    "\n",
    "def word_feats(df):\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def essay_feats(df):\n",
    "    # Number of characters in paragraphs\n",
    "    df['essay_len'] = df['essay'].apply(lambda x: len(x))\n",
    "    # Number of words in paragraphs\n",
    "    df['essay_word_count'] = df['essay'].apply(lambda x: len(x.split(' ')))\n",
    "    return df[['id','essay_len','essay_word_count']]\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_features(train_feats, temp_feats, suffix):\n",
    "    for col in temp_feats.columns:\n",
    "        if col != 'id':\n",
    "            temp_feats = temp_feats.rename(columns={col: col + suffix})\n",
    "    train_feats = train_feats.merge(temp_feats, on='id', how='left')\n",
    "    return train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(essay_feats(train_essays), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-word_sent_parag_agg')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-pressed_keys')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pauses_2s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 2000).sum()\n",
    "\n",
    "def count_pauses_5s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 5000).sum()\n",
    "\n",
    "def pause_proportion_2s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def pause_proportion_5s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def mean_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.mean() / 1000 if not pauses.empty else 0\n",
    "\n",
    "def median_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.median() / 1000 if not pauses.empty else 0\n",
    "\n",
    "# Method to aggregate all pause-related features\n",
    "def aggregate_pause_features(df):\n",
    "    \"\"\"Aggregates all pause-related features for each essay.\"\"\"\n",
    "    grouped = df.groupby('id')\n",
    "    pause_features = pd.DataFrame()\n",
    "    pause_features['n_pauses_2s'] = grouped.apply(count_pauses_2s)\n",
    "    pause_features['n_pauses_5s'] = grouped.apply(count_pauses_5s)\n",
    "    pause_features['pause_proportion_2s'] = grouped.apply(pause_proportion_2s)\n",
    "    pause_features['pause_proportion_5s'] = grouped.apply(pause_proportion_5s)\n",
    "    pause_features['mean_pause_length'] = grouped.apply(mean_pause_length)\n",
    "    pause_features['median_pause_length'] = grouped.apply(median_pause_length)\n",
    "    return pause_features\n",
    "\n",
    "def process_variance(group):\n",
    "    \"\"\"Calculates the variance in the writing process over time for each essay.\"\"\"\n",
    "    if len(group) < 2:  # Handling for groups with a single row\n",
    "        return 0\n",
    "\n",
    "    bins = np.linspace(group['down_time'].min(), group['up_time'].max(), 11)\n",
    "    divisions = pd.cut(group['down_time'], bins=bins, include_lowest=True, labels=range(1, 11))\n",
    "    production_deciles = group.groupby(divisions).agg(n_events=('event_id', 'count'))\n",
    "    return np.std(production_deciles['n_events'], ddof=1)\n",
    "\n",
    "def aggregate_process_variance(df):\n",
    "    \"\"\"Aggregates the process variance feature for each essay.\"\"\"\n",
    "    return df.groupby('id').apply(process_variance).rename('process_variance').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(aggregate_pause_features(train_logs).reset_index(), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(aggregate_process_variance(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-paussed_features')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cursor visits to different segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeeds polars dataset\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Needs polars dataset\n",
    "'''\n",
    "\n",
    "# print(\"< Cursor moving animation >\")\n",
    "\n",
    "# # Adding 'pos' and 'line' columns\n",
    "# df = df.with_columns([\n",
    "#     (pl.col('cursor_position') % 30).alias('pos'),\n",
    "#     (pl.col('cursor_position') / 30).cast(pl.Int32).alias('line')\n",
    "# ])\n",
    "\n",
    "# # Adding 'dist_moved' column\n",
    "# df = df.with_columns(\n",
    "#     [pl.col('cursor_position').diff().over('id').fill_null(0).abs().alias('dist_moved')]\n",
    "# )\n",
    "\n",
    "# # Calculating average distance moved per event\n",
    "# avg_dist_per_event = df.groupby('id').agg(\n",
    "#     pl.mean('dist_moved').alias('avg_dist_per_event')\n",
    "# )\n",
    "\n",
    "# # Joining with the feature set\n",
    "# feats = feats.join(avg_dist_per_event, on='id', how='left')\n",
    "\n",
    "# # Adding 'line_change' column\n",
    "# df = df.with_columns(\n",
    "#     [pl.col('line').diff().over('id').fill_null(0).ne(0).alias('line_change')]\n",
    "# )\n",
    "\n",
    "# # Calculating revisits per line and first line revisits\n",
    "# revisits_per_line = df.filter(pl.col('line_change') == False).group_by(['id', 'line']).agg(\n",
    "#     pl.count().alias('revisit_count')\n",
    "# )\n",
    "# first_line_revisits = revisits_per_line.filter(pl.col('line') == 0).group_by('id').agg(\n",
    "#     pl.sum('revisit_count').alias('first_line_revisits')\n",
    "# )\n",
    "\n",
    "# # Joining with the feature set\n",
    "# feats = feats.join(first_line_revisits, on='id', how='left')\n",
    "\n",
    "# # Calculating line changes\n",
    "# line_changes = df.groupby('id').agg(\n",
    "#     pl.sum('line_change').alias('line_changes')\n",
    "# )\n",
    "\n",
    "# # Joining with the feature set\n",
    "# feats = feats.join(line_changes, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_segment_visits(train_logs, train_essays):\n",
    "    # Calculate the end position of the intro and body for each essay\n",
    "    train_essays['intro_end'] = train_essays['essay'].apply(lambda x: len(x.split('\\n')[0]))\n",
    "    train_essays['body_end'] = train_essays['essay'].apply(lambda x: len('\\n'.join(x.split('\\n')[:-1])))\n",
    "\n",
    "    # Create a dictionary for quick lookup\n",
    "    ends_dict = train_essays.set_index('id')[['intro_end', 'body_end']].to_dict('index')\n",
    "\n",
    "    # Function to categorize position\n",
    "    def categorize_position(row):\n",
    "        intro_end = ends_dict[row['id']]['intro_end']\n",
    "        body_end = ends_dict[row['id']]['body_end']\n",
    "        if row['cursor_position'] <= intro_end:\n",
    "            return 'intro'\n",
    "        elif row['cursor_position'] <= body_end:\n",
    "            return 'body'\n",
    "        else:\n",
    "            return 'conclusion'\n",
    "\n",
    "    # Categorize cursor positions\n",
    "    train_logs['segment'] = train_logs.apply(categorize_position, axis=1)\n",
    "\n",
    "    # Count visits to each segment\n",
    "    segment_visits = train_logs.groupby(['id', 'segment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    segment_visits.columns = [f'{col}_visits' for col in segment_visits.columns]\n",
    "\n",
    "    return segment_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(calculate_segment_visits(train_logs, train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-segments_visit')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative size of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_paragraph_sizes(input_df, essay_column):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    # Calculate the total number of paragraphs\n",
    "    df['total_paragraphs'] = df['paragraphs'].apply(len)\n",
    "\n",
    "    # Calculate the relative sizes\n",
    "    df['relative_intro_size'] = 1 / df['total_paragraphs']  # First paragraph is the introduction\n",
    "    df['relative_body_size'] = (df['total_paragraphs'] - 2) / df['total_paragraphs']  # Middle paragraphs are the body\n",
    "    df['relative_conclusion_size'] = 1 / df['total_paragraphs']  # Last paragraph is the conclusion\n",
    "\n",
    "    # Calculate the word count for each paragraph\n",
    "    df['paragraph_word_count'] = df['paragraphs'].apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "\n",
    "    # Separate paragraphs into intro, body, and conclusion\n",
    "    df['word_count_intro'] = df['paragraph_word_count'].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    df['word_count_body'] = df['paragraph_word_count'].apply(lambda x: sum(x[1:-1]) if len(x) > 2 else 0)\n",
    "    df['word_count_conclusion'] = df['paragraph_word_count'].apply(lambda x: x[-1] if len(x) > 1 else 0)\n",
    "\n",
    "    # Calculate total word count for each essay\n",
    "    df['total_word_count'] = df['paragraph_word_count'].apply(sum)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    df['intro_ratio'] = df['word_count_intro'] / df['total_word_count']\n",
    "    df['body_ratio'] = df['word_count_body'] / df['total_word_count']\n",
    "    df['conclusion_ratio'] = df['word_count_conclusion'] / df['total_word_count']\n",
    "    \n",
    "    df['intro_body_ratio'] = df['word_count_intro'] / df['word_count_body']\n",
    "    df['intro_conclusion_ratio'] = df['word_count_intro'] / df['word_count_conclusion']\n",
    "    df['body_conclusion_ratio'] = df['word_count_body'] / df['word_count_conclusion']\n",
    "\n",
    "    # Drop intermediate columns if needed\n",
    "    df.drop(columns=['word', 'sent', 'paragraph', 'paragraphs', 'total_paragraphs', essay_column, 'paragraph_word_count'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(calculate_relative_paragraph_sizes(train_essays, 'essay').reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-paragraph_ratios')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Customer tokenizer\n",
    "# def custom_tokenizer(text):\n",
    "#     words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "#     filtered_words = [word for word in words if word.count('q') < 11] # Do not consider words with more than 10 q's\n",
    "#     return filtered_words\n",
    "\n",
    "# tfidf_vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,2))\n",
    "\n",
    "# def make_text_features(self, df, column='text_change', fit_transform=True):\n",
    "#     \"\"\"Extracts text features using CountVectorizer and TfidfVectorizer, along with custom features.\"\"\"\n",
    "#     # Filter and concatenate text changes\n",
    "#     filtered_df = df[(~df[column].str.contains('=>')) & (df[column] != 'NoChange')]\n",
    "#     concatenated_texts = filtered_df.groupby('id')[column].apply(' '.join).reset_index()\n",
    "\n",
    "#     # Apply CountVectorizer and TfidfVectorizer\n",
    "#     if fit_transform:\n",
    "#         bow_features = self.count_vect.fit_transform(concatenated_texts[column])\n",
    "#         tfidf_features = self.tfidf_vect.fit_transform(concatenated_texts[column])\n",
    "#     else:\n",
    "#         bow_features = self.count_vect.transform(concatenated_texts[column])\n",
    "#         tfidf_features = self.tfidf_vect.transform(concatenated_texts[column])\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     bow_df = pd.DataFrame(bow_features.toarray(), columns=[f'bow_{name}' for name in self.count_vect.get_feature_names_out()])\n",
    "#     tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{name}' for name in self.tfidf_vect.get_feature_names_out()])\n",
    "\n",
    "#     # Custom Feature: Length of each essay\n",
    "#     custom_features_df = pd.DataFrame({'custom_length': concatenated_texts[column].apply(len)})\n",
    "\n",
    "#     # Merge all features\n",
    "#     merged_features = pd.concat([concatenated_texts[['id']], bow_df, tfidf_df, custom_features_df], axis=1)\n",
    "#     return merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gap list for time gap features\n",
    "gap_list = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "def compute_time_gaps(df, gap_list):\n",
    "    \"\"\"Computes time gaps between events for a list of specified gaps.\"\"\"\n",
    "    for gap in gap_list:\n",
    "        df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "        df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "\n",
    "    time_gap_cols = [f'action_time_gap{gap}' for gap in gap_list]\n",
    "    return df[['id'] + time_gap_cols].groupby('id').agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "\n",
    "def compute_cursor_position_change_features(df, gap_list):\n",
    "    \"\"\"Computes cursor position change features for specified gaps per 'id'.\"\"\"\n",
    "    result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for gap in gap_list:\n",
    "        col_shift = f'cursor_position_shift{gap}'\n",
    "        df[col_shift] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "        df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[col_shift]\n",
    "        df[f'cursor_position_abs_change{gap}'] = abs(df[f'cursor_position_change{gap}'])\n",
    "        # Aggregate the results per 'id'\n",
    "        id_features = df.groupby('id').agg({\n",
    "            f'cursor_position_change{gap}': 'mean',  # You can choose different aggregation functions as needed\n",
    "            f'cursor_position_abs_change{gap}': 'mean'\n",
    "        }).reset_index()\n",
    "        result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "    result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_word_count_change_features(df, gap_list):\n",
    "    \"\"\"Computes word count change features for specified gaps per 'id'.\"\"\"\n",
    "    result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for gap in gap_list:\n",
    "        col_shift = f'word_count_shift{gap}'\n",
    "        df[col_shift] = df.groupby('id')['word_count'].shift(gap)\n",
    "        df[f'word_count_change{gap}'] = df['word_count'] - df[col_shift]\n",
    "        df[f'word_count_abs_change{gap}'] = abs(df[f'word_count_change{gap}'])\n",
    "        # Aggregate the results per 'id'\n",
    "        id_features = df.groupby('id').agg({\n",
    "            f'word_count_change{gap}': 'mean',  # You can choose different aggregation functions as needed\n",
    "            f'word_count_abs_change{gap}': 'mean'\n",
    "        }).reset_index()\n",
    "        result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "    result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "# Compute time gap features\n",
    "time_gap_features = compute_time_gaps(train_logs, gap_list)\n",
    "time_gap_features.columns = ['{}_{}'.format(col1, col2) for col1, col2 in time_gap_features.columns]\n",
    "time_gap_features = time_gap_features.reset_index()\n",
    "\n",
    "train_feats = append_features(train_feats, time_gap_features, suffix='-time_gaps')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(compute_cursor_position_change_features(train_logs, gap_list))\n",
    "temp_feats = temp_feats.merge(compute_word_count_change_features(train_logs, gap_list))\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-cursor_word_changes')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_punctuations(df):\n",
    "    \"\"\"Counts the number of punctuation marks used in each essay.\"\"\"\n",
    "    punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                    '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "\n",
    "    # Filter the DataFrame to include only rows with punctuation events\n",
    "    punctuation_df = df[df['down_event'].isin(punctuations)]\n",
    "    punctuation_df = df[~df['down_event'].isin(['\"','#','$','%',')','*',';','<','?','@','^'])] # Not useful characters\n",
    "\n",
    "    # Group by 'id' and 'down_event' and count the occurrences of each punctuation\n",
    "    punctuation_counts = punctuation_df.groupby(['id', 'down_event'])['down_event'].count().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate the total punctuation count for each 'id'\n",
    "    total_punctuation_counts = punctuation_counts.sum(axis=1)\n",
    "\n",
    "    # Add the total count as a new column\n",
    "    punctuation_counts['Total'] = total_punctuation_counts\n",
    "\n",
    "    return punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(match_punctuations(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-punctuation')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get input words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_input_words(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Extracts and aggregates information about input words from the text changes in the dataset.\"\"\"\n",
    "#     # Filter relevant rows and reset index\n",
    "#     filtered_df = df[(~df['text_change'].str.contains('=>')) & (df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "\n",
    "#     # Group and concatenate text changes\n",
    "#     grouped_df = filtered_df.groupby('id')['text_change'].apply(''.join).reset_index()\n",
    "\n",
    "#     # Find all occurrences of 'q+'\n",
    "#     grouped_df['input_words'] = grouped_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "\n",
    "#     # Calculate various statistics\n",
    "#     stats_df = grouped_df['input_words'].apply(lambda words: pd.Series({\n",
    "#         'input_word_count': len(words),\n",
    "#         'input_word_length_mean': np.mean([len(word) for word in words]) if words else 0,\n",
    "#         'input_word_length_max': np.max([len(word) for word in words]) if words else 0,\n",
    "#         'input_word_length_std': np.std([len(word) for word in words]) if words else 0\n",
    "#     }))\n",
    "\n",
    "#     return pd.concat([grouped_df[['id']], stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard / mouse feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyboard_mouse_feats(train_logs_df):\n",
    "\n",
    "    # Creates two shift variables which lag the original variable by 1 and 2 periods respectively.\n",
    "    event_df = train_logs_df[['id', 'event_id', 'down_event']].copy(deep=True)\n",
    "\n",
    "    event_df['down_event_shift_1'] = event_df['down_event'].shift(periods=1)\n",
    "\n",
    "    event_df = event_df[['id', 'event_id', 'down_event_shift_1', 'down_event']]\n",
    "\n",
    "    ctrl_x_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'x')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "\n",
    "    # Creating a DataFrame that contains all counts at an id level.\n",
    "    kb_shortcut_df = pd.DataFrame(event_df['id'].unique(), columns=['id'])\n",
    "    kb_shortcut_df['ctrl_x_cnt'] = ctrl_x_df['count']\n",
    "\n",
    "    # Calculating the proportion of mouse click events\n",
    "    mouse_event_df = pd.DataFrame(train_logs_df['id'].unique(), columns=['id'])\n",
    "    \n",
    "    mouse_event_df['mouse_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['down_event'].apply(lambda x: (x.isin(['Leftclick', 'Rightclick', 'Middleclick', 'Unknownclick']).sum())).reset_index()['down_event']\n",
    "\n",
    "    mouse_event_df['all_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['event_id'].max().reset_index()['event_id']\n",
    "\n",
    "    mouse_event_df['mouse_event_perc'] = (mouse_event_df['mouse_event_cnt']/mouse_event_df['all_event_cnt'])*100.0\n",
    "    \n",
    "    return kb_shortcut_df.merge(mouse_event_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_keyboard_mouse_feats(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-key_mouse')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"Generates aggregated time-related features for each essay ID.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Time-based calculations\n",
    "    df['action_time_sec'] = (df['up_time'] - df['down_time']) / 1000.0\n",
    "    df['time_since_last_event'] = df.groupby('id')['down_time'].diff() / 1000.0\n",
    "    df['cumulative_action_time'] = df.groupby('id')['action_time_sec'].cumsum()\n",
    "\n",
    "    # Prepare aggregation dictionary\n",
    "    aggregations = {\n",
    "        'action_time_sec': ['mean', 'sum', 'max', 'std'],\n",
    "        'time_since_last_event': ['mean', 'max', 'std'],\n",
    "        'cumulative_action_time': ['max']\n",
    "    }\n",
    "\n",
    "    # Add rolling window features to aggregations\n",
    "    for window in [5, 10, 15, 20, 30, 50]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).mean())\n",
    "        df[f'rolling_std_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).std())\n",
    "        aggregations[f'rolling_mean_{window}'] = ['mean']\n",
    "        aggregations[f'rolling_std_{window}'] = ['mean']\n",
    "\n",
    "    # Aggregating features for each ID\n",
    "    aggregated_features = df.groupby('id').agg(aggregations)\n",
    "    aggregated_features.columns = ['_'.join(col) for col in aggregated_features.columns]\n",
    "    return aggregated_features\n",
    "\n",
    "def create_additional_time_features(df):\n",
    "    \"\"\"Generates additional aggregated time features for each essay ID.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['time_diff'] = abs(df.groupby('id')['down_time'].diff() - df['up_time'].shift(1)) / 1000\n",
    "    df['time_diff'] = df['time_diff'].fillna(0)  # Handling the first row for each ID\n",
    "\n",
    "    # Prepare aggregation dictionary\n",
    "    aggregates = {'time_diff': ['max', 'median']}\n",
    "\n",
    "    # Adding boolean counts for pauses\n",
    "    for pause in [0.5, 1, 1.5, 2, 3, 5, 10, 20]:\n",
    "        df[f'pauses_{pause}_sec'] = df['time_diff'].apply(lambda x: x > pause)\n",
    "        aggregates[f'pauses_{pause}_sec'] = ['sum']\n",
    "\n",
    "    # Aggregating features for each ID\n",
    "    additional_features = df.groupby('id').agg(aggregates)\n",
    "    additional_features.columns = ['_'.join(col) for col in additional_features.columns]\n",
    "    return additional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(create_time_features(train_logs).reset_index(), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(create_additional_time_features(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-time_feat')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaNs: []\n",
      "Columns with all infinite values: []\n",
      "Columns with more than 90% of the same value: ['cursor_position_min-count_bursts']\n"
     ]
    }
   ],
   "source": [
    "def filter_columns(df):\n",
    "    # Initialize lists to store column names for each condition\n",
    "    all_nan_columns = []\n",
    "    all_inf_columns = []\n",
    "    single_value_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for col in df.columns:\n",
    "        # Check if the column contains numeric data\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check for all NaN values\n",
    "            if df[col].isna().all():\n",
    "                all_nan_columns.append(col)\n",
    "            else:\n",
    "                # Check for all infinite values (positive or negative infinity)\n",
    "                if np.isinf(df[col]).all():\n",
    "                    all_inf_columns.append(col)\n",
    "                else:\n",
    "                    # Check for more than 90% of columns with only one unique value\n",
    "                    unique_value_count = df[col].nunique()\n",
    "                    total_count = df.shape[0]\n",
    "                    if unique_value_count == 1 and (total_count - df[col].isna().sum()) / total_count > 0.8:\n",
    "                        single_value_columns.append(col)\n",
    "\n",
    "    return {\n",
    "        \"AllNaNColumns\": all_nan_columns,\n",
    "        \"AllInfColumns\": all_inf_columns,\n",
    "        \"SingleValueColumns\": single_value_columns\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame\n",
    "filtered_columns = filter_columns(train_feats)\n",
    "print(\"Columns with all NaNs:\", filtered_columns[\"AllNaNColumns\"])\n",
    "print(\"Columns with all infinite values:\", filtered_columns[\"AllInfColumns\"])\n",
    "print(\"Columns with more than 90% of the same value:\", filtered_columns[\"SingleValueColumns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats.to_csv('train_452feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_compute_lengths(df, column, delimiter, new_column):\n",
    "    \"\"\"Split essays in a DataFrame by a delimiter, remove empty items, and compute lengths, word counts, and word count lists of paragraphs/sentences per id.\"\"\"\n",
    "    # Split the essays using the specified delimiter\n",
    "    split_essays = df[column].str.split(delimiter)\n",
    "    \n",
    "    # Remove empty items (empty paragraphs/sentences)\n",
    "    split_essays = split_essays.apply(lambda x: [item.strip() for item in x if item.strip()])\n",
    "    \n",
    "    # Compute the lengths of each paragraph/sentence\n",
    "    lengths = split_essays.apply(lambda x: [len(paragraph) for paragraph in x])\n",
    "    \n",
    "    # Compute the word counts of each paragraph/sentence\n",
    "    word_counts = split_essays.apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    result_df = pd.DataFrame({'id': df['id'], new_column+'_len': lengths, new_column+'_word_count': word_counts})\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def compute_aggregations(df, id_col, agg_columns):\n",
    "    \"\"\"Computes specified aggregations for each id in the DataFrame using split_and_compute_lengths.\"\"\"\n",
    "    \n",
    "    # Specify the aggregation functions\n",
    "    agg_funcs = ['count', 'mean', 'median', 'std', 'min', 'max', 'var', 'sem']\n",
    "    \n",
    "    # Group by 'id' and compute the specified aggregations for the specified columns\n",
    "    df = df.explode(agg_columns)\n",
    "    agg_df = df.groupby(id_col).agg(agg_funcs)\n",
    "    agg_df.columns = ['_'.join(col) for col in agg_df.columns]\n",
    "    \n",
    "    return agg_df.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "class FeatureEngineering:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_words(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extracts and aggregates information about input words from the text changes in the dataset.\"\"\"\n",
    "        # Filter relevant rows and reset index\n",
    "        filtered_df = df[(~df['text_change'].str.contains('=>')) & (df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "    \n",
    "        # Group and concatenate text changes\n",
    "        grouped_df = filtered_df.groupby('id')['text_change'].apply(''.join).reset_index()\n",
    "    \n",
    "        # Find all occurrences of 'q+'\n",
    "        grouped_df['input_words'] = grouped_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "    \n",
    "        # Calculate various statistics\n",
    "        stats_df = grouped_df['input_words'].apply(lambda words: pd.Series({\n",
    "            'input_word_count': len(words),\n",
    "            'input_word_length_mean': np.mean([len(word) for word in words]) if words else 0,\n",
    "            'input_word_length_max': np.max([len(word) for word in words]) if words else 0,\n",
    "            'input_word_length_std': np.std([len(word) for word in words]) if words else 0\n",
    "        }))\n",
    "\n",
    "        return pd.concat([grouped_df[['id']], stats_df], axis=1)\n",
    "\n",
    "class FeatureStats:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_word_counts(df: pd.DataFrame, id_col: str, word_count_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Aggregates the final word count for each essay.\"\"\"\n",
    "        return df.groupby(id_col).agg(final_word_count=(word_count_col, 'last'))\n",
    "\n",
    "# Customer tokenizer\n",
    "def custom_tokenizer(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    filtered_words = [word for word in words if word.count('q') < 11]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.count_vect = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,2))\n",
    "        self.tfidf_vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,2))\n",
    "        self.idf = {}\n",
    "\n",
    "    def activity_counts(self, df):\n",
    "        \"\"\"Calculates activity counts for each essay.\"\"\"\n",
    "        activity_counts = df.groupby('id')['activity'].value_counts().unstack(fill_value=0)\n",
    "        activity_counts = activity_counts.reindex(columns=self.activities, fill_value=0)\n",
    "\n",
    "        # Apply IDF scaling if needed\n",
    "        if not self.idf:\n",
    "            self.idf = {col: np.log(df.shape[0] / (activity_counts[col].sum() + 1)) for col in self.activities}\n",
    "        activity_counts = activity_counts.apply(lambda x: (1 + np.log(x)) * self.idf.get(x.name, 0), axis=0)\n",
    "\n",
    "        return activity_counts.add_prefix('activity_')\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        \"\"\"Calculates event counts for each essay.\"\"\"\n",
    "        events = ['ArrowRight', 'ArrowLeft', 'ArrowDown', 'ArrowUp', 'CapsLock', \n",
    "                  \"'\", 'Unidentified']\n",
    "\n",
    "        event_counts = df.groupby('id')[colname].value_counts().unstack(fill_value=0)\n",
    "        event_counts = event_counts.reindex(columns=events, fill_value=0)\n",
    "\n",
    "        # Apply IDF scaling if needed\n",
    "        if not self.idf:\n",
    "            self.idf = {col: np.log(df.shape[0] / (event_counts[col].sum() + 1)) for col in events}\n",
    "        event_counts = event_counts.apply(lambda x: (1 + np.log(x)) * self.idf.get(x.name, 0), axis=0)\n",
    "\n",
    "        return event_counts.add_prefix(f'{colname}_')\n",
    "\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        \"\"\"Calculates counts of different types of text changes for each essay.\"\"\"\n",
    "        text_change_counts = df.groupby('id')['text_change'].value_counts().unstack(fill_value=0)\n",
    "        text_change_counts = text_change_counts.reindex(columns=self.text_changes, fill_value=0)\n",
    "\n",
    "        return text_change_counts.add_prefix('text_change_')\n",
    "    \n",
    "    def match_punctuations(self, df):\n",
    "        \"\"\"Counts the number of punctuation marks used in each essay.\"\"\"\n",
    "        punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "\n",
    "        # Filter the DataFrame to include only rows with punctuation events\n",
    "        punctuation_df = df[df['down_event'].isin(punctuations)]\n",
    "        punctuation_df = df[~df['down_event'].isin(['\"','#','$','%',')','*',';','<','?','@','^'])]\n",
    "\n",
    "        # Group by 'id' and 'down_event' and count the occurrences of each punctuation\n",
    "        punctuation_counts = punctuation_df.groupby(['id', 'down_event'])['down_event'].count().unstack(fill_value=0)\n",
    "\n",
    "        # Calculate the total punctuation count for each 'id'\n",
    "        total_punctuation_counts = punctuation_counts.sum(axis=1)\n",
    "\n",
    "        # Add the total count as a new column\n",
    "        punctuation_counts['Total'] = total_punctuation_counts\n",
    "\n",
    "        return punctuation_counts\n",
    "\n",
    "    def compute_time_gaps(self, df, gap_list):\n",
    "        \"\"\"Computes time gaps between events for a list of specified gaps.\"\"\"\n",
    "        for gap in gap_list:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "\n",
    "        time_gap_cols = [f'action_time_gap{gap}' for gap in gap_list]\n",
    "        return df[['id'] + time_gap_cols].groupby('id').agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "    @staticmethod\n",
    "    def count_pauses(group):\n",
    "        \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "        gap = group['down_time'] - group['up_time'].shift(1)\n",
    "        return (gap > 2000).sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def pause_proportion(group):\n",
    "        \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "        gap = group['down_time'] - group['up_time'].shift(1)\n",
    "        total_pause_time = gap[gap > 2000].sum()\n",
    "        total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "        return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pause_length(group):\n",
    "        \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "        gap = group['down_time'] - group['up_time'].shift(1)\n",
    "        pauses = gap[gap > 2000]\n",
    "        return pauses.mean() / 1000 if not pauses.empty else 0\n",
    "\n",
    "    # Method to aggregate all pause-related features\n",
    "    def aggregate_pause_features(self, df):\n",
    "        \"\"\"Aggregates all pause-related features for each essay.\"\"\"\n",
    "        grouped = df.groupby('id')\n",
    "        pause_features = pd.DataFrame()\n",
    "        pause_features['n_pauses'] = grouped.apply(self.count_pauses)\n",
    "        pause_features['pause_proportion'] = grouped.apply(self.pause_proportion)\n",
    "        pause_features['mean_pause_length'] = grouped.apply(self.mean_pause_length)\n",
    "        return pause_features\n",
    "\n",
    "    @staticmethod\n",
    "    def process_variance(group):\n",
    "        \"\"\"Calculates the variance in the writing process over time for each essay.\"\"\"\n",
    "        if len(group) < 2:  # Handling for groups with a single row\n",
    "            return 0\n",
    "\n",
    "        bins = np.linspace(group['down_time'].min(), group['up_time'].max(), 11)\n",
    "        divisions = pd.cut(group['down_time'], bins=bins, include_lowest=True, labels=range(1, 11))\n",
    "        production_deciles = group.groupby(divisions).agg(n_events=('event_id', 'count'))\n",
    "        return np.std(production_deciles['n_events'], ddof=1)\n",
    "\n",
    "    def aggregate_process_variance(self, df):\n",
    "        \"\"\"Aggregates the process variance feature for each essay.\"\"\"\n",
    "        return df.groupby('id').apply(self.process_variance).rename('process_variance').to_frame()\n",
    "\n",
    "    def create_time_features(self, df):\n",
    "        \"\"\"Generates aggregated time-related features for each essay ID.\"\"\"\n",
    "        df = df.copy()\n",
    "        # Time-based calculations\n",
    "        df['action_time_sec'] = (df['up_time'] - df['down_time']) / 1000.0\n",
    "        df['time_since_last_event'] = df.groupby('id')['down_time'].diff() / 1000.0\n",
    "        df['cumulative_action_time'] = df.groupby('id')['action_time_sec'].cumsum()\n",
    "\n",
    "        # Prepare aggregation dictionary\n",
    "        aggregations = {\n",
    "            'action_time_sec': ['mean', 'sum', 'max', 'std'],\n",
    "            'time_since_last_event': ['mean', 'max', 'std'],\n",
    "            'cumulative_action_time': ['max']\n",
    "        }\n",
    "\n",
    "        # Add rolling window features to aggregations\n",
    "        for window in [5, 10, 15, 20, 30, 50]:\n",
    "            df[f'rolling_mean_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).mean())\n",
    "            df[f'rolling_std_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).std())\n",
    "            aggregations[f'rolling_mean_{window}'] = ['mean']\n",
    "            aggregations[f'rolling_std_{window}'] = ['mean']\n",
    "\n",
    "        # Aggregating features for each ID\n",
    "        aggregated_features = df.groupby('id').agg(aggregations)\n",
    "        aggregated_features.columns = ['_'.join(col) for col in aggregated_features.columns]\n",
    "        return aggregated_features.reset_index()\n",
    "\n",
    "    def create_additional_time_features(self, df):\n",
    "        \"\"\"Generates additional aggregated time features for each essay ID.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['time_diff'] = abs(df.groupby('id')['down_time'].diff() - df['up_time'].shift(1)) / 1000\n",
    "        df['time_diff'] = df['time_diff'].fillna(0)  # Handling the first row for each ID\n",
    "\n",
    "        # Prepare aggregation dictionary\n",
    "        aggregates = {'time_diff': ['max', 'median']}\n",
    "\n",
    "        # Adding boolean counts for pauses\n",
    "        for pause in [0.5, 1, 1.5, 2, 3, 5, 10, 20]:\n",
    "            df[f'pauses_{pause}_sec'] = df['time_diff'].apply(lambda x: x > pause)\n",
    "            aggregates[f'pauses_{pause}_sec'] = ['sum']\n",
    "\n",
    "        # Aggregating features for each ID\n",
    "        additional_features = df.groupby('id').agg(aggregates)\n",
    "        additional_features.columns = ['_'.join(col) for col in additional_features.columns]\n",
    "        return additional_features.reset_index()\n",
    "\n",
    "\n",
    "    def make_text_features(self, df, column='text_change', fit_transform=True):\n",
    "        \"\"\"Extracts text features using CountVectorizer and TfidfVectorizer, along with custom features.\"\"\"\n",
    "        # Filter and concatenate text changes\n",
    "        filtered_df = df[(~df[column].str.contains('=>')) & (df[column] != 'NoChange')]\n",
    "        concatenated_texts = filtered_df.groupby('id')[column].apply(' '.join).reset_index()\n",
    "\n",
    "        # Apply CountVectorizer and TfidfVectorizer\n",
    "        if fit_transform:\n",
    "            bow_features = self.count_vect.fit_transform(concatenated_texts[column])\n",
    "            tfidf_features = self.tfidf_vect.fit_transform(concatenated_texts[column])\n",
    "        else:\n",
    "            bow_features = self.count_vect.transform(concatenated_texts[column])\n",
    "            tfidf_features = self.tfidf_vect.transform(concatenated_texts[column])\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        bow_df = pd.DataFrame(bow_features.toarray(), columns=[f'bow_{name}' for name in self.count_vect.get_feature_names_out()])\n",
    "        tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{name}' for name in self.tfidf_vect.get_feature_names_out()])\n",
    "\n",
    "        # Custom Feature: Length of each essay\n",
    "        custom_features_df = pd.DataFrame({'custom_length': concatenated_texts[column].apply(len)})\n",
    "\n",
    "        # Merge all features\n",
    "        merged_features = pd.concat([concatenated_texts[['id']], bow_df, tfidf_df, custom_features_df], axis=1)\n",
    "        return merged_features\n",
    "\n",
    "    def compute_cursor_position_change_features(self, df, gap_list):\n",
    "        \"\"\"Computes cursor position change features for specified gaps per 'id'.\"\"\"\n",
    "        result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "        for gap in gap_list:\n",
    "            col_shift = f'cursor_position_shift{gap}'\n",
    "            df[col_shift] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[col_shift]\n",
    "            df[f'cursor_position_abs_change{gap}'] = abs(df[f'cursor_position_change{gap}'])\n",
    "            # Aggregate the results per 'id'\n",
    "            id_features = df.groupby('id').agg({\n",
    "                f'cursor_position_change{gap}': 'mean',  # You can choose different aggregation functions as needed\n",
    "                f'cursor_position_abs_change{gap}': 'mean'\n",
    "            }).reset_index()\n",
    "            result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "        result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "        return result\n",
    "\n",
    "    def compute_word_count_change_features(self, df, gap_list):\n",
    "        \"\"\"Computes word count change features for specified gaps per 'id'.\"\"\"\n",
    "        result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "        for gap in gap_list:\n",
    "            col_shift = f'word_count_shift{gap}'\n",
    "            df[col_shift] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[col_shift]\n",
    "            df[f'word_count_abs_change{gap}'] = abs(df[f'word_count_change{gap}'])\n",
    "            # Aggregate the results per 'id'\n",
    "            id_features = df.groupby('id').agg({\n",
    "                f'word_count_change{gap}': 'mean',  # You can choose different aggregation functions as needed\n",
    "                f'word_count_abs_change{gap}': 'mean'\n",
    "            }).reset_index()\n",
    "            result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "        result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "        return result\n",
    "\n",
    "    # def compute_ratio_based_features(self, df):\n",
    "    #     \"\"\"Computes various ratio-based features per 'id'.\"\"\"\n",
    "    #     result = df.groupby('id').agg({\n",
    "    #         'word_time_ratio': 'mean',  # You can choose different aggregation functions as needed\n",
    "    #         'word_event_ratio': 'mean',\n",
    "    #         'event_time_ratio': 'mean'\n",
    "    #     }).reset_index()\n",
    "    #     return result\n",
    "\n",
    "    # ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])] It needs word_count_max to work\n",
    "    \n",
    "    def get_keyboard_mouse_feats(self, train_logs_df):\n",
    "\n",
    "        # Creates two shift variables which lag the original variable by 1 and 2 periods respectively.\n",
    "        event_df = train_logs_df[['id', 'event_id', 'down_event']].copy(deep=True)\n",
    "\n",
    "        event_df['down_event_shift_1'] = event_df['down_event'].shift(periods=1)\n",
    "\n",
    "        event_df = event_df[['id', 'event_id', 'down_event_shift_1', 'down_event']]\n",
    "\n",
    "        ctrl_x_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'x')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "\n",
    "        # Creating a DataFrame that contains all counts at an id level.\n",
    "        kb_shortcut_df = pd.DataFrame(event_df['id'].unique(), columns=['id'])\n",
    "        kb_shortcut_df['ctrl_x_cnt'] = ctrl_x_df['count']\n",
    "\n",
    "        # Calculating the proportion of mouse click events\n",
    "        mouse_event_df = pd.DataFrame(train_logs_df['id'].unique(), columns=['id'])\n",
    "        \n",
    "        mouse_event_df['mouse_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['down_event'].apply(lambda x: (x.isin(['Leftclick', 'Rightclick', 'Middleclick', 'Unknownclick']).sum())).reset_index()['down_event']\n",
    "\n",
    "        mouse_event_df['all_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['event_id'].max().reset_index()['event_id']\n",
    "\n",
    "        mouse_event_df['mouse_event_perc'] = (mouse_event_df['mouse_event_cnt']/mouse_event_df['all_event_cnt'])*100.0\n",
    "        \n",
    "        return kb_shortcut_df.merge(mouse_event_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n"
     ]
    }
   ],
   "source": [
    "train_logs = pl.scan_csv(f'{data_dir}/train_logs.csv')\n",
    "feat_ = dev_feats(train_logs).collect().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>activity_0_cnt</th>\n",
       "      <th>activity_1_cnt</th>\n",
       "      <th>activity_2_cnt</th>\n",
       "      <th>activity_3_cnt</th>\n",
       "      <th>activity_4_cnt</th>\n",
       "      <th>text_change_0_cnt</th>\n",
       "      <th>text_change_1_cnt</th>\n",
       "      <th>text_change_2_cnt</th>\n",
       "      <th>text_change_3_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>up_event_6_cnt</th>\n",
       "      <th>up_event_7_cnt</th>\n",
       "      <th>up_event_8_cnt</th>\n",
       "      <th>up_event_9_cnt</th>\n",
       "      <th>up_event_10_cnt</th>\n",
       "      <th>up_event_11_cnt</th>\n",
       "      <th>up_event_12_cnt</th>\n",
       "      <th>up_event_13_cnt</th>\n",
       "      <th>up_event_14_cnt</th>\n",
       "      <th>up_event_15_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>2010</td>\n",
       "      <td>417</td>\n",
       "      <td>120</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1940</td>\n",
       "      <td>436</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>1938</td>\n",
       "      <td>260</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1698</td>\n",
       "      <td>432</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>3515</td>\n",
       "      <td>439</td>\n",
       "      <td>175</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3257</td>\n",
       "      <td>615</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>1304</td>\n",
       "      <td>151</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1146</td>\n",
       "      <td>281</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>1942</td>\n",
       "      <td>517</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1964</td>\n",
       "      <td>397</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>3588</td>\n",
       "      <td>960</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3605</td>\n",
       "      <td>813</td>\n",
       "      <td>59</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>2395</td>\n",
       "      <td>60</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1920</td>\n",
       "      <td>457</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>2849</td>\n",
       "      <td>88</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1031</td>\n",
       "      <td>1879</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>2895</td>\n",
       "      <td>276</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2593</td>\n",
       "      <td>490</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>2452</td>\n",
       "      <td>310</td>\n",
       "      <td>843</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2177</td>\n",
       "      <td>474</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows  52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  activity_0_cnt  activity_1_cnt  activity_2_cnt  \\\n",
       "0     001519c8            2010             417             120   \n",
       "1     0022f953            1938             260             254   \n",
       "2     0042269b            3515             439             175   \n",
       "3     0059420b            1304             151              99   \n",
       "4     0075873a            1942             517              72   \n",
       "...        ...             ...             ...             ...   \n",
       "2466  ffb8c745            3588             960             189   \n",
       "2467  ffbef7e5            2395              60             148   \n",
       "2468  ffccd6fd            2849              88             126   \n",
       "2469  ffec5b38            2895             276              71   \n",
       "2470  fff05981            2452             310             843   \n",
       "\n",
       "      activity_3_cnt  activity_4_cnt  text_change_0_cnt  text_change_1_cnt  \\\n",
       "0                  7               0               1940                436   \n",
       "1                  1               1               1698                432   \n",
       "2                  7               0               3257                615   \n",
       "3                  1               1               1146                281   \n",
       "4                  0               0               1964                397   \n",
       "...              ...             ...                ...                ...   \n",
       "2466               2               0               3605                813   \n",
       "2467               1               0               1920                457   \n",
       "2468               0               0               1031               1879   \n",
       "2469               0               0               2593                490   \n",
       "2470              12               1               2177                474   \n",
       "\n",
       "      text_change_2_cnt  text_change_3_cnt  ...  up_event_6_cnt  \\\n",
       "0                    28                 14  ...               2   \n",
       "1                    18                 24  ...              49   \n",
       "2                    23                 26  ...               0   \n",
       "3                    13                  3  ...               0   \n",
       "4                    32                 25  ...               0   \n",
       "...                 ...                ...  ...             ...   \n",
       "2466                 59                 42  ...               0   \n",
       "2467                 33                 24  ...               0   \n",
       "2468                  6                  3  ...              53   \n",
       "2469                 34                 29  ...               0   \n",
       "2470                 17                 26  ...             208   \n",
       "\n",
       "      up_event_7_cnt  up_event_8_cnt  up_event_9_cnt  up_event_10_cnt  \\\n",
       "0                 21              12               0                0   \n",
       "1                 15              21               3                2   \n",
       "2                 21              23               0                0   \n",
       "3                 13               3               0                0   \n",
       "4                 23              24               0                0   \n",
       "...              ...             ...             ...              ...   \n",
       "2466              43              32               0                0   \n",
       "2467              31              24               0                0   \n",
       "2468               5               2              29                0   \n",
       "2469              31              27               0                0   \n",
       "2470              15              19              13               14   \n",
       "\n",
       "      up_event_11_cnt  up_event_12_cnt  up_event_13_cnt  up_event_14_cnt  \\\n",
       "0                   4                0                3                0   \n",
       "1                   6                0                3                0   \n",
       "2                  17                0                0                0   \n",
       "3                   3                2                2                0   \n",
       "4                  10                0               17                0   \n",
       "...               ...              ...              ...              ...   \n",
       "2466                7                0                7                0   \n",
       "2467               12                0                8                0   \n",
       "2468               12                6                0                0   \n",
       "2469                6                2                4                0   \n",
       "2470               21                0                4                0   \n",
       "\n",
       "      up_event_15_cnt  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "2466                0  \n",
       "2467                0  \n",
       "2468                0  \n",
       "2469                0  \n",
       "2470                0  \n",
       "\n",
       "[2471 rows x 52 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences and paragraphs and compute aggregations\n",
    "sentences = split_and_compute_lengths(train_essays, 'essay', r'[.?!]', 'sent')\n",
    "sentence_aggregations = compute_aggregations(sentences, 'id', ['sent_len', 'sent_word_count'])\n",
    "\n",
    "paragraphs = split_and_compute_lengths(train_essays, 'essay', '\\n', 'paragraph')\n",
    "paragraph_aggregations = compute_aggregations(paragraphs, 'id', ['paragraph_len', 'paragraph_word_count'])\n",
    "\n",
    "paragraph_ratios = calculate_relative_paragraph_sizes(train_essays, 'essay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your classes\n",
    "feature_engineering = FeatureEngineering()\n",
    "feature_stats = FeatureStats()\n",
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luis.pinto1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\luis.pinto1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compute basic features\n",
    "input_words_features = feature_engineering.get_input_words(train_logs_df)\n",
    "word_counts = feature_stats.get_word_counts(train_logs_df, 'id', 'word_count')\n",
    "\n",
    "# Additional features from Preprocessor\n",
    "activity_counts = preprocessor.activity_counts(train_logs_df).reset_index()\n",
    "event_counts = preprocessor.event_counts(train_logs_df, 'down_event').reset_index()\n",
    "text_change_counts = preprocessor.text_change_counts(train_logs_df).reset_index()\n",
    "punctuation_counts = preprocessor.match_punctuations(train_logs_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luis.pinto1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the gap list for time gap features\n",
    "gap_list = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Compute time gap features\n",
    "time_gap_features = preprocessor.compute_time_gaps(train_logs_df, gap_list)\n",
    "time_gap_features.columns = ['{}_{}'.format(col1, col2) for col1, col2 in time_gap_features.columns]\n",
    "time_gap_features = time_gap_features.reset_index()\n",
    "\n",
    "# Compute pause-related features\n",
    "pause_features = preprocessor.aggregate_pause_features(train_logs_df).reset_index()\n",
    "\n",
    "# Compute process variance\n",
    "process_variance = preprocessor.aggregate_process_variance(train_logs_df).reset_index()\n",
    "\n",
    "# Create additional time features\n",
    "time_features = preprocessor.create_time_features(train_logs_df)\n",
    "additional_time_features = preprocessor.create_additional_time_features(train_logs_df)\n",
    "\n",
    "# Text IDF features\n",
    "text_features = preprocessor.make_text_features(train_logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_feats = preprocessor.compute_cursor_position_change_features(train_logs_df, gap_list)\n",
    "word_counts_feats = preprocessor.compute_word_count_change_features(train_logs_df, gap_list)\n",
    "# ratios_feats = preprocessor.compute_ratio_based_features(train_logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_keyboard_feats = preprocessor.get_keyboard_mouse_feats(train_logs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = [sentence_aggregations, paragraph_aggregations, paragraph_ratios, input_words_features,\n",
    "            word_counts, activity_counts, event_counts, text_change_counts, punctuation_counts,\n",
    "            time_gap_features, pause_features, process_variance, time_features, additional_time_features,\n",
    "            text_features, cursor_feats, word_counts_feats, mouse_keyboard_feats]\n",
    "\n",
    "# Define a function to merge two DataFrames on 'id' with 'inner' join\n",
    "merge_two_dfs = lambda left, right: pd.merge(left, right, on='id', how='inner')\n",
    "\n",
    "# Use reduce to iteratively merge all DataFrames in the list\n",
    "merged_df = reduce(merge_two_dfs, list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>down_event</th>\n",
       "      <th>id</th>\n",
       "      <th>!</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>(</th>\n",
       "      <th>+</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>.</th>\n",
       "      <th>/</th>\n",
       "      <th>...</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows  122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "down_event        id  !  &   '  (  +   ,  -   .  /  ...              \\\n",
       "0           001519c8  0  0   3  0  0  12  0  21  0  ...  0  0  0   0   0   0   \n",
       "1           0022f953  0  0   3  0  0  21  5  15  0  ...  0  0  0   0   0   0   \n",
       "2           0042269b  0  0   0  0  0  23  1  21  0  ...  0  0  0   0   0   0   \n",
       "3           0059420b  0  0   2  0  0   3  0  13  0  ...  0  0  0   0   0   0   \n",
       "4           0075873a  0  0  17  0  0  24  0  23  0  ...  0  0  0   0   0   0   \n",
       "...              ... .. ..  .. .. ..  .. ..  .. ..  ... .. .. ..  ..  ..  ..   \n",
       "2466        ffb8c745  0  0   7  0  0  32  1  43  0  ...  0  0  0   0   0   0   \n",
       "2467        ffbef7e5  0  0   8  0  0  24  0  31  0  ...  0  0  0   0   0   0   \n",
       "2468        ffccd6fd  0  0   0  0  0   2  0   5  0  ...  0  0  0   0   0   0   \n",
       "2469        ffec5b38  0  0   4  0  0  27  1  31  0  ...  0  0  0   0   0   0   \n",
       "2470        fff05981  0  0   4  0  0  19  2  15  0  ...  0  0  0   0   0   0   \n",
       "\n",
       "down_event        Total  \n",
       "0            0    0  0   2556  \n",
       "1            0    0  0   2445  \n",
       "2            0    0  0   4134  \n",
       "3            0    0  0   1556  \n",
       "4            0    0  0   2529  \n",
       "...         ..  ... ..    ...  \n",
       "2466         0    0  0   4735  \n",
       "2467         0    0  0   2604  \n",
       "2468         0    0  0   3063  \n",
       "2469         0    0  0   3235  \n",
       "2470         0    0  0   3615  \n",
       "\n",
       "[2471 rows x 122 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_event        id  down_event_ArrowRight  down_event_ArrowLeft  \\\n",
      "0           001519c8                    0.0                   0.0   \n",
      "1           0022f953                    0.0                   0.0   \n",
      "2           0042269b                    0.0                   NaN   \n",
      "3           0059420b                    NaN                   NaN   \n",
      "4           0075873a                    NaN                   NaN   \n",
      "...              ...                    ...                   ...   \n",
      "2466        ffb8c745                    NaN                   NaN   \n",
      "2467        ffbef7e5                    NaN                   NaN   \n",
      "2468        ffccd6fd                    0.0                   0.0   \n",
      "2469        ffec5b38                    NaN                   NaN   \n",
      "2470        fff05981                    0.0                   0.0   \n",
      "\n",
      "down_event  down_event_ArrowDown  down_event_ArrowUp  down_event_CapsLock  \\\n",
      "0                            NaN                 NaN                  NaN   \n",
      "1                            0.0                 0.0                  NaN   \n",
      "2                            NaN                 NaN                  NaN   \n",
      "3                            NaN                 NaN                  0.0   \n",
      "4                            NaN                 NaN                  NaN   \n",
      "...                          ...                 ...                  ...   \n",
      "2466                         NaN                 NaN                  NaN   \n",
      "2467                         NaN                 NaN                  NaN   \n",
      "2468                         0.0                 NaN                  0.0   \n",
      "2469                         NaN                 NaN                  0.0   \n",
      "2470                         0.0                 0.0                  NaN   \n",
      "\n",
      "down_event  down_event_'  down_event_Unidentified  \n",
      "0                    0.0                      NaN  \n",
      "1                    0.0                      NaN  \n",
      "2                    NaN                      NaN  \n",
      "3                    0.0                      NaN  \n",
      "4                    0.0                      NaN  \n",
      "...                  ...                      ...  \n",
      "2466                 0.0                      NaN  \n",
      "2467                 0.0                      NaN  \n",
      "2468                 NaN                      NaN  \n",
      "2469                 0.0                      NaN  \n",
      "2470                 0.0                      NaN  \n",
      "\n",
      "[2471 rows x 8 columns]\n",
      "down_event        id  !  &   '  (  +   ,  -   .  /  ...              \\\n",
      "0           001519c8  0  0   3  0  0  12  0  21  0  ...  0  0  0   0   0   0   \n",
      "1           0022f953  0  0   3  0  0  21  5  15  0  ...  0  0  0   0   0   0   \n",
      "2           0042269b  0  0   0  0  0  23  1  21  0  ...  0  0  0   0   0   0   \n",
      "3           0059420b  0  0   2  0  0   3  0  13  0  ...  0  0  0   0   0   0   \n",
      "4           0075873a  0  0  17  0  0  24  0  23  0  ...  0  0  0   0   0   0   \n",
      "...              ... .. ..  .. .. ..  .. ..  .. ..  ... .. .. ..  ..  ..  ..   \n",
      "2466        ffb8c745  0  0   7  0  0  32  1  43  0  ...  0  0  0   0   0   0   \n",
      "2467        ffbef7e5  0  0   8  0  0  24  0  31  0  ...  0  0  0   0   0   0   \n",
      "2468        ffccd6fd  0  0   0  0  0   2  0   5  0  ...  0  0  0   0   0   0   \n",
      "2469        ffec5b38  0  0   4  0  0  27  1  31  0  ...  0  0  0   0   0   0   \n",
      "2470        fff05981  0  0   4  0  0  19  2  15  0  ...  0  0  0   0   0   0   \n",
      "\n",
      "down_event        Total  \n",
      "0            0    0  0   2556  \n",
      "1            0    0  0   2445  \n",
      "2            0    0  0   4134  \n",
      "3            0    0  0   1556  \n",
      "4            0    0  0   2529  \n",
      "...         ..  ... ..    ...  \n",
      "2466         0    0  0   4735  \n",
      "2467         0    0  0   2604  \n",
      "2468         0    0  0   3063  \n",
      "2469         0    0  0   3235  \n",
      "2470         0    0  0   3615  \n",
      "\n",
      "[2471 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "for df_ in list_of_dfs:\n",
    "    for col in df_.columns:\n",
    "        if 'ArrowUp' in col:\n",
    "            print(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input_word_count</th>\n",
       "      <th>input_word_length_mean</th>\n",
       "      <th>input_word_length_max</th>\n",
       "      <th>input_word_length_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>366.0</td>\n",
       "      <td>5.325137</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.487804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>385.0</td>\n",
       "      <td>4.410390</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.199496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>627.0</td>\n",
       "      <td>5.446571</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.474895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>251.0</td>\n",
       "      <td>4.609562</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.949601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>412.0</td>\n",
       "      <td>4.766990</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.986064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>734.0</td>\n",
       "      <td>4.915531</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>470.0</td>\n",
       "      <td>4.085106</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.231589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>222.0</td>\n",
       "      <td>4.644144</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.707087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.294000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.541689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>526.0</td>\n",
       "      <td>5.095057</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.382815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  input_word_count  input_word_length_mean  \\\n",
       "0     001519c8             366.0                5.325137   \n",
       "1     0022f953             385.0                4.410390   \n",
       "2     0042269b             627.0                5.446571   \n",
       "3     0059420b             251.0                4.609562   \n",
       "4     0075873a             412.0                4.766990   \n",
       "...        ...               ...                     ...   \n",
       "2466  ffb8c745             734.0                4.915531   \n",
       "2467  ffbef7e5             470.0                4.085106   \n",
       "2468  ffccd6fd             222.0                4.644144   \n",
       "2469  ffec5b38             500.0                5.294000   \n",
       "2470  fff05981             526.0                5.095057   \n",
       "\n",
       "      input_word_length_max  input_word_length_std  \n",
       "0                      20.0               3.487804  \n",
       "1                      33.0               3.199496  \n",
       "2                      25.0               3.474895  \n",
       "3                      19.0               2.949601  \n",
       "4                      18.0               2.986064  \n",
       "...                     ...                    ...  \n",
       "2466                   20.0               3.001989  \n",
       "2467                   13.0               2.231589  \n",
       "2468                   15.0               2.707087  \n",
       "2469                   24.0               3.541689  \n",
       "2470                   21.0               3.382815  \n",
       "\n",
       "[2471 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.merge(train_scores_df, on='id').to_csv('example_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sent_len_count</th>\n",
       "      <th>sent_len_mean</th>\n",
       "      <th>sent_len_median</th>\n",
       "      <th>sent_len_std</th>\n",
       "      <th>sent_len_min</th>\n",
       "      <th>sent_len_max</th>\n",
       "      <th>sent_len_var</th>\n",
       "      <th>sent_len_sem</th>\n",
       "      <th>sent_word_count_count</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_abs_change50</th>\n",
       "      <th>word_count_change100</th>\n",
       "      <th>word_count_abs_change100</th>\n",
       "      <th>ctrl_bksp_cnt</th>\n",
       "      <th>ctrl_c_cnt</th>\n",
       "      <th>ctrl_v_cnt</th>\n",
       "      <th>ctrl_x_cnt</th>\n",
       "      <th>mouse_event_cnt</th>\n",
       "      <th>all_event_cnt</th>\n",
       "      <th>mouse_event_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>14</td>\n",
       "      <td>106.142857</td>\n",
       "      <td>119.5</td>\n",
       "      <td>41.128050</td>\n",
       "      <td>31</td>\n",
       "      <td>196</td>\n",
       "      <td>1691.516484</td>\n",
       "      <td>10.991934</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>5.064619</td>\n",
       "      <td>10.060643</td>\n",
       "      <td>10.060643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2557</td>\n",
       "      <td>3.597966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>15</td>\n",
       "      <td>107.666667</td>\n",
       "      <td>92.0</td>\n",
       "      <td>64.713287</td>\n",
       "      <td>19</td>\n",
       "      <td>226</td>\n",
       "      <td>4187.809524</td>\n",
       "      <td>16.708899</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>6.634359</td>\n",
       "      <td>13.378505</td>\n",
       "      <td>13.397196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2454</td>\n",
       "      <td>2.281989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>19</td>\n",
       "      <td>133.842105</td>\n",
       "      <td>139.0</td>\n",
       "      <td>33.480115</td>\n",
       "      <td>73</td>\n",
       "      <td>189</td>\n",
       "      <td>1120.918129</td>\n",
       "      <td>7.680865</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>6.161527</td>\n",
       "      <td>9.679386</td>\n",
       "      <td>10.833499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>4136</td>\n",
       "      <td>3.143133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>13</td>\n",
       "      <td>86.846154</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.195999</td>\n",
       "      <td>39</td>\n",
       "      <td>144</td>\n",
       "      <td>1101.974359</td>\n",
       "      <td>9.206914</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>6.879150</td>\n",
       "      <td>13.580357</td>\n",
       "      <td>13.580357</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.221080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>16</td>\n",
       "      <td>86.812500</td>\n",
       "      <td>74.0</td>\n",
       "      <td>44.094170</td>\n",
       "      <td>22</td>\n",
       "      <td>182</td>\n",
       "      <td>1944.295833</td>\n",
       "      <td>11.023543</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.958484</td>\n",
       "      <td>9.937063</td>\n",
       "      <td>11.323324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2531</td>\n",
       "      <td>1.303832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>13</td>\n",
       "      <td>121.076923</td>\n",
       "      <td>132.0</td>\n",
       "      <td>40.376275</td>\n",
       "      <td>55</td>\n",
       "      <td>180</td>\n",
       "      <td>1630.243590</td>\n",
       "      <td>11.198364</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>8.452975</td>\n",
       "      <td>5.897392</td>\n",
       "      <td>16.040526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4739</td>\n",
       "      <td>0.506436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>29</td>\n",
       "      <td>78.310345</td>\n",
       "      <td>67.0</td>\n",
       "      <td>40.481127</td>\n",
       "      <td>20</td>\n",
       "      <td>175</td>\n",
       "      <td>1638.721675</td>\n",
       "      <td>7.517157</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>8.516445</td>\n",
       "      <td>16.759585</td>\n",
       "      <td>16.759585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>2604</td>\n",
       "      <td>1.459293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>4</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>274.5</td>\n",
       "      <td>77.395090</td>\n",
       "      <td>200</td>\n",
       "      <td>359</td>\n",
       "      <td>5990.000000</td>\n",
       "      <td>38.697545</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.269831</td>\n",
       "      <td>6.522781</td>\n",
       "      <td>6.522781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3063</td>\n",
       "      <td>0.293830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>27</td>\n",
       "      <td>92.592593</td>\n",
       "      <td>98.0</td>\n",
       "      <td>33.747090</td>\n",
       "      <td>36</td>\n",
       "      <td>176</td>\n",
       "      <td>1138.866097</td>\n",
       "      <td>6.494631</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>6.435777</td>\n",
       "      <td>12.716104</td>\n",
       "      <td>12.716741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3242</td>\n",
       "      <td>0.431832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>11</td>\n",
       "      <td>133.272727</td>\n",
       "      <td>95.0</td>\n",
       "      <td>104.683419</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "      <td>10958.618182</td>\n",
       "      <td>31.563238</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>4.628748</td>\n",
       "      <td>6.594487</td>\n",
       "      <td>7.956238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>3619</td>\n",
       "      <td>1.243437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows  327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sent_len_count  sent_len_mean  sent_len_median  sent_len_std  \\\n",
       "0     001519c8              14     106.142857            119.5     41.128050   \n",
       "1     0022f953              15     107.666667             92.0     64.713287   \n",
       "2     0042269b              19     133.842105            139.0     33.480115   \n",
       "3     0059420b              13      86.846154             80.0     33.195999   \n",
       "4     0075873a              16      86.812500             74.0     44.094170   \n",
       "...        ...             ...            ...              ...           ...   \n",
       "2466  ffb8c745              13     121.076923            132.0     40.376275   \n",
       "2467  ffbef7e5              29      78.310345             67.0     40.481127   \n",
       "2468  ffccd6fd               4     277.000000            274.5     77.395090   \n",
       "2469  ffec5b38              27      92.592593             98.0     33.747090   \n",
       "2470  fff05981              11     133.272727             95.0    104.683419   \n",
       "\n",
       "      sent_len_min  sent_len_max  sent_len_var  sent_len_sem  \\\n",
       "0               31           196   1691.516484     10.991934   \n",
       "1               19           226   4187.809524     16.708899   \n",
       "2               73           189   1120.918129      7.680865   \n",
       "3               39           144   1101.974359      9.206914   \n",
       "4               22           182   1944.295833     11.023543   \n",
       "...            ...           ...           ...           ...   \n",
       "2466            55           180   1630.243590     11.198364   \n",
       "2467            20           175   1638.721675      7.517157   \n",
       "2468           200           359   5990.000000     38.697545   \n",
       "2469            36           176   1138.866097      6.494631   \n",
       "2470            56           411  10958.618182     31.563238   \n",
       "\n",
       "      sent_word_count_count  ...  word_count_abs_change50  \\\n",
       "0                        14  ...                 5.064619   \n",
       "1                        15  ...                 6.634359   \n",
       "2                        19  ...                 6.161527   \n",
       "3                        13  ...                 6.879150   \n",
       "4                        16  ...                 5.958484   \n",
       "...                     ...  ...                      ...   \n",
       "2466                     13  ...                 8.452975   \n",
       "2467                     29  ...                 8.516445   \n",
       "2468                      4  ...                 3.269831   \n",
       "2469                     27  ...                 6.435777   \n",
       "2470                     11  ...                 4.628748   \n",
       "\n",
       "      word_count_change100  word_count_abs_change100  ctrl_bksp_cnt  \\\n",
       "0                10.060643                 10.060643              0   \n",
       "1                13.378505                 13.397196              0   \n",
       "2                 9.679386                 10.833499              0   \n",
       "3                13.580357                 13.580357              0   \n",
       "4                 9.937063                 11.323324              0   \n",
       "...                    ...                       ...            ...   \n",
       "2466              5.897392                 16.040526              0   \n",
       "2467             16.759585                 16.759585              0   \n",
       "2468              6.522781                  6.522781              0   \n",
       "2469             12.716104                 12.716741              0   \n",
       "2470              6.594487                  7.956238              0   \n",
       "\n",
       "      ctrl_c_cnt  ctrl_v_cnt  ctrl_x_cnt  mouse_event_cnt  all_event_cnt  \\\n",
       "0              0           0           0               92           2557   \n",
       "1              0           0           0               56           2454   \n",
       "2              0           0           0              130           4136   \n",
       "3              1           2           0               19           1556   \n",
       "4              0           0           0               33           2531   \n",
       "...          ...         ...         ...              ...            ...   \n",
       "2466           0           0           0               24           4739   \n",
       "2467           0           0           0               38           2604   \n",
       "2468           0           0           0                9           3063   \n",
       "2469           0           0           0               14           3242   \n",
       "2470           0           1           0               45           3619   \n",
       "\n",
       "      mouse_event_perc  \n",
       "0             3.597966  \n",
       "1             2.281989  \n",
       "2             3.143133  \n",
       "3             1.221080  \n",
       "4             1.303832  \n",
       "...                ...  \n",
       "2466          0.506436  \n",
       "2467          1.459293  \n",
       "2468          0.293830  \n",
       "2469          0.431832  \n",
       "2470          1.243437  \n",
       "\n",
       "[2471 rows x 327 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaNs: []\n",
      "Columns with all infinite values: []\n",
      "Columns with more than 90% of the same value: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def filter_columns(df):\n",
    "    # Initialize lists to store column names for each condition\n",
    "    all_nan_columns = []\n",
    "    all_inf_columns = []\n",
    "    single_value_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for col in df.columns:\n",
    "        # Check if the column contains numeric data\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check for all NaN values\n",
    "            if df[col].isna().all():\n",
    "                all_nan_columns.append(col)\n",
    "            else:\n",
    "                # Check for all infinite values (positive or negative infinity)\n",
    "                if np.isinf(df[col]).all():\n",
    "                    all_inf_columns.append(col)\n",
    "                else:\n",
    "                    # Check for more than 90% of columns with only one unique value\n",
    "                    unique_value_count = df[col].nunique()\n",
    "                    total_count = df.shape[0]\n",
    "                    if unique_value_count == 1 and (total_count - df[col].isna().sum()) / total_count > 0.8:\n",
    "                        single_value_columns.append(col)\n",
    "\n",
    "    return {\n",
    "        \"AllNaNColumns\": all_nan_columns,\n",
    "        \"AllInfColumns\": all_inf_columns,\n",
    "        \"SingleValueColumns\": single_value_columns\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame\n",
    "filtered_columns = filter_columns(merged_df)\n",
    "print(\"Columns with all NaNs:\", filtered_columns[\"AllNaNColumns\"])\n",
    "print(\"Columns with all infinite values:\", filtered_columns[\"AllInfColumns\"])\n",
    "print(\"Columns with more than 90% of the same value:\", filtered_columns[\"SingleValueColumns\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
