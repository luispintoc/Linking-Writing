{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kaggle = False\n",
    "\n",
    "if is_kaggle:\n",
    "    base_dir = '/kaggle/input'\n",
    "    data_dir = f'{base_dir}/linking-writing-processes-to-writing-quality'\n",
    "    output_dir = '/kaggle/working'\n",
    "else:\n",
    "    base_dir = '../'\n",
    "    data_dir = f'{base_dir}/data'\n",
    "    models_dir = f'{base_dir}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs_df = pd.read_csv(f'{data_dir}/train_logs.csv')\n",
    "train_scores_df = pd.read_csv(f'{data_dir}/train_scores.csv')\n",
    "\n",
    "test_logs_df = pd.read_csv(f'{data_dir}/test_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayConstructor:\n",
    "    \n",
    "    def processingInputs(self,currTextInput):\n",
    "        # Where the essay content will be stored\n",
    "        essayText = \"\"\n",
    "        # Produces the essay\n",
    "        for Input in currTextInput.values:\n",
    "            # Input[0] = activity\n",
    "            # Input[1] = cursor_position\n",
    "            # Input[2] = text_change\n",
    "            # Input[3] = id\n",
    "            # If activity = Replace\n",
    "            if Input[0] == 'Replace':\n",
    "                # splits text_change at ' => '\n",
    "                replaceTxt = Input[2].split(' => ')\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Paste    \n",
    "            if Input[0] == 'Paste':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Remove/Cut\n",
    "            if Input[0] == 'Remove/Cut':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Move...\n",
    "            if \"M\" in Input[0]:\n",
    "                # Gets rid of the \"Move from to\" text\n",
    "                croppedTxt = Input[0][10:]              \n",
    "                # Splits cropped text by ' To '\n",
    "                splitTxt = croppedTxt.split(' To ')              \n",
    "                # Splits split text again by ', ' for each item\n",
    "                valueArr = [item.split(', ') for item in splitTxt]              \n",
    "                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n",
    "                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "                # Skip if someone manages to activiate this by moving to same place\n",
    "                if moveData[0] != moveData[2]:\n",
    "                    # Check if they move text forward in essay (they are different)\n",
    "                    if moveData[0] < moveData[2]:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                    else:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                continue                \n",
    "                \n",
    "            # If activity = input\n",
    "            # DONT TOUCH\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        return essayText\n",
    "            \n",
    "            \n",
    "    def getEssays(self,df):\n",
    "        # Copy required columns\n",
    "        textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n",
    "        # Get rid of text inputs that make no change\n",
    "        textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n",
    "        # construct essay, fast \n",
    "        tqdm.pandas()\n",
    "        essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(lambda x: self.processingInputs(x))      \n",
    "        # to dataframe\n",
    "        essayFrame=essay.to_frame().reset_index()\n",
    "        essayFrame.columns=['id','essay']\n",
    "        # Returns the essay series\n",
    "        return essayFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:16<00:00, 150.39it/s]\n"
     ]
    }
   ],
   "source": [
    "essayConstructor = EssayConstructor()\n",
    "train_essays = essayConstructor.getEssays(train_logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              essay\n",
       "0  001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...\n",
       "1  0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...\n",
       "2  0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...\n",
       "3  0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...\n",
       "4  0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_essays.index = train_essays[\"id\"]\n",
    "# train_essays.index.name = None\n",
    "# train_essays.drop(columns=[\"id\"], inplace=True)\n",
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_compute_lengths(df, column, delimiter, new_column):\n",
    "    \"\"\"Split essays in a DataFrame by a delimiter, remove empty items, and compute lengths, word counts, and word count lists of paragraphs/sentences per id.\"\"\"\n",
    "    # Split the essays using the specified delimiter\n",
    "    split_essays = df[column].str.split(delimiter)\n",
    "    \n",
    "    # Remove empty items (empty paragraphs/sentences)\n",
    "    split_essays = split_essays.apply(lambda x: [item.strip() for item in x if item.strip()])\n",
    "    \n",
    "    # Compute the lengths of each paragraph/sentence\n",
    "    lengths = split_essays.apply(lambda x: [len(paragraph) for paragraph in x])\n",
    "    \n",
    "    # Compute the word counts of each paragraph/sentence\n",
    "    word_counts = split_essays.apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "    \n",
    "    # Create a new DataFrame to store the results\n",
    "    result_df = pd.DataFrame({'id': df['id'], new_column+'_len': lengths, new_column+'_word_count': word_counts})\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def compute_aggregations(df, id_col, agg_columns):\n",
    "    \"\"\"Computes specified aggregations for each id in the DataFrame using split_and_compute_lengths.\"\"\"\n",
    "    \n",
    "    # Specify the aggregation functions\n",
    "    agg_funcs = ['count', 'mean', 'median', 'std', 'min', 'max', 'var', 'sem']\n",
    "    \n",
    "    # Group by 'id' and compute the specified aggregations for the specified columns\n",
    "    df = df.explode(agg_columns)\n",
    "    agg_df = df.groupby(id_col).agg(agg_funcs)\n",
    "    agg_df.columns = ['_'.join(col) for col in agg_df.columns]\n",
    "    \n",
    "    return agg_df.reset_index()\n",
    "\n",
    "def calculate_relative_paragraph_sizes(input_df, essay_column):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    # Calculate the total number of paragraphs\n",
    "    df['total_paragraphs'] = df['paragraphs'].apply(len)\n",
    "\n",
    "    # Calculate the relative sizes\n",
    "    df['relative_intro_size'] = 1 / df['total_paragraphs']  # First paragraph is the introduction\n",
    "    df['relative_body_size'] = (df['total_paragraphs'] - 2) / df['total_paragraphs']  # Middle paragraphs are the body\n",
    "    df['relative_conclusion_size'] = 1 / df['total_paragraphs']  # Last paragraph is the conclusion\n",
    "\n",
    "    # Calculate the word count for each paragraph\n",
    "    df['paragraph_word_count'] = df['paragraphs'].apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "\n",
    "    # Separate paragraphs into intro, body, and conclusion\n",
    "    df['word_count_intro'] = df['paragraph_word_count'].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    df['word_count_body'] = df['paragraph_word_count'].apply(lambda x: sum(x[1:-1]) if len(x) > 2 else 0)\n",
    "    df['word_count_conclusion'] = df['paragraph_word_count'].apply(lambda x: x[-1] if len(x) > 1 else 0)\n",
    "\n",
    "    # Calculate total word count for each essay\n",
    "    df['total_word_count'] = df['paragraph_word_count'].apply(sum)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    df['intro_ratio'] = df['word_count_intro'] / df['total_word_count']\n",
    "    df['body_ratio'] = df['word_count_body'] / df['total_word_count']\n",
    "    df['conclusion_ratio'] = df['word_count_conclusion'] / df['total_word_count']\n",
    "    \n",
    "    df['intro_body_ratio'] = df['word_count_intro'] / df['word_count_body']\n",
    "    df['intro_conclusion_ratio'] = df['word_count_intro'] / df['word_count_conclusion']\n",
    "    df['body_conclusion_ratio'] = df['word_count_body'] / df['word_count_conclusion']\n",
    "\n",
    "\n",
    "    # Drop intermediate columns if needed\n",
    "    df.drop(columns=['paragraphs', 'total_paragraphs', essay_column, 'paragraph_word_count'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "class FeatureEngineering:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_words(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extracts and aggregates information about input words from the text changes in the dataset.\"\"\"\n",
    "        # Filter relevant rows and reset index\n",
    "        filtered_df = df[(~df['text_change'].str.contains('=>')) & (df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "    \n",
    "        # Group and concatenate text changes\n",
    "        grouped_df = filtered_df.groupby('id')['text_change'].apply(''.join).reset_index()\n",
    "    \n",
    "        # Find all occurrences of 'q+'\n",
    "        grouped_df['input_words'] = grouped_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "    \n",
    "        # Calculate various statistics\n",
    "        stats_df = grouped_df['input_words'].apply(lambda words: pd.Series({\n",
    "            'input_word_count': len(words),\n",
    "            'input_word_length_mean': np.mean([len(word) for word in words]) if words else 0,\n",
    "            'input_word_length_max': np.max([len(word) for word in words]) if words else 0,\n",
    "            'input_word_length_std': np.std([len(word) for word in words]) if words else 0\n",
    "        }))\n",
    "\n",
    "        return pd.concat([grouped_df[['id']], stats_df], axis=1)\n",
    "\n",
    "class FeatureStats:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_word_counts(df: pd.DataFrame, id_col: str, word_count_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Aggregates the final word count for each essay.\"\"\"\n",
    "        return df.groupby(id_col).agg(final_word_count=(word_count_col, 'last'))\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.count_vect = CountVectorizer()\n",
    "        self.tfidf_vect = TfidfVectorizer()\n",
    "        self.idf = {}\n",
    "\n",
    "    def activity_counts(self, df):\n",
    "        \"\"\"Calculates activity counts for each essay.\"\"\"\n",
    "        activity_counts = df.groupby('id')['activity'].value_counts().unstack(fill_value=0)\n",
    "        activity_counts = activity_counts.reindex(columns=self.activities, fill_value=0)\n",
    "\n",
    "        # Apply IDF scaling if needed\n",
    "        if not self.idf:\n",
    "            self.idf = {col: np.log(df.shape[0] / (activity_counts[col].sum() + 1)) for col in self.activities}\n",
    "        activity_counts = activity_counts.apply(lambda x: (1 + np.log(x)) * self.idf.get(x.name, 0), axis=0)\n",
    "\n",
    "        return activity_counts.add_prefix('activity_')\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        \"\"\"Calculates event counts for each essay.\"\"\"\n",
    "        events = ['ArrowRight', 'ArrowLeft', 'ArrowDown', 'ArrowUp', 'CapsLock', \n",
    "                  \"'\", 'Delete', 'Unidentified']\n",
    "\n",
    "        event_counts = df.groupby('id')[colname].value_counts().unstack(fill_value=0)\n",
    "        event_counts = event_counts.reindex(columns=events, fill_value=0)\n",
    "\n",
    "        # Apply IDF scaling if needed\n",
    "        if not self.idf:\n",
    "            self.idf = {col: np.log(df.shape[0] / (event_counts[col].sum() + 1)) for col in events}\n",
    "        event_counts = event_counts.apply(lambda x: (1 + np.log(x)) * self.idf.get(x.name, 0), axis=0)\n",
    "\n",
    "        return event_counts.add_prefix(f'{colname}_')\n",
    "\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        \"\"\"Calculates counts of different types of text changes for each essay.\"\"\"\n",
    "        text_change_counts = df.groupby('id')['text_change'].value_counts().unstack(fill_value=0)\n",
    "        text_change_counts = text_change_counts.reindex(columns=self.text_changes, fill_value=0)\n",
    "\n",
    "        return text_change_counts.add_prefix('text_change_')\n",
    "    \n",
    "    def match_punctuations(self, df):\n",
    "        \"\"\"Counts the number of punctuation marks used in each essay.\"\"\"\n",
    "        punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "\n",
    "        # Filter the DataFrame to include only rows with punctuation events\n",
    "        punctuation_df = df[df['down_event'].isin(punctuations)]\n",
    "\n",
    "        # Group by 'id' and 'down_event' and count the occurrences of each punctuation\n",
    "        punctuation_counts = punctuation_df.groupby(['id', 'down_event'])['down_event'].count().unstack(fill_value=0)\n",
    "\n",
    "        # Calculate the total punctuation count for each 'id'\n",
    "        total_punctuation_counts = punctuation_counts.sum(axis=1)\n",
    "\n",
    "        # Add the total count as a new column\n",
    "        punctuation_counts['Total'] = total_punctuation_counts\n",
    "\n",
    "        return punctuation_counts\n",
    "\n",
    "    def compute_time_gaps(self, df, gap_list):\n",
    "        \"\"\"Computes time gaps between events for a list of specified gaps.\"\"\"\n",
    "        for gap in gap_list:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "\n",
    "        time_gap_cols = [f'action_time_gap{gap}' for gap in gap_list]\n",
    "        return df[['id'] + time_gap_cols].groupby('id').agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "    @staticmethod\n",
    "    def count_pauses(group):\n",
    "        \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "        gap = group['down_time'] - group['up_time'].shift(1)\n",
    "        return (gap > 2000).sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def pause_proportion(group):\n",
    "        \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "        gap = group['down_time'] - group['up_time'].shift(1)\n",
    "        total_pause_time = gap[gap > 2000].sum()\n",
    "        total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "        return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pause_length(group):\n",
    "        \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "        gap = group['down_time'] - group['up_time'].shift(1)\n",
    "        pauses = gap[gap > 2000]\n",
    "        return pauses.mean() / 1000 if not pauses.empty else 0\n",
    "\n",
    "    # Method to aggregate all pause-related features\n",
    "    def aggregate_pause_features(self, df):\n",
    "        \"\"\"Aggregates all pause-related features for each essay.\"\"\"\n",
    "        grouped = df.groupby('id')\n",
    "        pause_features = pd.DataFrame()\n",
    "        pause_features['n_pauses'] = grouped.apply(self.count_pauses)\n",
    "        pause_features['pause_proportion'] = grouped.apply(self.pause_proportion)\n",
    "        pause_features['mean_pause_length'] = grouped.apply(self.mean_pause_length)\n",
    "        return pause_features\n",
    "\n",
    "    @staticmethod\n",
    "    def process_variance(group):\n",
    "        \"\"\"Calculates the variance in the writing process over time for each essay.\"\"\"\n",
    "        if len(group) < 2:  # Handling for groups with a single row\n",
    "            return 0\n",
    "\n",
    "        bins = np.linspace(group['down_time'].min(), group['up_time'].max(), 11)\n",
    "        divisions = pd.cut(group['down_time'], bins=bins, include_lowest=True, labels=range(1, 11))\n",
    "        production_deciles = group.groupby(divisions).agg(n_events=('event_id', 'count'))\n",
    "        return np.std(production_deciles['n_events'], ddof=1)\n",
    "\n",
    "    def aggregate_process_variance(self, df):\n",
    "        \"\"\"Aggregates the process variance feature for each essay.\"\"\"\n",
    "        return df.groupby('id').apply(self.process_variance).rename('process_variance').to_frame()\n",
    "\n",
    "    def create_time_features(self, df):\n",
    "        \"\"\"Generates aggregated time-related features for each essay ID.\"\"\"\n",
    "        df = df.copy()\n",
    "        # Time-based calculations\n",
    "        df['action_time_sec'] = (df['up_time'] - df['down_time']) / 1000.0\n",
    "        df['time_since_last_event'] = df.groupby('id')['down_time'].diff() / 1000.0\n",
    "        df['cumulative_action_time'] = df.groupby('id')['action_time_sec'].cumsum()\n",
    "\n",
    "        # Prepare aggregation dictionary\n",
    "        aggregations = {\n",
    "            'action_time_sec': ['mean', 'sum', 'max', 'std'],\n",
    "            'time_since_last_event': ['mean', 'max', 'std'],\n",
    "            'cumulative_action_time': ['max']\n",
    "        }\n",
    "\n",
    "        # Add rolling window features to aggregations\n",
    "        for window in [5, 10, 15, 20, 30, 50]:\n",
    "            df[f'rolling_mean_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).mean())\n",
    "            df[f'rolling_std_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).std())\n",
    "            aggregations[f'rolling_mean_{window}'] = ['mean']\n",
    "            aggregations[f'rolling_std_{window}'] = ['mean']\n",
    "\n",
    "        # Aggregating features for each ID\n",
    "        aggregated_features = df.groupby('id').agg(aggregations)\n",
    "        aggregated_features.columns = ['_'.join(col) for col in aggregated_features.columns]\n",
    "        return aggregated_features.reset_index()\n",
    "\n",
    "    def create_additional_time_features(self, df):\n",
    "        \"\"\"Generates additional aggregated time features for each essay ID.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['time_diff'] = abs(df.groupby('id')['down_time'].diff() - df['up_time'].shift(1)) / 1000\n",
    "        df['time_diff'] = df['time_diff'].fillna(0)  # Handling the first row for each ID\n",
    "\n",
    "        # Prepare aggregation dictionary\n",
    "        aggregates = {\n",
    "            'time_diff': ['max', 'median']  # Initial pause as the first value\n",
    "        }\n",
    "\n",
    "        # Adding boolean counts for pauses\n",
    "        for pause in [0.5, 1, 1.5, 2, 3, 5, 10, 20]:\n",
    "            df[f'pauses_{pause}_sec'] = df['time_diff'].apply(lambda x: x > pause)\n",
    "            aggregates[f'pauses_{pause}_sec'] = ['sum']\n",
    "\n",
    "        # Aggregating features for each ID\n",
    "        additional_features = df.groupby('id').agg(aggregates)\n",
    "        additional_features.columns = ['_'.join(col) for col in additional_features.columns]\n",
    "        return additional_features.reset_index()\n",
    "\n",
    "\n",
    "    def make_text_features(self, df, column='text_change', fit_transform=True):\n",
    "        \"\"\"Extracts text features using CountVectorizer and TfidfVectorizer, along with custom features.\"\"\"\n",
    "        # Filter and concatenate text changes\n",
    "        filtered_df = df[(~df[column].str.contains('=>')) & (df[column] != 'NoChange')]\n",
    "        concatenated_texts = filtered_df.groupby('id')[column].apply(' '.join).reset_index()\n",
    "\n",
    "        # Apply CountVectorizer and TfidfVectorizer\n",
    "        if fit_transform:\n",
    "            bow_features = self.count_vect.fit_transform(concatenated_texts[column])\n",
    "            tfidf_features = self.tfidf_vect.fit_transform(concatenated_texts[column])\n",
    "        else:\n",
    "            bow_features = self.count_vect.transform(concatenated_texts[column])\n",
    "            tfidf_features = self.tfidf_vect.transform(concatenated_texts[column])\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        bow_df = pd.DataFrame(bow_features.toarray(), columns=[f'bow_{name}' for name in self.count_vect.get_feature_names_out()])\n",
    "        tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{name}' for name in self.tfidf_vect.get_feature_names_out()])\n",
    "\n",
    "        # Custom Feature: Length of each essay\n",
    "        custom_features_df = pd.DataFrame({'custom_length': concatenated_texts[column].apply(len)})\n",
    "\n",
    "        # Merge all features\n",
    "        merged_features = pd.concat([concatenated_texts[['id']], bow_df, tfidf_df, custom_features_df], axis=1)\n",
    "        return merged_features\n",
    "\n",
    "    def compute_cursor_position_change_features(self, df, gap_list):\n",
    "        \"\"\"Computes cursor position change features for specified gaps per 'id'.\"\"\"\n",
    "        result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "        for gap in gap_list:\n",
    "            col_shift = f'cursor_position_shift{gap}'\n",
    "            df[col_shift] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[col_shift]\n",
    "            df[f'cursor_position_abs_change{gap}'] = abs(df[f'cursor_position_change{gap}'])\n",
    "            # Aggregate the results per 'id'\n",
    "            id_features = df.groupby('id').agg({\n",
    "                f'cursor_position_change{gap}': 'mean',  # You can choose different aggregation functions as needed\n",
    "                f'cursor_position_abs_change{gap}': 'mean'\n",
    "            }).reset_index()\n",
    "            result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "        result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "        return result\n",
    "\n",
    "    def compute_word_count_change_features(self, df, gap_list):\n",
    "        \"\"\"Computes word count change features for specified gaps per 'id'.\"\"\"\n",
    "        result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "        for gap in gap_list:\n",
    "            col_shift = f'word_count_shift{gap}'\n",
    "            df[col_shift] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[col_shift]\n",
    "            df[f'word_count_abs_change{gap}'] = abs(df[f'word_count_change{gap}'])\n",
    "            # Aggregate the results per 'id'\n",
    "            id_features = df.groupby('id').agg({\n",
    "                f'word_count_change{gap}': 'mean',  # You can choose different aggregation functions as needed\n",
    "                f'word_count_abs_change{gap}': 'mean'\n",
    "            }).reset_index()\n",
    "            result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "        result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "        return result\n",
    "\n",
    "    # def compute_ratio_based_features(self, df):\n",
    "    #     \"\"\"Computes various ratio-based features per 'id'.\"\"\"\n",
    "    #     result = df.groupby('id').agg({\n",
    "    #         'word_time_ratio': 'mean',  # You can choose different aggregation functions as needed\n",
    "    #         'word_event_ratio': 'mean',\n",
    "    #         'event_time_ratio': 'mean'\n",
    "    #     }).reset_index()\n",
    "    #     return result\n",
    "\n",
    "    # ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])] It needs word_count_max to work\n",
    "    \n",
    "    def get_keyboard_mouse_feats(self, train_logs_df):\n",
    "\n",
    "        # Creates two shift variables which lag the original variable by 1 and 2 periods respectively.\n",
    "        event_df = train_logs_df[['id', 'event_id', 'down_event']].copy(deep=True)\n",
    "\n",
    "        event_df['down_event_shift_1'] = event_df['down_event'].shift(periods=1)\n",
    "        event_df['down_event_shift_2'] = event_df['down_event'].shift(periods=2)\n",
    "\n",
    "        event_df = event_df[['id', 'event_id', 'down_event_shift_2', 'down_event_shift_1', 'down_event']]\n",
    "\n",
    "        ctrl_bksp_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'] == 'Backspace')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "        ctrl_c_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'c')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "        ctrl_v_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'v')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "        ctrl_x_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'x')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "\n",
    "        # Creating a DataFrame that contains all counts at an id level.\n",
    "\n",
    "        kb_shortcut_df = pd.DataFrame(event_df['id'].unique(), columns=['id'])\n",
    "\n",
    "        kb_shortcut_df['ctrl_bksp_cnt'] = ctrl_bksp_df['count']\n",
    "        kb_shortcut_df['ctrl_c_cnt'] = ctrl_c_df['count']\n",
    "        kb_shortcut_df['ctrl_v_cnt'] = ctrl_v_df['count']\n",
    "        kb_shortcut_df['ctrl_x_cnt'] = ctrl_x_df['count']\n",
    "\n",
    "        mouse_event_df = pd.DataFrame(train_logs_df['id'].unique(), columns=['id'])\n",
    "\n",
    "        # # Calculating the proportion of mouse click events\n",
    "        mouse_event_df['mouse_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['down_event'].apply(lambda x: (x.isin(['Leftclick', 'Rightclick', 'Middleclick', 'Unknownclick']).sum())).reset_index()['down_event']\n",
    "\n",
    "        mouse_event_df['all_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['event_id'].max().reset_index()['event_id']\n",
    "\n",
    "        mouse_event_df['mouse_event_perc'] = (mouse_event_df['mouse_event_cnt']/mouse_event_df['all_event_cnt'])*100.0\n",
    "        \n",
    "        return kb_shortcut_df.merge(mouse_event_df, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences and paragraphs and compute aggregations\n",
    "sentences = split_and_compute_lengths(train_essays, 'essay', r'[.?!]', 'sent')\n",
    "sentence_aggregations = compute_aggregations(sentences, 'id', ['sent_len', 'sent_word_count'])\n",
    "\n",
    "paragraphs = split_and_compute_lengths(train_essays, 'essay', '\\n', 'paragraph')\n",
    "paragraph_aggregations = compute_aggregations(paragraphs, 'id', ['paragraph_len', 'paragraph_word_count'])\n",
    "\n",
    "paragraph_ratios = calculate_relative_paragraph_sizes(train_essays, 'essay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your classes\n",
    "feature_engineering = FeatureEngineering()\n",
    "feature_stats = FeatureStats()\n",
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luizi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\luizi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\arraylike.py:397: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compute basic features\n",
    "input_words_features = feature_engineering.get_input_words(train_logs_df)\n",
    "word_counts = feature_stats.get_word_counts(train_logs_df, 'id', 'word_count')\n",
    "\n",
    "# Additional features from Preprocessor\n",
    "activity_counts = preprocessor.activity_counts(train_logs_df).reset_index()\n",
    "event_counts = preprocessor.event_counts(train_logs_df, 'down_event').reset_index()\n",
    "text_change_counts = preprocessor.text_change_counts(train_logs_df).reset_index()\n",
    "punctuation_counts = preprocessor.match_punctuations(train_logs_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gap list for time gap features\n",
    "gap_list = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Compute time gap features\n",
    "time_gap_features = preprocessor.compute_time_gaps(train_logs_df, gap_list)\n",
    "time_gap_features.columns = ['{}_{}'.format(col1, col2) for col1, col2 in time_gap_features.columns]\n",
    "time_gap_features = time_gap_features.reset_index()\n",
    "\n",
    "# Compute pause-related features\n",
    "pause_features = preprocessor.aggregate_pause_features(train_logs_df).reset_index()\n",
    "\n",
    "# Compute process variance\n",
    "process_variance = preprocessor.aggregate_process_variance(train_logs_df).reset_index()\n",
    "\n",
    "# Create additional time features\n",
    "time_features = preprocessor.create_time_features(train_logs_df)\n",
    "additional_time_features = preprocessor.create_additional_time_features(train_logs_df)\n",
    "\n",
    "# Text IDF features\n",
    "text_features = preprocessor.make_text_features(train_logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_feats = preprocessor.compute_cursor_position_change_features(train_logs_df, gap_list)\n",
    "word_counts_feats = preprocessor.compute_word_count_change_features(train_logs_df, gap_list)\n",
    "# ratios_feats = preprocessor.compute_ratio_based_features(train_logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_keyboard_feats = preprocessor.get_keyboard_mouse_feats(train_logs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = [sentence_aggregations, paragraph_aggregations, paragraph_ratios, input_words_features,\n",
    "            word_counts, activity_counts, event_counts, text_change_counts, punctuation_counts,\n",
    "            time_gap_features, pause_features, process_variance, time_features, additional_time_features,\n",
    "            text_features, cursor_feats, word_counts_feats, mouse_keyboard_feats]\n",
    "\n",
    "# Define a function to merge two DataFrames on 'id' with 'inner' join\n",
    "merge_two_dfs = lambda left, right: pd.merge(left, right, on='id', how='inner')\n",
    "\n",
    "# Use reduce to iteratively merge all DataFrames in the list\n",
    "merged_df = reduce(merge_two_dfs, list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.merge(train_scores_df, on='id').to_csv('example_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sent_len_count</th>\n",
       "      <th>sent_len_mean</th>\n",
       "      <th>sent_len_median</th>\n",
       "      <th>sent_len_std</th>\n",
       "      <th>sent_len_min</th>\n",
       "      <th>sent_len_max</th>\n",
       "      <th>sent_len_var</th>\n",
       "      <th>sent_len_sem</th>\n",
       "      <th>sent_word_count_count</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_abs_change50</th>\n",
       "      <th>word_count_change100</th>\n",
       "      <th>word_count_abs_change100</th>\n",
       "      <th>ctrl_bksp_cnt</th>\n",
       "      <th>ctrl_c_cnt</th>\n",
       "      <th>ctrl_v_cnt</th>\n",
       "      <th>ctrl_x_cnt</th>\n",
       "      <th>mouse_event_cnt</th>\n",
       "      <th>all_event_cnt</th>\n",
       "      <th>mouse_event_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>14</td>\n",
       "      <td>106.142857</td>\n",
       "      <td>119.5</td>\n",
       "      <td>41.128050</td>\n",
       "      <td>31</td>\n",
       "      <td>196</td>\n",
       "      <td>1691.516484</td>\n",
       "      <td>10.991934</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>5.064619</td>\n",
       "      <td>10.060643</td>\n",
       "      <td>10.060643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>2557</td>\n",
       "      <td>3.597966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>15</td>\n",
       "      <td>107.666667</td>\n",
       "      <td>92.0</td>\n",
       "      <td>64.713287</td>\n",
       "      <td>19</td>\n",
       "      <td>226</td>\n",
       "      <td>4187.809524</td>\n",
       "      <td>16.708899</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>6.634359</td>\n",
       "      <td>13.378505</td>\n",
       "      <td>13.397196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2454</td>\n",
       "      <td>2.281989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>19</td>\n",
       "      <td>133.842105</td>\n",
       "      <td>139.0</td>\n",
       "      <td>33.480115</td>\n",
       "      <td>73</td>\n",
       "      <td>189</td>\n",
       "      <td>1120.918129</td>\n",
       "      <td>7.680865</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>6.161527</td>\n",
       "      <td>9.679386</td>\n",
       "      <td>10.833499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>4136</td>\n",
       "      <td>3.143133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>13</td>\n",
       "      <td>86.846154</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.195999</td>\n",
       "      <td>39</td>\n",
       "      <td>144</td>\n",
       "      <td>1101.974359</td>\n",
       "      <td>9.206914</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>6.879150</td>\n",
       "      <td>13.580357</td>\n",
       "      <td>13.580357</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.221080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>16</td>\n",
       "      <td>86.812500</td>\n",
       "      <td>74.0</td>\n",
       "      <td>44.094170</td>\n",
       "      <td>22</td>\n",
       "      <td>182</td>\n",
       "      <td>1944.295833</td>\n",
       "      <td>11.023543</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>5.958484</td>\n",
       "      <td>9.937063</td>\n",
       "      <td>11.323324</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2531</td>\n",
       "      <td>1.303832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>ffb8c745</td>\n",
       "      <td>13</td>\n",
       "      <td>121.076923</td>\n",
       "      <td>132.0</td>\n",
       "      <td>40.376275</td>\n",
       "      <td>55</td>\n",
       "      <td>180</td>\n",
       "      <td>1630.243590</td>\n",
       "      <td>11.198364</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>8.452975</td>\n",
       "      <td>5.897392</td>\n",
       "      <td>16.040526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4739</td>\n",
       "      <td>0.506436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>ffbef7e5</td>\n",
       "      <td>29</td>\n",
       "      <td>78.310345</td>\n",
       "      <td>67.0</td>\n",
       "      <td>40.481127</td>\n",
       "      <td>20</td>\n",
       "      <td>175</td>\n",
       "      <td>1638.721675</td>\n",
       "      <td>7.517157</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>8.516445</td>\n",
       "      <td>16.759585</td>\n",
       "      <td>16.759585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>2604</td>\n",
       "      <td>1.459293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>ffccd6fd</td>\n",
       "      <td>4</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>274.5</td>\n",
       "      <td>77.395090</td>\n",
       "      <td>200</td>\n",
       "      <td>359</td>\n",
       "      <td>5990.000000</td>\n",
       "      <td>38.697545</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.269831</td>\n",
       "      <td>6.522781</td>\n",
       "      <td>6.522781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3063</td>\n",
       "      <td>0.293830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>ffec5b38</td>\n",
       "      <td>27</td>\n",
       "      <td>92.592593</td>\n",
       "      <td>98.0</td>\n",
       "      <td>33.747090</td>\n",
       "      <td>36</td>\n",
       "      <td>176</td>\n",
       "      <td>1138.866097</td>\n",
       "      <td>6.494631</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>6.435777</td>\n",
       "      <td>12.716104</td>\n",
       "      <td>12.716741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3242</td>\n",
       "      <td>0.431832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>fff05981</td>\n",
       "      <td>11</td>\n",
       "      <td>133.272727</td>\n",
       "      <td>95.0</td>\n",
       "      <td>104.683419</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "      <td>10958.618182</td>\n",
       "      <td>31.563238</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>4.628748</td>\n",
       "      <td>6.594487</td>\n",
       "      <td>7.956238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>3619</td>\n",
       "      <td>1.243437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sent_len_count  sent_len_mean  sent_len_median  sent_len_std  \\\n",
       "0     001519c8              14     106.142857            119.5     41.128050   \n",
       "1     0022f953              15     107.666667             92.0     64.713287   \n",
       "2     0042269b              19     133.842105            139.0     33.480115   \n",
       "3     0059420b              13      86.846154             80.0     33.195999   \n",
       "4     0075873a              16      86.812500             74.0     44.094170   \n",
       "...        ...             ...            ...              ...           ...   \n",
       "2466  ffb8c745              13     121.076923            132.0     40.376275   \n",
       "2467  ffbef7e5              29      78.310345             67.0     40.481127   \n",
       "2468  ffccd6fd               4     277.000000            274.5     77.395090   \n",
       "2469  ffec5b38              27      92.592593             98.0     33.747090   \n",
       "2470  fff05981              11     133.272727             95.0    104.683419   \n",
       "\n",
       "      sent_len_min  sent_len_max  sent_len_var  sent_len_sem  \\\n",
       "0               31           196   1691.516484     10.991934   \n",
       "1               19           226   4187.809524     16.708899   \n",
       "2               73           189   1120.918129      7.680865   \n",
       "3               39           144   1101.974359      9.206914   \n",
       "4               22           182   1944.295833     11.023543   \n",
       "...            ...           ...           ...           ...   \n",
       "2466            55           180   1630.243590     11.198364   \n",
       "2467            20           175   1638.721675      7.517157   \n",
       "2468           200           359   5990.000000     38.697545   \n",
       "2469            36           176   1138.866097      6.494631   \n",
       "2470            56           411  10958.618182     31.563238   \n",
       "\n",
       "      sent_word_count_count  ...  word_count_abs_change50  \\\n",
       "0                        14  ...                 5.064619   \n",
       "1                        15  ...                 6.634359   \n",
       "2                        19  ...                 6.161527   \n",
       "3                        13  ...                 6.879150   \n",
       "4                        16  ...                 5.958484   \n",
       "...                     ...  ...                      ...   \n",
       "2466                     13  ...                 8.452975   \n",
       "2467                     29  ...                 8.516445   \n",
       "2468                      4  ...                 3.269831   \n",
       "2469                     27  ...                 6.435777   \n",
       "2470                     11  ...                 4.628748   \n",
       "\n",
       "      word_count_change100  word_count_abs_change100  ctrl_bksp_cnt  \\\n",
       "0                10.060643                 10.060643              0   \n",
       "1                13.378505                 13.397196              0   \n",
       "2                 9.679386                 10.833499              0   \n",
       "3                13.580357                 13.580357              0   \n",
       "4                 9.937063                 11.323324              0   \n",
       "...                    ...                       ...            ...   \n",
       "2466              5.897392                 16.040526              0   \n",
       "2467             16.759585                 16.759585              0   \n",
       "2468              6.522781                  6.522781              0   \n",
       "2469             12.716104                 12.716741              0   \n",
       "2470              6.594487                  7.956238              0   \n",
       "\n",
       "      ctrl_c_cnt  ctrl_v_cnt  ctrl_x_cnt  mouse_event_cnt  all_event_cnt  \\\n",
       "0              0           0           0               92           2557   \n",
       "1              0           0           0               56           2454   \n",
       "2              0           0           0              130           4136   \n",
       "3              1           2           0               19           1556   \n",
       "4              0           0           0               33           2531   \n",
       "...          ...         ...         ...              ...            ...   \n",
       "2466           0           0           0               24           4739   \n",
       "2467           0           0           0               38           2604   \n",
       "2468           0           0           0                9           3063   \n",
       "2469           0           0           0               14           3242   \n",
       "2470           0           1           0               45           3619   \n",
       "\n",
       "      mouse_event_perc  \n",
       "0             3.597966  \n",
       "1             2.281989  \n",
       "2             3.143133  \n",
       "3             1.221080  \n",
       "4             1.303832  \n",
       "...                ...  \n",
       "2466          0.506436  \n",
       "2467          1.459293  \n",
       "2468          0.293830  \n",
       "2469          0.431832  \n",
       "2470          1.243437  \n",
       "\n",
       "[2471 rows x 327 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def filter_columns(df):\n",
    "#     # Initialize lists to store column names for each condition\n",
    "#     all_nan_columns = []\n",
    "#     all_inf_columns = []\n",
    "#     single_value_columns = []\n",
    "\n",
    "#     # Iterate through columns\n",
    "#     for col in df.columns:\n",
    "#         # Check if the column contains numeric data\n",
    "#         if pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             # Check for all NaN values\n",
    "#             if df[col].isna().all():\n",
    "#                 all_nan_columns.append(col)\n",
    "#             else:\n",
    "#                 # Check for all infinite values (positive or negative infinity)\n",
    "#                 if np.isinf(df[col]).all():\n",
    "#                     all_inf_columns.append(col)\n",
    "#                 else:\n",
    "#                     # Check for more than 90% of columns with only one unique value\n",
    "#                     unique_value_count = df[col].nunique()\n",
    "#                     total_count = df.shape[0]\n",
    "#                     if unique_value_count == 1 and (total_count - df[col].isna().sum()) / total_count > 0.8:\n",
    "#                         single_value_columns.append(col)\n",
    "\n",
    "#     return {\n",
    "#         \"AllNaNColumns\": all_nan_columns,\n",
    "#         \"AllInfColumns\": all_inf_columns,\n",
    "#         \"SingleValueColumns\": single_value_columns\n",
    "#     }\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'df' is your DataFrame\n",
    "# filtered_columns = filter_columns(merged_df)\n",
    "# print(\"Columns with all NaNs:\", filtered_columns[\"AllNaNColumns\"])\n",
    "# print(\"Columns with all infinite values:\", filtered_columns[\"AllInfColumns\"])\n",
    "# print(\"Columns with more than 90% of the same value:\", filtered_columns[\"SingleValueColumns\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
