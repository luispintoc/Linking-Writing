{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kaggle = False\n",
    "\n",
    "if is_kaggle:\n",
    "    base_dir = '/kaggle/input'\n",
    "    data_dir = f'{base_dir}/linking-writing-processes-to-writing-quality'\n",
    "    output_dir = '/kaggle/working'\n",
    "else:\n",
    "    base_dir = '../'\n",
    "    data_dir = f'{base_dir}/data'\n",
    "    models_dir = f'{base_dir}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = pl.scan_csv('../kaggle feat_eng/train_logs_double_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'train_double_corr_786feats.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_logs_df = pd.read_csv(f'../kaggle feat_eng/train_logs_corrected.csv')\n",
    "# train_scores_df = pd.read_csv(f'{data_dir}/train_scores.csv')\n",
    "\n",
    "# test_logs_df = pd.read_csv(f'../kaggle feat_eng/test_logs_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_features(train_feats, temp_feats, suffix):\n",
    "    for col in temp_feats.columns:\n",
    "        if col != 'id':\n",
    "            temp_feats = temp_feats.rename(columns={col: col + suffix})\n",
    "    train_feats = train_feats.merge(temp_feats, on='id', how='left')\n",
    "    return train_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features related to counts and bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete'] #, 'Unidentified', 'Clear', 'PageUp', 'PageDown']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left')\n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.len(),\n",
    "                             input_word_length_mean = pl.col('text_change').map_elements(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').map_elements(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').map_elements(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').map_elements(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').map_elements(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').name.suffix('_sum'), pl.mean(num_cols).name.suffix('_mean'), pl.std(num_cols).name.suffix('_std'),\n",
    "                                 pl.median(num_cols).name.suffix('_median'), pl.min(num_cols).name.suffix('_min'), pl.max(num_cols).name.suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).name.suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "\n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last_distinct()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').name.suffix('_mean'), pl.std('P-bursts').name.suffix('_std'), pl.count('P-bursts').name.suffix('_count'),\n",
    "                                   pl.median('P-bursts').name.suffix('_median'), pl.max('P-bursts').name.suffix('_max'),\n",
    "                                   pl.first('P-bursts').name.suffix('_first'), pl.last('P-bursts').name.suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last_distinct()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').name.suffix('_mean'), pl.std('R-bursts').name.suffix('_std'), \n",
    "                                   pl.median('R-bursts').name.suffix('_median'), pl.max('R-bursts').name.suffix('_max'),\n",
    "                                   pl.first('R-bursts').name.suffix('_first'), pl.last('R-bursts').name.suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "for col in train_feats.columns:\n",
    "    if col != 'id':\n",
    "        train_feats = train_feats.rename(columns={col: col + '-count_bursts'})\n",
    "len(train_feats.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Essay Reconstruction >\n"
     ]
    }
   ],
   "source": [
    "train_logs = train_logs.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_essays = get_essay_df(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001519c8</td>\n",
       "      <td>qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022f953</td>\n",
       "      <td>qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0042269b</td>\n",
       "      <td>qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0059420b</td>\n",
       "      <td>qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0075873a</td>\n",
       "      <td>qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              essay\n",
       "0  001519c8  qqqqqqqqq qq qqqqq qq qqqq qqqq.  qqqqqq qqq q...\n",
       "1  0022f953  qqqq qq qqqqqqqqqqq ? qq qq qqq qqq qqq, qqqqq...\n",
       "2  0042269b  qqqqqqqqqqq qq qqqqq qqqqqqqqq qq qqqqqqqqqqq ...\n",
       "3  0059420b  qq qqqqqqq qqqqqq qqqqqqqqqqqqq qqqq q qqqq qq...\n",
       "4  0075873a  qqqqqqqqqqq qq qqq qqqqq qq qqqqqqqqqq, qqq qq..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idle time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feats = train_logs.groupby('id')['idle_time'].apply(lambda x: x.unique()[0]).reset_index()\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-idle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations at word/sentence/paragraph/essay level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q0(x):\n",
    "    return x.quantile(0.10)\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def q4(x):\n",
    "    return x.quantile(0.90)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum'] #, q0, q4, 'sem', 'std', 'var', 'skew', pd.DataFrame.kurt]\n",
    "\n",
    "def word_feats(df):\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def essay_feats(df):\n",
    "    # Number of characters in paragraphs\n",
    "    df['essay_len'] = df['essay'].apply(lambda x: len(x))\n",
    "    # Number of words in paragraphs\n",
    "    df['essay_word_count'] = df['essay'].apply(lambda x: len(x.split(' ')))\n",
    "    return df[['id','essay_len','essay_word_count']]\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(essay_feats(train_essays), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-word_sent_parag_agg')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-pressed_keys')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pauses_2s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 2000).sum()\n",
    "\n",
    "def count_pauses_5s(group):\n",
    "    \"\"\"Counts pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    return (gap > 5000).sum()\n",
    "\n",
    "def pause_proportion_2s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def pause_proportion_5s(group):\n",
    "    \"\"\"Calculates the proportion of pause time to total essay time.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    total_pause_time = gap[gap > 2000].sum()\n",
    "    total_essay_time = group['up_time'].max() - group['down_time'].min()\n",
    "    return total_pause_time / total_essay_time if total_essay_time else 0\n",
    "\n",
    "def mean_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.mean() / 1000 if not pauses.empty else 0\n",
    "\n",
    "def median_pause_length(group):\n",
    "    \"\"\"Calculates the mean length of pauses longer than 2000 ms.\"\"\"\n",
    "    gap = group['down_time'] - group['up_time'].shift(1)\n",
    "    pauses = gap[gap > 2000]\n",
    "    return pauses.median() / 1000 if not pauses.empty else 0\n",
    "\n",
    "# Method to aggregate all pause-related features\n",
    "def aggregate_pause_features(df):\n",
    "    \"\"\"Aggregates all pause-related features for each essay.\"\"\"\n",
    "    grouped = df.groupby('id')\n",
    "    pause_features = pd.DataFrame()\n",
    "    pause_features['n_pauses_2s'] = grouped.apply(count_pauses_2s)\n",
    "    pause_features['n_pauses_5s'] = grouped.apply(count_pauses_5s)\n",
    "    pause_features['pause_proportion_2s'] = grouped.apply(pause_proportion_2s)\n",
    "    pause_features['pause_proportion_5s'] = grouped.apply(pause_proportion_5s)\n",
    "    pause_features['mean_pause_length'] = grouped.apply(mean_pause_length)\n",
    "    pause_features['median_pause_length'] = grouped.apply(median_pause_length)\n",
    "    return pause_features\n",
    "\n",
    "def process_variance(group):\n",
    "    \"\"\"Calculates the variance in the writing process over time for each essay.\"\"\"\n",
    "    if len(group) < 2:  # Handling for groups with a single row\n",
    "        return 0\n",
    "\n",
    "    bins = np.linspace(group['down_time'].min(), group['up_time'].max(), 11)\n",
    "    divisions = pd.cut(group['down_time'], bins=bins, include_lowest=True, labels=range(1, 11))\n",
    "    production_deciles = group.groupby(divisions).agg(n_events=('event_id', 'count'))\n",
    "    return np.std(production_deciles['n_events'], ddof=1)\n",
    "\n",
    "def aggregate_process_variance(df):\n",
    "    \"\"\"Aggregates the process variance feature for each essay.\"\"\"\n",
    "    return df.groupby('id').apply(process_variance).rename('process_variance').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(aggregate_pause_features(train_logs).reset_index(), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(aggregate_process_variance(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-paussed_features')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cursor visits to different segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeeds polars dataset\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Needs polars dataset\n",
    "'''\n",
    "\n",
    "# print(\"< Cursor moving animation >\")\n",
    "\n",
    "# # Adding 'pos' and 'line' columns\n",
    "# df = df.with_columns([\n",
    "#     (pl.col('cursor_position') % 30).alias('pos'),\n",
    "#     (pl.col('cursor_position') / 30).cast(pl.Int32).alias('line')\n",
    "# ])\n",
    "\n",
    "# # Adding 'dist_moved' column\n",
    "# df = df.with_columns(\n",
    "#     [pl.col('cursor_position').diff().over('id').fill_null(0).abs().alias('dist_moved')]\n",
    "# )\n",
    "\n",
    "# # Calculating average distance moved per event\n",
    "# avg_dist_per_event = df.groupby('id').agg(\n",
    "#     pl.mean('dist_moved').alias('avg_dist_per_event')\n",
    "# )\n",
    "\n",
    "# # Joining with the feature set\n",
    "# feats = feats.join(avg_dist_per_event, on='id', how='left')\n",
    "\n",
    "# # Adding 'line_change' column\n",
    "# df = df.with_columns(\n",
    "#     [pl.col('line').diff().over('id').fill_null(0).ne(0).alias('line_change')]\n",
    "# )\n",
    "\n",
    "# # Calculating revisits per line and first line revisits\n",
    "# revisits_per_line = df.filter(pl.col('line_change') == False).group_by(['id', 'line']).agg(\n",
    "#     pl.count().alias('revisit_count')\n",
    "# )\n",
    "# first_line_revisits = revisits_per_line.filter(pl.col('line') == 0).group_by('id').agg(\n",
    "#     pl.sum('revisit_count').alias('first_line_revisits')\n",
    "# )\n",
    "\n",
    "# # Joining with the feature set\n",
    "# feats = feats.join(first_line_revisits, on='id', how='left')\n",
    "\n",
    "# # Calculating line changes\n",
    "# line_changes = df.groupby('id').agg(\n",
    "#     pl.sum('line_change').alias('line_changes')\n",
    "# )\n",
    "\n",
    "# # Joining with the feature set\n",
    "# feats = feats.join(line_changes, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_segment_visits(train_logs, train_essays):\n",
    "    # Calculate the end position of the intro and body for each essay\n",
    "    train_essays['intro_end'] = train_essays['essay'].apply(lambda x: len(x.split('\\n')[0]))\n",
    "    train_essays['body_end'] = train_essays['essay'].apply(lambda x: len('\\n'.join(x.split('\\n')[:-1])))\n",
    "\n",
    "    # Create a dictionary for quick lookup\n",
    "    ends_dict = train_essays.set_index('id')[['intro_end', 'body_end']].to_dict('index')\n",
    "\n",
    "    # Function to categorize position\n",
    "    def categorize_position(row):\n",
    "        intro_end = ends_dict[row['id']]['intro_end']\n",
    "        body_end = ends_dict[row['id']]['body_end']\n",
    "        if row['cursor_position'] <= intro_end:\n",
    "            return 'intro'\n",
    "        elif row['cursor_position'] <= body_end:\n",
    "            return 'body'\n",
    "        else:\n",
    "            return 'conclusion'\n",
    "\n",
    "    # Categorize cursor positions\n",
    "    train_logs['segment'] = train_logs.apply(categorize_position, axis=1)\n",
    "\n",
    "    # Count visits to each segment\n",
    "    segment_visits = train_logs.groupby(['id', 'segment']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    segment_visits.columns = [f'{col}_visits' for col in segment_visits.columns]\n",
    "\n",
    "    return segment_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(calculate_segment_visits(train_logs, train_essays).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-segments_visit')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('../data/train_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>down_time</th>\n",
       "      <th>up_time</th>\n",
       "      <th>action_time</th>\n",
       "      <th>activity</th>\n",
       "      <th>down_event</th>\n",
       "      <th>up_event</th>\n",
       "      <th>text_change</th>\n",
       "      <th>cursor_position</th>\n",
       "      <th>word_count</th>\n",
       "      <th>idle_time</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59875</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>654258</td>\n",
       "      <td>intro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59876</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>2</td>\n",
       "      <td>344</td>\n",
       "      <td>449</td>\n",
       "      <td>105</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>654258</td>\n",
       "      <td>intro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59877</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>665</td>\n",
       "      <td>153</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>654258</td>\n",
       "      <td>intro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59878</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>4</td>\n",
       "      <td>608</td>\n",
       "      <td>721</td>\n",
       "      <td>113</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>654258</td>\n",
       "      <td>intro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59879</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>5</td>\n",
       "      <td>704</td>\n",
       "      <td>833</td>\n",
       "      <td>129</td>\n",
       "      <td>Input</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>q</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>654258</td>\n",
       "      <td>intro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64883</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>5009</td>\n",
       "      <td>1127643</td>\n",
       "      <td>1127736</td>\n",
       "      <td>93</td>\n",
       "      <td>Remove/Cut</td>\n",
       "      <td>Backspace</td>\n",
       "      <td>Backspace</td>\n",
       "      <td></td>\n",
       "      <td>1654</td>\n",
       "      <td>413</td>\n",
       "      <td>654258</td>\n",
       "      <td>conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64884</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>5010</td>\n",
       "      <td>1128386</td>\n",
       "      <td>1128472</td>\n",
       "      <td>86</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>1961</td>\n",
       "      <td>413</td>\n",
       "      <td>654258</td>\n",
       "      <td>conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64885</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>5011</td>\n",
       "      <td>1136893</td>\n",
       "      <td>1137582</td>\n",
       "      <td>689</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>1757</td>\n",
       "      <td>413</td>\n",
       "      <td>654258</td>\n",
       "      <td>conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64886</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>5012</td>\n",
       "      <td>1139574</td>\n",
       "      <td>1139663</td>\n",
       "      <td>89</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>1758</td>\n",
       "      <td>413</td>\n",
       "      <td>654258</td>\n",
       "      <td>conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64887</th>\n",
       "      <td>01963e20</td>\n",
       "      <td>5013</td>\n",
       "      <td>1142546</td>\n",
       "      <td>1142644</td>\n",
       "      <td>98</td>\n",
       "      <td>Nonproduction</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>Leftclick</td>\n",
       "      <td>NoChange</td>\n",
       "      <td>2596</td>\n",
       "      <td>413</td>\n",
       "      <td>654258</td>\n",
       "      <td>conclusion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5013 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  event_id  down_time  up_time  action_time       activity  \\\n",
       "59875  01963e20         1          0      136          136          Input   \n",
       "59876  01963e20         2        344      449          105          Input   \n",
       "59877  01963e20         3        512      665          153          Input   \n",
       "59878  01963e20         4        608      721          113          Input   \n",
       "59879  01963e20         5        704      833          129          Input   \n",
       "...         ...       ...        ...      ...          ...            ...   \n",
       "64883  01963e20      5009    1127643  1127736           93     Remove/Cut   \n",
       "64884  01963e20      5010    1128386  1128472           86  Nonproduction   \n",
       "64885  01963e20      5011    1136893  1137582          689  Nonproduction   \n",
       "64886  01963e20      5012    1139574  1139663           89  Nonproduction   \n",
       "64887  01963e20      5013    1142546  1142644           98  Nonproduction   \n",
       "\n",
       "      down_event   up_event text_change  cursor_position  word_count  \\\n",
       "59875          q          q           q                1           1   \n",
       "59876          q          q           q                2           1   \n",
       "59877          q          q           q                3           1   \n",
       "59878          q          q           q                4           1   \n",
       "59879          q          q           q                5           1   \n",
       "...          ...        ...         ...              ...         ...   \n",
       "64883  Backspace  Backspace                         1654         413   \n",
       "64884  Leftclick  Leftclick    NoChange             1961         413   \n",
       "64885  Leftclick  Leftclick    NoChange             1757         413   \n",
       "64886  Leftclick  Leftclick    NoChange             1758         413   \n",
       "64887  Leftclick  Leftclick    NoChange             2596         413   \n",
       "\n",
       "       idle_time     segment  \n",
       "59875     654258       intro  \n",
       "59876     654258       intro  \n",
       "59877     654258       intro  \n",
       "59878     654258       intro  \n",
       "59879     654258       intro  \n",
       "...          ...         ...  \n",
       "64883     654258  conclusion  \n",
       "64884     654258  conclusion  \n",
       "64885     654258  conclusion  \n",
       "64886     654258  conclusion  \n",
       "64887     654258  conclusion  \n",
       "\n",
       "[5013 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_logs[train_logs['id']=='01963e20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative size of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_paragraph_sizes(input_df, essay_column):\n",
    "\n",
    "    df = input_df.copy()\n",
    "    # Split the essay text into paragraphs\n",
    "    df['paragraphs'] = df[essay_column].str.split('\\n')\n",
    "\n",
    "    # Filter out empty paragraphs\n",
    "    df['paragraphs'] = df['paragraphs'].apply(lambda paragraphs: [p for p in paragraphs if p.strip() != ''])\n",
    "\n",
    "    # Calculate the total number of paragraphs\n",
    "    df['total_paragraphs'] = df['paragraphs'].apply(len)\n",
    "\n",
    "    # Calculate the relative sizes\n",
    "    df['relative_intro_size'] = 1 / df['total_paragraphs']  # First paragraph is the introduction\n",
    "    df['relative_body_size'] = (df['total_paragraphs'] - 2) / df['total_paragraphs']  # Middle paragraphs are the body\n",
    "    df['relative_conclusion_size'] = 1 / df['total_paragraphs']  # Last paragraph is the conclusion\n",
    "\n",
    "    # Calculate the word count for each paragraph\n",
    "    df['paragraph_word_count'] = df['paragraphs'].apply(lambda x: [len(paragraph.split()) for paragraph in x])\n",
    "\n",
    "    # Separate paragraphs into intro, body, and conclusion\n",
    "    df['word_count_intro'] = df['paragraph_word_count'].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    df['word_count_body'] = df['paragraph_word_count'].apply(lambda x: sum(x[1:-1]) if len(x) > 2 else 0)\n",
    "    df['word_count_conclusion'] = df['paragraph_word_count'].apply(lambda x: x[-1] if len(x) > 1 else 0)\n",
    "\n",
    "    # Calculate total word count for each essay\n",
    "    df['total_word_count'] = df['paragraph_word_count'].apply(sum)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    df['intro_ratio'] = df['word_count_intro'] / df['total_word_count']\n",
    "    df['body_ratio'] = df['word_count_body'] / df['total_word_count']\n",
    "    df['conclusion_ratio'] = df['word_count_conclusion'] / df['total_word_count']\n",
    "    \n",
    "    df['intro_body_ratio'] = df['word_count_intro'] / df['word_count_body']\n",
    "    df['intro_conclusion_ratio'] = df['word_count_intro'] / df['word_count_conclusion']\n",
    "    df['body_conclusion_ratio'] = df['word_count_body'] / df['word_count_conclusion']\n",
    "\n",
    "    # Drop intermediate columns if needed\n",
    "    df.drop(columns=['word', 'sent', 'paragraph', 'paragraphs', 'total_paragraphs', essay_column, 'paragraph_word_count'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(calculate_relative_paragraph_sizes(train_essays, 'essay').reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-paragraph_ratios')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Customer tokenizer\n",
    "# def custom_tokenizer(text):\n",
    "#     words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "#     filtered_words = [word for word in words if word.count('q') < 11] # Do not consider words with more than 10 q's\n",
    "#     return filtered_words\n",
    "\n",
    "# tfidf_vect = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,2))\n",
    "\n",
    "# def make_text_features(self, df, column='text_change', fit_transform=True):\n",
    "#     \"\"\"Extracts text features using CountVectorizer and TfidfVectorizer, along with custom features.\"\"\"\n",
    "#     # Filter and concatenate text changes\n",
    "#     filtered_df = df[(~df[column].str.contains('=>')) & (df[column] != 'NoChange')]\n",
    "#     concatenated_texts = filtered_df.groupby('id')[column].apply(' '.join).reset_index()\n",
    "\n",
    "#     # Apply CountVectorizer and TfidfVectorizer\n",
    "#     if fit_transform:\n",
    "#         bow_features = self.count_vect.fit_transform(concatenated_texts[column])\n",
    "#         tfidf_features = self.tfidf_vect.fit_transform(concatenated_texts[column])\n",
    "#     else:\n",
    "#         bow_features = self.count_vect.transform(concatenated_texts[column])\n",
    "#         tfidf_features = self.tfidf_vect.transform(concatenated_texts[column])\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     bow_df = pd.DataFrame(bow_features.toarray(), columns=[f'bow_{name}' for name in self.count_vect.get_feature_names_out()])\n",
    "#     tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=[f'tfidf_{name}' for name in self.tfidf_vect.get_feature_names_out()])\n",
    "\n",
    "#     # Custom Feature: Length of each essay\n",
    "#     custom_features_df = pd.DataFrame({'custom_length': concatenated_texts[column].apply(len)})\n",
    "\n",
    "#     # Merge all features\n",
    "#     merged_features = pd.concat([concatenated_texts[['id']], bow_df, tfidf_df, custom_features_df], axis=1)\n",
    "#     return merged_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gap list for time gap features\n",
    "gap_list = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "def compute_time_gaps(df, gap_list):\n",
    "    \"\"\"Computes time gaps between events for a list of specified gaps.\"\"\"\n",
    "    for gap in gap_list:\n",
    "        df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "        df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "\n",
    "    time_gap_cols = [f'action_time_gap{gap}' for gap in gap_list]\n",
    "    return df[['id'] + time_gap_cols].groupby('id').agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "\n",
    "def compute_cursor_position_change_features(df, gap_list, aggregations):\n",
    "    \"\"\"Computes cursor position change features for specified gaps per 'id'.\"\"\"\n",
    "    result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for gap in gap_list:\n",
    "        col_shift = f'cursor_position_shift{gap}'\n",
    "        df[col_shift] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "        df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[col_shift]\n",
    "        df[f'cursor_position_abs_change{gap}'] = abs(df[f'cursor_position_change{gap}'])\n",
    "        # Aggregate the results per 'id'\n",
    "        id_features = df.groupby('id').agg({\n",
    "            f'cursor_position_change{gap}': aggregations,\n",
    "            f'cursor_position_abs_change{gap}': aggregations\n",
    "        }).reset_index()\n",
    "        result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "    result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_word_count_change_features(df, gap_list, aggregations):\n",
    "    \"\"\"Computes word count change features for specified gaps per 'id'.\"\"\"\n",
    "    result = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for gap in gap_list:\n",
    "        col_shift = f'word_count_shift{gap}'\n",
    "        df[col_shift] = df.groupby('id')['word_count'].shift(gap)\n",
    "        df[f'word_count_change{gap}'] = df['word_count'] - df[col_shift]\n",
    "        df[f'word_count_abs_change{gap}'] = abs(df[f'word_count_change{gap}'])\n",
    "        # Aggregate the results per 'id'\n",
    "        id_features = df.groupby('id').agg({\n",
    "            f'word_count_change{gap}': aggregations,  # You can choose different aggregation functions as needed\n",
    "            f'word_count_abs_change{gap}': aggregations\n",
    "        }).reset_index()\n",
    "        result = pd.concat([result, id_features], axis=1)  # Concatenate the results\n",
    "\n",
    "    result = result.loc[:, ~result.columns.duplicated()]  # Remove duplicate columns\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n"
     ]
    }
   ],
   "source": [
    "# Compute time gap features\n",
    "time_gap_features = compute_time_gaps(train_logs, gap_list)\n",
    "time_gap_features.columns = ['{}_{}'.format(col1, col2) for col1, col2 in time_gap_features.columns]\n",
    "time_gap_features = time_gap_features.reset_index()\n",
    "\n",
    "train_feats = append_features(train_feats, time_gap_features, suffix='-time_gaps')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "\n",
    "temp = compute_cursor_position_change_features(train_logs, gap_list, AGGREGATIONS)\n",
    "temp.columns = ['_'.join(col).strip() for col in temp.columns.values]\n",
    "temp_feats = temp_feats.merge(temp, left_on='id', right_on='id_', how='left').drop('id_', axis=1)\n",
    "\n",
    "temp = compute_word_count_change_features(train_logs, gap_list, AGGREGATIONS)\n",
    "temp.columns = ['_'.join(col).strip() for col in temp.columns.values]\n",
    "temp_feats = temp_feats.merge(temp, left_on='id', right_on='id_', how='left').drop('id_', axis=1)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-cursor_word_changes')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_punctuations(df):\n",
    "    \"\"\"Counts the number of punctuation marks used in each essay.\"\"\"\n",
    "    punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                    '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "\n",
    "    # Filter the DataFrame to include only rows with punctuation events\n",
    "    punctuation_df = df[df['down_event'].isin(punctuations)]\n",
    "    punctuation_df = df[~df['down_event'].isin(['\"','#','$','%',')','*',';','<','?','@','^'])] # Not useful characters\n",
    "\n",
    "    # Group by 'id' and 'down_event' and count the occurrences of each punctuation\n",
    "    punctuation_counts = punctuation_df.groupby(['id', 'down_event'])['down_event'].count().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate the total punctuation count for each 'id'\n",
    "    total_punctuation_counts = punctuation_counts.sum(axis=1)\n",
    "\n",
    "    # Add the total count as a new column\n",
    "    punctuation_counts['Total'] = total_punctuation_counts\n",
    "\n",
    "    return punctuation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(match_punctuations(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-punctuation')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the function to calculate top N frequencies and their magnitudes for each 'id' using groupby and apply\n",
    "def calculate_fft_features(group):\n",
    "\n",
    "    from scipy.fft import fft\n",
    "    import scipy\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy.signal import savgol_filter\n",
    "\n",
    "    group['pos'] = group['cursor_position']%30\n",
    "    group['line'] = (group['cursor_position']/30).astype(int)\n",
    "\n",
    "    # Perform Fourier Transform on 'pos'\n",
    "    fft_values = fft(group['pos'])[1:]\n",
    "    \n",
    "    # Generate frequencies corresponding to the Fourier Transform values\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), 1)[1:]\n",
    "    \n",
    "    # Take absolute value to get magnitude\n",
    "    fft_magnitude = np.abs(fft_values)\n",
    "    \n",
    "    # Identify indices where the frequencies are positive\n",
    "    positive_indices = np.where(frequencies > 0)[0]\n",
    "    \n",
    "    # Filter out only positive frequencies and skip the zero frequency\n",
    "    frequencies = frequencies[positive_indices]\n",
    "    magnitudes = fft_magnitude[positive_indices]\n",
    "    \n",
    "    # Frequency Domain Features\n",
    "    peak_freq = frequencies[np.argmax(magnitudes)]\n",
    "    mean_freq = np.average(frequencies, weights=magnitudes)\n",
    "    median_freq = frequencies[len(magnitudes) // 2]\n",
    "    bandwidth = np.ptp(frequencies)\n",
    "    freq_skewness = scipy.stats.skew(magnitudes)\n",
    "    freq_kurtosis = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    # Other Features\n",
    "    total_energy = np.sum(magnitudes ** 2)\n",
    "    \n",
    "    # Spectral Entropy\n",
    "    psd_norm = np.abs(magnitudes) / np.sum(np.abs(magnitudes))\n",
    "    spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + np.finfo(float).eps))\n",
    "    \n",
    "    # Spectral Flatness\n",
    "    spectral_flatness = np.exp(np.mean(np.log(magnitudes + np.finfo(float).eps))) / np.mean(magnitudes)\n",
    "    \n",
    "    # Spectral Roll-off\n",
    "    spectral_sum = np.cumsum(magnitudes)\n",
    "    spectral_rolloff = frequencies[np.searchsorted(spectral_sum, 0.85 * spectral_sum[-1])]\n",
    "    \n",
    "    # Statistical Features\n",
    "    mean_amplitude = np.mean(magnitudes)\n",
    "    std_amplitude = np.std(magnitudes)\n",
    "    skew_amplitude = scipy.stats.skew(magnitudes)\n",
    "    kurtosis_amplitude = scipy.stats.kurtosis(magnitudes)\n",
    "\n",
    "    def exponential_decay(t, A, B, C):\n",
    "        return A * np.exp(-B * t) + C\n",
    "\n",
    "    # Fit curve to FFT\n",
    "    try:\n",
    "        popt, pcov = curve_fit(exponential_decay, frequencies, savgol_filter(magnitudes, 30, 3))\n",
    "        A, B, C = popt\n",
    "    except:\n",
    "        A, B, C = 0, 0, 0\n",
    "\n",
    "    features = {\n",
    "        \"Fit Amplitude\": A,\n",
    "        \"Fit Decay\": B,\n",
    "        \"Fit Offset\": C,\n",
    "        # \"Magnitudes\": magnitudes,\n",
    "        # \"Frequencies\": frequencies,\n",
    "        \"Peak Frequency\": peak_freq,\n",
    "        \"Mean Frequency\": mean_freq,\n",
    "        \"Median Frequency\": median_freq,\n",
    "        \"Bandwidth\": bandwidth,\n",
    "        \"Frequency Skewness\": freq_skewness,\n",
    "        \"Frequency Kurtosis\": freq_kurtosis,\n",
    "        \"Total Energy\": total_energy,\n",
    "        \"Spectral Entropy\": spectral_entropy,\n",
    "        \"Spectral Flatness\": spectral_flatness,\n",
    "        \"Spectral Roll-off\": spectral_rolloff,\n",
    "        \"Mean Amplitude\": mean_amplitude,\n",
    "        \"Std Amplitude\": std_amplitude,\n",
    "        \"Skew Amplitude\": skew_amplitude,\n",
    "        \"Kurtosis Amplitude\": kurtosis_amplitude\n",
    "    }\n",
    "    \n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "temp_feats = train_logs.groupby('id').apply(calculate_fft_features).reset_index()\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-fft')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard / mouse feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyboard_mouse_feats(train_logs_df):\n",
    "\n",
    "    # Creates two shift variables which lag the original variable by 1 and 2 periods respectively.\n",
    "    event_df = train_logs_df[['id', 'event_id', 'down_event']].copy(deep=True)\n",
    "\n",
    "    event_df['down_event_shift_1'] = event_df['down_event'].shift(periods=1)\n",
    "\n",
    "    event_df = event_df[['id', 'event_id', 'down_event_shift_1', 'down_event']]\n",
    "\n",
    "    ctrl_x_df = ((event_df['down_event_shift_1'] == 'Control') & (event_df['down_event'].str.lower() == 'x')).groupby(event_df['id']).sum().reset_index(name='count')\n",
    "\n",
    "    # Creating a DataFrame that contains all counts at an id level.\n",
    "    kb_shortcut_df = pd.DataFrame(event_df['id'].unique(), columns=['id'])\n",
    "    kb_shortcut_df['ctrl_x_cnt'] = ctrl_x_df['count']\n",
    "\n",
    "    # Calculating the proportion of mouse click events\n",
    "    mouse_event_df = pd.DataFrame(train_logs_df['id'].unique(), columns=['id'])\n",
    "    \n",
    "    mouse_event_df['mouse_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['down_event'].apply(lambda x: (x.isin(['Leftclick', 'Rightclick', 'Middleclick', 'Unknownclick']).sum())).reset_index()['down_event']\n",
    "\n",
    "    mouse_event_df['all_event_cnt'] = train_logs_df.groupby(train_logs_df['id'])['event_id'].max().reset_index()['event_id']\n",
    "\n",
    "    mouse_event_df['mouse_event_perc'] = (mouse_event_df['mouse_event_cnt']/mouse_event_df['all_event_cnt'])*100.0\n",
    "    \n",
    "    return kb_shortcut_df.merge(mouse_event_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_keyboard_mouse_feats(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-key_mouse')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"Generates aggregated time-related features for each essay ID.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Time-based calculations\n",
    "    df['action_time_sec'] = (df['up_time'] - df['down_time']) / 1000.0\n",
    "    df['time_since_last_event'] = df.groupby('id')['down_time'].diff() / 1000.0\n",
    "    df['cumulative_action_time'] = df.groupby('id')['action_time_sec'].cumsum()\n",
    "\n",
    "    # Prepare aggregation dictionary\n",
    "    aggregations = {\n",
    "        'action_time_sec': ['mean', 'sum', 'max', 'std'],\n",
    "        'time_since_last_event': ['mean', 'max', 'std'],\n",
    "        'cumulative_action_time': ['max']\n",
    "    }\n",
    "\n",
    "    # Add rolling window features to aggregations\n",
    "    for window in [5, 10, 15, 20, 30, 50]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).mean())\n",
    "        df[f'rolling_std_{window}'] = df.groupby('id')['action_time_sec'].transform(lambda x: x.rolling(window).std())\n",
    "        aggregations[f'rolling_mean_{window}'] = ['mean']\n",
    "        aggregations[f'rolling_std_{window}'] = ['mean']\n",
    "\n",
    "    # Aggregating features for each ID\n",
    "    aggregated_features = df.groupby('id').agg(aggregations)\n",
    "    aggregated_features.columns = ['_'.join(col) for col in aggregated_features.columns]\n",
    "    return aggregated_features\n",
    "\n",
    "def create_additional_time_features(df):\n",
    "    \"\"\"Generates additional aggregated time features for each essay ID.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['time_diff'] = abs(df.groupby('id')['down_time'].diff() - df['up_time'].shift(1)) / 1000\n",
    "    df['time_diff'] = df['time_diff'].fillna(0)  # Handling the first row for each ID\n",
    "\n",
    "    # Prepare aggregation dictionary\n",
    "    aggregates = {'time_diff': ['max', 'median']}\n",
    "\n",
    "    # Adding boolean counts for pauses\n",
    "    for pause in [0.5, 1, 1.5, 2, 3, 5, 10, 20]:\n",
    "        df[f'pauses_{pause}_sec'] = df['time_diff'].apply(lambda x: x > pause)\n",
    "        aggregates[f'pauses_{pause}_sec'] = ['sum']\n",
    "\n",
    "    # Aggregating features for each ID\n",
    "    additional_features = df.groupby('id').agg(aggregates)\n",
    "    additional_features.columns = ['_'.join(col) for col in additional_features.columns]\n",
    "    return additional_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(create_time_features(train_logs).reset_index(), on='id', how='left')\n",
    "temp_feats = temp_feats.merge(create_additional_time_features(train_logs).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-time_feat')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average down event per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_down_event_per_minute(df, events):\n",
    "\n",
    "    # Convert max down_time from milliseconds to minutes for each id\n",
    "    max_down_time = df.groupby('id')['down_time'].max() / (1000 * 60)\n",
    "\n",
    "    # Count number of each unique down_event for each id\n",
    "    event_count = df[df['down_event'].isin(events)].groupby(['id', 'down_event']).size().reset_index(name='counts')\n",
    "\n",
    "    # Calculate average number of each unique down_event per minute\n",
    "    event_count['avg_event_per_minute'] = event_count['counts'] / event_count['id'].map(max_down_time)\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    pivot_df = event_count.pivot(index='id', columns='down_event', values='avg_event_per_minute')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736\n"
     ]
    }
   ],
   "source": [
    "temp_feats = pd.DataFrame()\n",
    "temp_feats['id'] = train_essays['id'].unique()\n",
    "temp_feats = temp_feats.merge(get_avg_down_event_per_minute(*(train_logs, events)).reset_index(), on='id', how='left')\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-avg_event_per_minute')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average deletion per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deletions_per_min(df):\n",
    "\n",
    "    def calc(df, event_name):\n",
    "        # Convert down_time from milliseconds to seconds\n",
    "        df['down_time_sec'] = df['down_time'] / 1000\n",
    "        # Convert down_time_sec to datetime\n",
    "        df['down_time_sec'] = pd.to_datetime(df['down_time_sec'], unit='s')\n",
    "        # Set down_time_sec as index\n",
    "        df.set_index('down_time_sec', inplace=True)\n",
    "        # Calculate the length of deletions\n",
    "        df['deletion_length'] = df['text_change'].str.len()\n",
    "        # Resample to 1 minute intervals and sum deletion length\n",
    "        df_sum = df.groupby('id').resample('1T')['deletion_length'].sum()\n",
    "        # Calculate the average of the sums for each ID\n",
    "        df_avg = df_sum.groupby('id').mean()\n",
    "        # Reset index\n",
    "        df_avg = df_avg.reset_index()\n",
    "        # Rename the column\n",
    "        df_avg.rename(columns={'deletion_length': f'{event_name}'}, inplace=True)\n",
    "\n",
    "        return df_avg\n",
    "\n",
    "    # Filter the DataFrame for 'Delete' down_event and 'q' text_change\n",
    "    df_delete = df[(df['down_event']=='Delete')&(df['text_change']=='q')]\n",
    "    \n",
    "    # Repeat the same steps for 'Backspace'\n",
    "    df_backspace = df[(df['down_event']=='Backspace')&(df['text_change']=='q')]\n",
    "\n",
    "    temp1 = calc(df_delete, 'Delete')\n",
    "    temp2 = calc(df_backspace, 'Backspace')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_avg = temp1.merge(temp2, on='id', how='outer')\n",
    "    df_avg = df_avg.fillna(0)\n",
    "\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\1561204781.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = df['down_time'] / 1000\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\1561204781.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = pd.to_datetime(df['down_time_sec'], unit='s')\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\1561204781.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deletion_length'] = df['text_change'].str.len()\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\1561204781.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = df['down_time'] / 1000\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\1561204781.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['down_time_sec'] = pd.to_datetime(df['down_time_sec'], unit='s')\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\1561204781.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deletion_length'] = df['text_change'].str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calculate_deletions_per_min(train_logs)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-avg_char_deletion_per_minute')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average insertion per min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_insertions_per_min(df):\n",
    "\n",
    "    df_q = df[(df['down_event']=='q')&(df['text_change']=='q')]\n",
    "\n",
    "    # Convert down_time from milliseconds to seconds\n",
    "    df_q['down_time_sec'] = df_q['down_time'] / 1000\n",
    "    # Convert down_time_sec to datetime\n",
    "    df_q['down_time_sec'] = pd.to_datetime(df_q['down_time_sec'], unit='s')\n",
    "    # Set down_time_sec as index\n",
    "    df_q.set_index('down_time_sec', inplace=True)\n",
    "    # Calculate the length of deletions\n",
    "    df_q['insert_q'] = df_q['text_change'].str.len()\n",
    "    # Resample to 1 minute intervals and sum deletion length\n",
    "    df_sum = df_q.groupby('id').resample('1T')['insert_q'].sum()\n",
    "    # Calculate the average of the sums for each ID\n",
    "    df_avg = df_sum.groupby('id').mean()\n",
    "    # Reset index}\n",
    "    df_avg = df_avg.reset_index().fillna(0)\n",
    "\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\2281980173.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q['down_time_sec'] = df_q['down_time'] / 1000\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\2281980173.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q['down_time_sec'] = pd.to_datetime(df_q['down_time_sec'], unit='s')\n",
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\2281980173.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q['insert_q'] = df_q['text_change'].str.len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "739\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calculate_insertions_per_min(train_logs)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-avg_char_insert_per_minute')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total number of words (no deletions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_words(currTextInput):\n",
    "#     word_count = 0\n",
    "#     for i in range(len(currTextInput.values)):\n",
    "#         Input = currTextInput.values[i]\n",
    "#         if Input[0] == 'Replace':\n",
    "#             replaceTxt = Input[2].split(' => ')\n",
    "#             word_count += len(replaceTxt[1].split())\n",
    "#         elif Input[0] == 'Paste':\n",
    "#             word_count += len(Input[2].split())\n",
    "#         elif Input[0] == 'Remove/Cut':\n",
    "#             if i < len(currTextInput.values) - 1 and currTextInput.values[i+1][0] != 'Paste':\n",
    "#                 continue\n",
    "#             word_count -= len(Input[2].split())\n",
    "#         elif \"M\" not in Input[0]:\n",
    "#             word_count += len(Input[2].split())\n",
    "#     return word_count\n",
    "\n",
    "# def get_word_count_df(df):\n",
    "#     df = df[df.activity != 'Nonproduction']\n",
    "#     temp = df.groupby('id').apply(lambda x: count_words(x[['activity', 'cursor_position', 'text_change']]))\n",
    "#     word_count_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "#     word_count_df = word_count_df.merge(temp.rename('word_count'), on='id')\n",
    "#     return word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('< Word Count Calculation >')\n",
    "# temp_feats = get_word_count_df(train_logs)\n",
    "\n",
    "# train_feats = append_features(train_feats, temp_feats, suffix='-count_without_del')\n",
    "# print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-keystroke intervals (IKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IKI(df, down_event, aggregations):\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_periods = df[df['down_event'] == down_event]\n",
    "\n",
    "    # Calculate the interkeystroke intervals (IKIs) for each ID\n",
    "    df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n",
    "    df_periods = df_periods[~df_periods[\"down_time_diff\"].isnull()]\n",
    "\n",
    "    # Calculate the IKI statistics for each ID\n",
    "    df_periods = df_periods.groupby('id')['down_time_diff'].agg(aggregations).reset_index()\n",
    "\n",
    "    return df_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\369627138.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\369627138.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis.pinto1\\AppData\\Local\\Temp\\ipykernel_32020\\369627138.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_periods[\"down_time_diff\"] = df_periods.groupby(\"id\")[\"down_time\"].diff()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calculate_IKI(train_logs, 'Space', AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_word')\n",
    "print(len(train_feats.columns))\n",
    "\n",
    "temp_feats = calculate_IKI(train_logs, '.', AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_sentence')\n",
    "print(len(train_feats.columns))\n",
    "\n",
    "temp_feats = calculate_IKI(train_logs, 'Enter', AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IKI_paragraph')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_latencies(logs):\n",
    "    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "    group = logs.groupby('id')['time_diff']\n",
    "    largest_lantency = group.max()\n",
    "    smallest_lantency = group.min()\n",
    "    median_lantency = group.median()\n",
    "    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'id': logs['id'].unique(),\n",
    "        'largest_lantency': largest_lantency,\n",
    "        'smallest_lantency': smallest_lantency,\n",
    "        'median_lantency': median_lantency,\n",
    "        'initial_pause': initial_pause,\n",
    "        'pauses_half_sec': pauses_half_sec,\n",
    "        'pauses_1_sec': pauses_1_sec,\n",
    "        'pauses_1_half_sec': pauses_1_half_sec,\n",
    "        'pauses_2_sec': pauses_2_sec,\n",
    "        'pauses_3_sec': pauses_3_sec,\n",
    "    }).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "temp_feats = calc_latencies(train_logs)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-latencies')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IWD(group):\n",
    "\n",
    "    group['word_group'] = (group['word_count'].diff() > 0).cumsum()\n",
    "\n",
    "    result = group.groupby(['word_count', 'word_group']).agg({\n",
    "        'down_time': 'min',\n",
    "        'up_time': 'max',\n",
    "        'action_time': 'sum'\n",
    "    })\n",
    "\n",
    "    result['duration'] = result['up_time'] - result['down_time']\n",
    "    result['intraword pause'] = result['duration'] - result['action_time']\n",
    "    result['intraword pause'] = result['intraword pause'].apply(lambda x: x if x > 0 else 0)\n",
    "    result['IWD'] = result['intraword pause'] / result['duration']\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_iwd_aggregations(temp_df, aggregations):\n",
    "\n",
    "    temp_df = temp_df[['id', 'duration', 'intraword pause', 'IWD']]\n",
    "    temp_df = temp_df.groupby('id').agg(aggregations)\n",
    "    temp_df.columns = ['_'.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n"
     ]
    }
   ],
   "source": [
    "temp_logs = train_logs[(train_logs['activity']=='Input')&(train_logs['text_change']=='q')]\n",
    "temp_feats = temp_logs.groupby('id').apply(calculate_IWD).reset_index()\n",
    "temp_feats = get_iwd_aggregations(temp_feats, aggregations=AGGREGATIONS)\n",
    "\n",
    "train_feats = append_features(train_feats, temp_feats, suffix='-IWD')\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all NaNs: []\n",
      "Columns with all infinite values: []\n",
      "Columns with more than 90% of the same value: ['down_time_min-count_bursts', 'cursor_position_change1_q3-cursor_word_changes', 'cursor_position_abs_change2_min-cursor_word_changes', 'word_count_change1_q1-cursor_word_changes', 'word_count_change1_median-cursor_word_changes', 'word_count_change1_q3-cursor_word_changes', 'word_count_abs_change1_min-cursor_word_changes', 'word_count_abs_change1_q1-cursor_word_changes', 'word_count_abs_change1_median-cursor_word_changes', 'word_count_abs_change1_q3-cursor_word_changes', 'word_count_change2_q1-cursor_word_changes', 'word_count_change2_median-cursor_word_changes', 'word_count_abs_change2_min-cursor_word_changes', 'word_count_abs_change2_q1-cursor_word_changes', 'word_count_abs_change2_median-cursor_word_changes', 'word_count_change3_q1-cursor_word_changes', 'word_count_abs_change3_min-cursor_word_changes', 'word_count_abs_change3_q1-cursor_word_changes', 'word_count_abs_change5_min-cursor_word_changes', 'word_count_abs_change10_min-cursor_word_changes', 'smallest_lantency-latencies', 'initial_pause-latencies']\n"
     ]
    }
   ],
   "source": [
    "def filter_columns(df):\n",
    "    # Initialize lists to store column names for each condition\n",
    "    all_nan_columns = []\n",
    "    all_inf_columns = []\n",
    "    single_value_columns = []\n",
    "\n",
    "    # Iterate through columns\n",
    "    for col in df.columns:\n",
    "        # Check if the column contains numeric data\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Check for all NaN values\n",
    "            if df[col].isna().all():\n",
    "                all_nan_columns.append(col)\n",
    "            else:\n",
    "                # Check for all infinite values (positive or negative infinity)\n",
    "                if np.isinf(df[col]).all():\n",
    "                    all_inf_columns.append(col)\n",
    "                else:\n",
    "                    # Check for more than 90% of columns with only one unique value\n",
    "                    unique_value_count = df[col].nunique()\n",
    "                    total_count = df.shape[0]\n",
    "                    if unique_value_count == 1 and (total_count - df[col].isna().sum()) / total_count > 0.8:\n",
    "                        single_value_columns.append(col)\n",
    "\n",
    "    return {\n",
    "        \"AllNaNColumns\": all_nan_columns,\n",
    "        \"AllInfColumns\": all_inf_columns,\n",
    "        \"SingleValueColumns\": single_value_columns\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'df' is your DataFrame\n",
    "filtered_columns = filter_columns(train_feats)\n",
    "print(\"Columns with all NaNs:\", filtered_columns[\"AllNaNColumns\"])\n",
    "print(\"Columns with all infinite values:\", filtered_columns[\"AllInfColumns\"])\n",
    "print(\"Columns with more than 90% of the same value:\", filtered_columns[\"SingleValueColumns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786\n"
     ]
    }
   ],
   "source": [
    "train_feats = train_feats.drop(columns=filtered_columns[\"AllNaNColumns\"] + filtered_columns[\"AllInfColumns\"] + filtered_columns[\"SingleValueColumns\"])\n",
    "print(len(train_feats.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats.to_csv('output/'+output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('../data/train_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = train_feats.merge(scores, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25d21f21630>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGsCAYAAACB/u5dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmjElEQVR4nO3df3BU5b3H8c+yacKvZBVKNJAlQVpEhFAKDKUYC4pahqFgKrWIbURbp96gIEN1aP9QpmrwFxdtLQraoNcirdwA6gyCUhNolSs/rlOwDoKihAAXq7KbpHWxm71/7CQlkB+7m+fsOU/yfs3sZHLyDft1Y3I++zzPeY4vFovFBAAAYEAPtxsAAABdB8ECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGONasNi+fbtmzJihgQMHyufzaePGjUn/G7FYTI888oiGDRumrKwsDRo0SPfff7/5ZgEAQEIy3HrihoYGjR49WjfffLNKSkpS+jcWLFigrVu36pFHHtGoUaP02Wef6bPPPjPcKQAASJTPCzch8/l82rBhg2bNmtV8LBKJ6Je//KVeeOEFnTp1SiNHjtSDDz6oyZMnS5Lee+89FRUVaf/+/br44ovdaRwAALTg2TUW8+fP11tvvaV169bpr3/9q2bPnq3vfve7OnjwoCTp5Zdf1kUXXaRXXnlFQ4YMUWFhoX7yk58wYgEAgIs8GSyOHDmiiooKvfjiiyouLtbQoUO1ePFiXXbZZaqoqJAkffjhh/r444/14osv6rnnntOaNWu0Z88eXXfddS53DwBA9+XaGov27Nu3T9FoVMOGDWtxPBKJqH///pKkxsZGRSIRPffcc811zzzzjMaOHasDBw4wPQIAgAs8GSzq6+vl9/u1Z88e+f3+Fl/r27evJCkvL08ZGRktwscll1wiKT7iQbAAACD9PBksxowZo2g0qpMnT6q4uLjVmkmTJulf//qXPvjgAw0dOlSS9P7770uSCgoK0tYrAAD4N9euCqmvr9ehQ4ckxYPE8uXLNWXKFPXr10+DBw/WjTfeqL/85S969NFHNWbMGH3yySfatm2bioqKNH36dDU2Nmr8+PHq27evVqxYocbGRpWVlSknJ0dbt2514z8JAIBuz7VgUVVVpSlTppxzvLS0VGvWrNGXX36p++67T88995xqa2v11a9+Vd/61re0dOlSjRo1SpJ07Ngx3X777dq6dav69OmjadOm6dFHH1W/fv3S/Z8DAADkkX0sAABA1+DJy00BAICdCBYAAMCYtF8V0tjYqGPHjik7O1s+ny/dTw8AAFIQi8VUV1engQMHqkePtscl0h4sjh07pmAwmO6nBQAABtTU1Cg/P7/Nr6c9WGRnZ0uKN5aTk5PupwcAACkIh8MKBoPN5/G2pD1YNE1/5OTkECwAALBMR8sYWLwJAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMCbtG2QBQFcRjUo7dkjHj0t5eVJxseT3u90V4C6CBQCkoLJSWrBAOnr038fy86XHHpNKStzrC3AbUyEAkKTKSum661qGCkmqrY0fr6x0py/ACwgWAJCEaDQ+UhGLnfu1pmMLF8brgO6IYAEASdix49yRijPFYlJNTbwO6I4IFgCQhOPHzdYBXQ3BAgCSkJdntg7oaggWAJCE4uL41R8+X+tf9/mkYDBeB3RHBAsASILfH7+kVDo3XDR9vmIF+1mg+yJYAECSSkqk9eulQYNaHs/Pjx9nHwt0Z2yQBQApKCmRZs5k503gbAQLAEiR3y9Nnux2F4C3MBUCAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAmKSCRWFhoXw+3zmPsrIyp/oDAAAWSeomZLt27VI0Gm3+fP/+/brqqqs0e/Zs440BAAD7JBUsBgwY0OLzZcuWaejQofrOd75jtCkAAGCnlG+bfvr0aT3//PNatGiRfD5fm3WRSESRSKT583A4nOpTAgAAj0t58ebGjRt16tQp3XTTTe3WlZeXKxAIND+CwWCqTwkAADzOF4vFYql84zXXXKPMzEy9/PLL7da1NmIRDAYVCoWUk5OTylMDAIA0C4fDCgQCHZ6/U5oK+fjjj/X666+rsrKyw9qsrCxlZWWl8jQAAMAyKU2FVFRUKDc3V9OnTzfdDwAAsFjSwaKxsVEVFRUqLS1VRkbKaz8BAEAXlHSweP3113XkyBHdfPPNTvQDAAAslvSQw9VXX60U13sCAIAujnuFAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwJikg0Vtba1uvPFG9e/fX7169dKoUaO0e/duJ3oDAACWyUim+PPPP9ekSZM0ZcoUbd68WQMGDNDBgwd1/vnnO9UfAACwSFLB4sEHH1QwGFRFRUXzsSFDhhhvCgAA2CmpqZCXXnpJ48aN0+zZs5Wbm6sxY8Zo9erV7X5PJBJROBxu8QAAAF1TUsHiww8/1MqVK/X1r39dW7Zs0W233aY77rhDzz77bJvfU15erkAg0PwIBoOdbhoAAHiTLxaLxRItzszM1Lhx4/Tmm282H7vjjju0a9cuvfXWW61+TyQSUSQSaf48HA4rGAwqFAopJyenE60DAIB0CYfDCgQCHZ6/kxqxyMvL04gRI1ocu+SSS3TkyJE2vycrK0s5OTktHgAAoGtKKlhMmjRJBw4caHHs/fffV0FBgdGmAACAnZIKFnfeead27typBx54QIcOHdLatWu1atUqlZWVOdUfAACwSFLBYvz48dqwYYNeeOEFjRw5Ur/61a+0YsUKzZ0716n+AACARZJavGlCoos/AACAdziyeBMAAKA9BAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgTFLB4t5775XP52vxGD58uFO9AQAAy2Qk+w2XXnqpXn/99X//AxlJ/xMAAKCLSjoVZGRk6MILL3SiFwAAYLmk11gcPHhQAwcO1EUXXaS5c+fqyJEj7dZHIhGFw+EWDwAA0DUlFSwmTJigNWvW6NVXX9XKlSt1+PBhFRcXq66urs3vKS8vVyAQaH4Eg8FONw0AALzJF4vFYql+86lTp1RQUKDly5frlltuabUmEokoEok0fx4OhxUMBhUKhZSTk5PqUwMAgDQKh8MKBAIdnr87tfLyvPPO07Bhw3To0KE2a7KyspSVldWZpwEAAJbo1D4W9fX1+uCDD5SXl2eqHwAAYLGkgsXixYtVXV2tjz76SG+++aauvfZa+f1+zZkzx6n+AACARZKaCjl69KjmzJmjTz/9VAMGDNBll12mnTt3asCAAU71BwAALJJUsFi3bp1TfQAAgC6Ae4UAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAmE4Fi2XLlsnn82nhwoWG2gEAADZLOVjs2rVLTz31lIqKikz2AwAALJZSsKivr9fcuXO1evVqnX/++aZ7AgAAlkopWJSVlWn69OmaOnVqh7WRSEThcLjFAwAAdE0ZyX7DunXrtHfvXu3atSuh+vLyci1dujTpxgAAgH2SGrGoqanRggUL9Pvf/149e/ZM6HuWLFmiUCjU/KipqUmpUQAA4H2+WCwWS7R448aNuvbaa+X3+5uPRaNR+Xw+9ejRQ5FIpMXXWhMOhxUIBBQKhZSTk5N65wAAIG0SPX8nNRVy5ZVXat++fS2OzZs3T8OHD9fdd9/dYagAAABdW1LBIjs7WyNHjmxxrE+fPurfv/85xwEAQPfDzpsAAMCYpK8KOVtVVZWBNgAAQFfAiAUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMIVgAAABjCBYAAMAYggUAADCGYAEAAIwhWAAAAGMIFgAAwBiCBQAAMIZgAQAAjCFYAAAAYwgWAADAGIIFAAAwhmABAACMSSpYrFy5UkVFRcrJyVFOTo4mTpyozZs3O9UbAACwTFLBIj8/X8uWLdOePXu0e/duXXHFFZo5c6beffddp/oDAAAW8cVisVhn/oF+/frp4Ycf1i233JJQfTgcViAQUCgUUk5OTmeeGgAApEmi5++MVJ8gGo3qxRdfVENDgyZOnNhmXSQSUSQSadEYAADompJevLlv3z717dtXWVlZ+tnPfqYNGzZoxIgRbdaXl5crEAg0P4LBYKcaBgAA3pX0VMjp06d15MgRhUIhrV+/Xk8//bSqq6vbDBetjVgEg0GmQgAAsEiiUyGdXmMxdepUDR06VE899ZTRxgAAgHckev7u9D4WjY2NLUYkAABA95XU4s0lS5Zo2rRpGjx4sOrq6rR27VpVVVVpy5YtTvUHAAAsklSwOHnypH784x/r+PHjCgQCKioq0pYtW3TVVVc51R8AALBIUsHimWeecaoPAADQBXCvEAAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGBMhtsNwDuiUWnHDun4cSkvTyoulvx+t7sCANiEYAFJUmWltGCBdPTov4/l50uPPSaVlLjXFwDALkyFQJWV0nXXtQwVklRbGz9eWelOXwAA+xAsurloND5SEYud+7WmYwsXxusAAOgIwaKb27Hj3JGKM8ViUk1NvA4AgI4QLLq548fN1gEAujeCRTeXl2e2DgDQvREsurni4vjVHz5f61/3+aRgMF4HAEBHCBbdnN8fv6RUOjdcNH2+YgX7WQAAEkOwgEpKpPXrpUGDWh7Pz48fZx8LAECi2CALkuLhYeZMdt4EAHQOwQLN/H5p8mS3uwAA2IxgAcCTuHcNYKek1liUl5dr/Pjxys7OVm5urmbNmqUDBw441RuAbqqyUioslKZMkW64If6xsJDt5QEbJBUsqqurVVZWpp07d+q1117Tl19+qauvvloNDQ1O9Qegm+HeNYDdfLFYa3eJSMwnn3yi3NxcVVdX6/LLL0/oe8LhsAKBgEKhkHJyclJ9agBdUDQaH5loa5t5ny9+tdLhw0yLAOmW6Pm7U2ssQqGQJKlfv35t1kQiEUUikRaNAali3r1rS+beNSw0Brwp5X0sGhsbtXDhQk2aNEkjR45ss668vFyBQKD5EQwGU31KdHPMu3d93LsGsF/KwaKsrEz79+/XunXr2q1bsmSJQqFQ86OmpibVp0Q3xrx798C9awD7pbTGYv78+dq0aZO2b9+uIUOGJPW9rLFAsph37z6afta1tfFpj7Pxswbck+j5O6kRi1gspvnz52vDhg3605/+lHSoAFKRzLw77Ma9awD7JRUsysrK9Pzzz2vt2rXKzs7WiRMndOLECf3zn/90qj+AefduhnvXAHZLairE18a9tSsqKnTTTTcl9G8wFYJkVVXFF2p25I03uFKgK+EKIMBbHLnctBNbXgApKy6Ov1vtaN69uDj9vcE53LsGsBO3TYfnMe8OAPYgWMAKzLsDgB24uymsUVIizZzJvDsAeBnBAlZh3h0AvI2pEAAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxBAsAAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgTIbbDQBIv2hU2rFDOn5cysuTioslv9/trgB0BQQLWIUTYudVVkoLFkhHj/77WH6+9NhjUkmJe30B6BqYCoE1KiulwkJpyhTphhviHwsL48eRmMpK6brrWoYKSaqtjR/ntQTQWQQLWIETYudFo/GRiljs3K81HVu4MF4HAKkiWMDzOCGasWPHucHsTLGYVFMTrwOAVBEs0iQalaqqpBdeiH/kJJg4TohmHD9utg4AWsPizTRgsVzncEI0IzfXbB0AtIYRC4fZtDbAq6MqeXlm6wAAziFYOMimtQFevuKiuDg+wuPztf51n08KBuN1aNvJk2brAKA1BAsH2bI2wOujKn5/fNpIOjdcNH2+YgX7WXSEkR8A6UCwcJANawNsGVUpKZHWr5cGDWp5PD8/fpy1Kh1j5AdAOhAsHGTDO0RbRlWkeHj46CPpjTektWvjHw8fJlQkqmnkp7UQKcWPM/IDoLO4KsRBTe8Qa2tb/2Pu88W/7uY7RBtGVc7k90uTJ7vdBQCgLUmPWGzfvl0zZszQwIED5fP5tHHjRgfa6hpsWBtgw6gKzGia9mqLz+eNaS8Adks6WDQ0NGj06NF64oknnOiny/H62gDm3bsPm6a9ANgr6amQadOmadq0aU700mWVlEgzZ3rzrpxNoyrXXRcPEWdO2XhlVAVm2DbtBcBOjq+xiEQiikQizZ+Hw2Gnn9KTvLw2oGlUpbXdQVescH9UBWYw7QUgHRwPFuXl5Vq6dKnTT4NO8vKoCsxomvZqbzqEaS8AneX45aZLlixRKBRqftTU1Dj9lEhR06jKnDnxj4SKrsXvj/9s2/PDH/JzB9A5jgeLrKws5eTktHgASL9oNH4fmPasW8dVIQA6hw2ygG6io6tCJK4KAdB5Sa+xqK+v16FDh5o/P3z4sN555x3169dPgwcPNtoc0isaZY1FV8ZVIQDSIelgsXv3bk2ZMqX580WLFkmSSktLtWbNGmONIb0qK1u/KuSxx7gqpKvgqhAA6eCLxdq6c4AzwuGwAoGAQqEQ6y08ounupm1tO+6FjbzQedGoVFjY8Rbzhw8zUgXgXImev1lj0c21d3dTKX6cbZ67hjO3mG+LlzZDi0alqqr4gtOqKm/+P2hDjzbgdexaCBbdHAv6zPPyH8mSEul732v9a9/7nndGpior46MrU6ZIN9wQ/1hYGD/uFTb0aANex66HYJEmXj3Z1NaarevuvP5H8q67pE2bWv/apk3xr7utaWru7MBbWxs/7oXX0oYebcDr2DWxxiINvLww8uGHEzuZPPSQ9POfO9+PzSorpe9/v+2v//d/u/vzPn1a6tVLamxsu6ZHD+mf/5QyM9PX15ma1oG0NYrmhXUgNvR4Jq9e7WXb6wjWWHhG08nm7F+eo0fjx91O5P/7v2bruqtoVLr11vZrbr3V3ZGqX/+6/VAhxb/+61+np5/W2HAHVht6bOLlETSbXkckh2DhIBtONkeOmK1zmlenlKqqpE8/bb/m00/jdW7585/N1jnBhr02bOhR8v40gy2vI5JHsHCQDSebwkKzdU7y8ruvRH+Gbv6s+/QxW+cEG/basKHH9q72ajrm9tVeNryOSA3BwkE2nGx+/GOzdU7x+rsvG3zjG2brnNB0B1afr/Wv+3zu34HVhh5tmGaw4XVEaggW3dyVV0o9e7Zf07NnvM4tNrz7mjzZbJ0TbHiHeOZeG2efcJo+d3uvDRt6tGGawYbXEakhWDgo0aTtdiLvaOjbzaFxyY53X5MnS337tl/Tt6+7weLCC83WOaWkRFq8OH6Fypl69Igfd/tKKinew/r10qBBLY/n53tjp1obQqTk/dcRqUn6XiFIXKJJ281EvmNHYutAduxw76Row7svmFNZKT3yyLkjVNFo/Pi3vuWNE05JiTRzpjcv5WyaZmgvkHtlmsHLryNSw4iFg06cMFvnBBtO2ja8+6qqkurr26+pr3d3PY0N/z92tMW85P6015n8/njgnjMn/tErJ0O/P95Te374Q+/0i66FYOGgTz4xW+eE3FyzdU6wYZGXDQt1O9q6Pdk6J9gw7XUmr17+HI3Ge2rPunXe6NfLV3shNQQLBw0YYLauu2pa5NXejdJY5NWx114zW+cEG0bQmnj5hGjLPYC42qtrIlg46OwFSZ2tc8LJk2bruisbrgqpqTFb5wQbRtAk758QbQhoNlzthdQQLBzUNITfHreH8G1YvxCNSqWl7deUlrr7B6i4+NyrGM7Wo4e7P2sbNsiygQ0nRBt+r22b9kLiCBYOahrCb29tgNtD+DasX9i2LbGFkdu2paef1rz5ZmL34XjzzfT005pE7/nn5r0BbVhgasMJsbhY6t+//Zr+/d39vbZhVAWpIVg4rOk67bNHLoJBb1ynbcP6hf/6L7N1TrDhj6QN017/939m65xgw8/aBjaMqiA17GORBlyn3Tl1dWbrnGDDH8nPPzdb54SO9lRJts4JNqwDsWF/mgkTzNbBOxix6OZsWL9gwx/yb3+77emkJj5fvM4trLHoPmprzdY54be/NVvnNK9eWuxFBIs08PJlaTasX/jgA7N1Tqiubn9TJyn+9erq9PTTmowExycTrXNCv35m65xgw5SSDXvoJLoGxQuLNysrpYKCln/DCwq88TfciwgWDmvrsrSjR71xWdpzz5mtc8KpU2brnFBRYbbOCadPm61zgg3BwoYRNBv20GloMFvnlMpK6fvfP3d0p7Y2ftztv+FeRLBwUEfbE8di7l+W9tFHZuuckJVlts4JiV7t4eZVITYEtI0bzdY5IdHfVzd/r23YQ2fcOLN1TohGpVtvbb/m1lu9My1y+nR8sf3tt8c/uvUmgWDhIBt2v+vVy2ydE2x4F2vDAtOO1oAkW+eEjz82W+cEG7ZvLy7u+G672dnuXm46darZOidUVSW2CNbNn3WTu+6SeveW7rxT+s1v4h97944fTzeChYNsWEBlw7uGY8fM1jnBhnex//qX2TonfPih2Ton/M//mK1zQjTa8dqpujp3/3+cPDmxvTbc3K3WhhApxcPDww+f+/OMRuPH0x0uukSw8OpqXRuuybfhXUMkYrbOCYleOuzmJcY2rLGwoUcbbua2YoXZOif4/dJll7Vfc9llXJbfkdOnpeXL269Zvjy9vzPWBwsvr9a14Zr8yZOlnj3br+nZ0913DTYM4dsQLGzo0YYQeeSI2TonPPus2TonnD4tvfRS+zUvveRuiLThHkC//W3Hb6aj0fRetmt1sPD6at2O7h2RbJ0TolHpiy/ar/niC/ax6MhXvmK2zgk2BDQbdPT7kmydE2zYHbS9HX+bxGLxOrckuu+Mm/vTePFyfGuDhQ0bO3U0zJdsnRNsGDK1YfGmDaNTBIvuw4ZdVhN94+fmG8QnnjBb54TCQrN1JlgbLGzY2GnfPrN1TlizxmydE2y4lNOGIfx//MNsHdAZ+/ebrXOCDeHn0kvN1plgbbCw4cZUf/mL2TonHDpkts4JNrz7soENV66g+7Ah6NpwlZIXdzC1Nlh0NFqRbJ0TbJh3t+ESRBvmtAEkp7HRbJ0TbBiJPHzYbJ0J1gYLG9Yv2LCrJQCgdaGQ2TonnDhhts4Ea4PF7bd3fDVFjx7xOrfYMIcIAGhdR1etJFvnBC9uIJhSsHjiiSdUWFionj17asKECXr77bdN99WhzExp7Nj2a8aOjde55csvzdY5wYZfHABA6w4eNFtnQtLB4g9/+IMWLVqke+65R3v37tXo0aN1zTXX6GSa7xF8+rS0d2/7NXv3uru5ig3rFwAA9vLiWpWkg8Xy5cv105/+VPPmzdOIESP05JNPqnfv3vrd737nRH9t8uJuYwAAdHdJBYvTp09rz549mnrGjSN69OihqVOn6q233mr1eyKRiMLhcIuHCV7cbQwAgO4uqWDx97//XdFoVBdccEGL4xdccIFOtLHktLy8XIFAoPkRDAZT7/YMQ4earXPCV79qtq67ys42WwcAXcVZp+NO15ng+FUhS5YsUSgUan7U1NQY+Xf/4z86vlmS3x+vc4sNO2/+9a9m65zw3ntm65xgww59Tz5pts4JixebrXNCoregTvetqs/09NNm65yQ6K7Ibu6evGmT2TonePE844vFEl/vf/r0afXu3Vvr16/XrFmzmo+Xlpbq1KlT2pTAqxsOhxUIBBQKhZSTk5NS002a7kHflp//XHrooU49Raf16dP+znG9e0sNDenrpzWJ3BvC7atCsrLaX4ibmenuJjWSHa8jPZpBj2bQoxnnndf+XhqBgHTqVOefJ9Hzd1IjFpmZmRo7dqy2nREhGxsbtW3bNk2cODH1blP00EPx8HD2yIXf741QIcVDQ+/erX/NC6FCSuwOg26LRNq+dNgLoUKy43WkRzPo0Qx6NOPUqXh4aI2pUJGMpKdCFi1apNWrV+vZZ5/Ve++9p9tuu00NDQ2aN2+eE/116KGH4iMC//mf0vz58Y//+Ic3QkWThob47YkvuCD+zvuCC+KfeyFUNInF4tMdTenc54t/7oVfmiaRiHT0qHT++VJGRvzj0aPeCBVNYrFzpzsqK731OsZi5053PPmk93o8e7pj8WLv9Xj2dMddd3mvx7OnO55+2ns9nj3dsW2b93o8e0B+0yZv9XjqlHTyZPwupn36xD+ePJn+UCElORXS5De/+Y0efvhhnThxQt/4xjf0+OOPa8KECQl9r8mpEAAAkB6Jnr9TChadQbAAAMA+jqyxAAAAaA/BAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAxGel+wqb9uMLhcLqfGgAApKjpvN3RvpppDxZ1dXWSpGAwmO6nBgAAnVRXV6dAW3c9kwtbejc2NurYsWPKzs6WL5H70XYh4XBYwWBQNTU1bGfeCbyOZvA6msHraAavoxlOvo6xWEx1dXUaOHCgevRoeyVF2kcsevToofz8/HQ/rafk5OTwi2MAr6MZvI5m8DqawetohlOvY3sjFU1YvAkAAIwhWAAAAGMIFmmUlZWle+65R1lZWW63YjVeRzN4Hc3gdTSD19EML7yOaV+8CQAAui5GLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAs0mD79u2aMWOGBg4cKJ/Pp40bN7rdkpXKy8s1fvx4ZWdnKzc3V7NmzdKBAwfcbss6K1euVFFRUfMGOhMnTtTmzZvdbstqy5Ytk8/n08KFC91uxTr33nuvfD5fi8fw4cPdbstKtbW1uvHGG9W/f3/16tVLo0aN0u7du9PeB8EiDRoaGjR69Gg98cQTbrditerqapWVlWnnzp167bXX9OWXX+rqq69WQ0OD261ZJT8/X8uWLdOePXu0e/duXXHFFZo5c6beffddt1uz0q5du/TUU0+pqKjI7Vasdemll+r48ePNjz//+c9ut2Sdzz//XJMmTdJXvvIVbd68WX/729/06KOP6vzzz097L2nf0rs7mjZtmqZNm+Z2G9Z79dVXW3y+Zs0a5ebmas+ePbr88std6so+M2bMaPH5/fffr5UrV2rnzp269NJLXerKTvX19Zo7d65Wr16t++67z+12rJWRkaELL7zQ7Tas9uCDDyoYDKqioqL52JAhQ1zphRELWCsUCkmS+vXr53In9opGo1q3bp0aGho0ceJEt9uxTllZmaZPn66pU6e63YrVDh48qIEDB+qiiy7S3LlzdeTIEbdbss5LL72kcePGafbs2crNzdWYMWO0evVqV3phxAJWamxs1MKFCzVp0iSNHDnS7Xass2/fPk2cOFFffPGF+vbtqw0bNmjEiBFut2WVdevWae/evdq1a5fbrVhtwoQJWrNmjS6++GIdP35cS5cuVXFxsfbv36/s7Gy327PGhx9+qJUrV2rRokX6xS9+oV27dumOO+5QZmamSktL09oLwQJWKisr0/79+5mLTdHFF1+sd955R6FQSOvXr1dpaamqq6sJFwmqqanRggUL9Nprr6lnz55ut2O1M6eJi4qKNGHCBBUUFOiPf/yjbrnlFhc7s0tjY6PGjRunBx54QJI0ZswY7d+/X08++WTagwVTIbDO/Pnz9corr+iNN95Qfn6+2+1YKTMzU1/72tc0duxYlZeXa/To0Xrsscfcbssae/bs0cmTJ/XNb35TGRkZysjIUHV1tR5//HFlZGQoGo263aK1zjvvPA0bNkyHDh1yuxWr5OXlnfPG4JJLLnFlWokRC1gjFovp9ttv14YNG1RVVeXawqSuqLGxUZFIxO02rHHllVdq3759LY7NmzdPw4cP19133y2/3+9SZ/arr6/XBx98oB/96Edut2KVSZMmnXP5/fvvv6+CgoK090KwSIP6+voW6fvw4cN655131K9fPw0ePNjFzuxSVlamtWvXatOmTcrOztaJEyckSYFAQL169XK5O3ssWbJE06ZN0+DBg1VXV6e1a9eqqqpKW7Zscbs1a2RnZ5+ztqdPnz7q378/a36StHjxYs2YMUMFBQU6duyY7rnnHvn9fs2ZM8ft1qxy55136tvf/rYeeOAB/eAHP9Dbb7+tVatWadWqVelvJgbHvfHGGzFJ5zxKS0vdbs0qrb2GkmIVFRVut2aVm2++OVZQUBDLzMyMDRgwIHbllVfGtm7d6nZb1vvOd74TW7BggdttWOf666+P5eXlxTIzM2ODBg2KXX/99bFDhw653ZaVXn755djIkSNjWVlZseHDh8dWrVrlSh/cNh0AABjD4k0AAGAMwQIAABhDsAAAAMYQLAAAgDEECwAAYAzBAgAAGEOwAAAAxhAsAACAMQQLAABgDMECAAAYQ7AAAADGECwAAIAx/w+zHqamdBA87AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_feats['score'], train_feats['idle_time-idle'], 'o', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
